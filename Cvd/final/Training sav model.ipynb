{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "from matplotlib import pyplot\n",
    "import math\n",
    "import pickle\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import sys\n",
    "import pandas as pd\n",
    "np.set_printoptions(threshold=sys.maxsize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_size = 13\n",
    "\n",
    "dataset = pd.read_csv('CVD dataset-plin-0-6-7.csv') #13\n",
    "\n",
    "data=dataset.iloc[:, :].values\n",
    "X = dataset.iloc[:, :-1].values\n",
    "y = dataset.iloc[:, f_size].values\n",
    "\n",
    "rows = len(data)   \n",
    "cols = len(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataNoCvd = data[~(data[:,f_size] > 0.0)]\n",
    "dataCvd = data[~(data[:,f_size] < 1.0)]\n",
    "rowsNoCvd = len(dataNoCvd)    \n",
    "rowsCvd = len(dataCvd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to create the appropriate limits for training, test and validation set\n",
    "def find_NoCvd_limitsArray(i,DownNoCvd, UpNoCvd, fold_NoCvd_total, fold_NoCvd_residue):\n",
    "    if i>0 :\n",
    "        DownNoCvd = UpNoCvd\n",
    "        UpNoCvd= fold_NoCvd_total+UpNoCvd\n",
    "    if i < fold_NoCvd_residue :\n",
    "        UpNoCvd+= 1\n",
    "    return   DownNoCvd, UpNoCvd   \n",
    " \n",
    "def find_Cvd_limitsArray(i,DownCvd, UpCvd, fold_Cvd_total, fold_Cvd_residue,cv):\n",
    "    if i>0 :\n",
    "        DownCvd = UpCvd\n",
    "        UpCvd= fold_Cvd_total+UpCvd\n",
    "    if i >= cv - fold_Cvd_residue :\n",
    "         UpCvd+= 1     \n",
    "    return   DownCvd, UpCvd  \n",
    "\n",
    "def find_testValSubset(DownNoCvd,UpNoCvd,DownCvd,UpCvd,dataNoCvd,dataCvd):\n",
    "    temp1=dataNoCvd[DownNoCvd:UpNoCvd,:]\n",
    "    temp2=dataCvd[DownCvd:UpCvd,:]\n",
    "    temp3=np.concatenate((temp1, temp2))\n",
    "    return temp3\n",
    "\n",
    "def find_trainSubset(DownNoCvd,UpNoCvd,DownCvd,UpCvd,dataNoCvd,dataCvd):\n",
    "    temp1 = np.delete(dataNoCvd, slice(DownNoCvd, UpNoCvd), axis=0)\n",
    "    temp2 = np.delete(dataCvd, slice(DownCvd, UpCvd), axis=0)\n",
    "    temp3 = np.concatenate((temp1, temp2))\n",
    "    return temp1, temp2, temp3\n",
    "\n",
    "def find_trainSubset1(DownNoCvd,UpNoCvd,DownCvd,UpCvd,dataNoCvd,dataCvd):\n",
    "    temp1 = np.delete(dataNoCvd, slice(DownNoCvd, UpNoCvd), axis=0)\n",
    "    temp2 = np.delete(dataCvd, slice(DownCvd, UpCvd), axis=0)\n",
    "    temp3 = np.delete(temp1, slice(0, 46), axis=0)\n",
    "    temp4 = np.delete(temp2, slice(0, 4), axis=0)\n",
    "    temp5 = np.concatenate((temp3, temp4))\n",
    "    \n",
    "    return temp3, temp4, temp5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_test_sets(f_size,cv,dataNoCvd,dataCvd,val_ratio):\n",
    "    test_total = []\n",
    "    train_total = []\n",
    "    train_total_Cvd = []\n",
    "    train_total_NoCvd = []\n",
    "    validation_total = []\n",
    "    \n",
    "    rowsNoCvd = len(dataNoCvd)    \n",
    "    rowsCvd = len(dataCvd)\n",
    "\n",
    "    fold_Cvd_total = rowsCvd//cv\n",
    "    fold_Cvd_residue= rowsCvd%cv\n",
    "#     print(\"fold_Cvd_total  :\",fold_Cvd_total,\" fold_Cvd_residue  :\",fold_Cvd_residue)\n",
    "\n",
    "    fold_NoCvd_total = rowsNoCvd//cv\n",
    "    fold_NoCvd_residue= rowsNoCvd%cv\n",
    "#     print(\"fold_NoCvd_total:\",fold_NoCvd_total,\"fold_NoCvd_residue:\",fold_NoCvd_residue)\n",
    "    \n",
    "    Cvd_val = round(504*val_ratio*(rowsCvd/len(data)))\n",
    "    noCvd_val = round(504*val_ratio)-Cvd_val\n",
    "#     print(noCvd_val)\n",
    "\n",
    "\n",
    "  \n",
    "    DownNoCvd = 468\n",
    "    UpNoCvd = 519\n",
    "    DownCvd = 36\n",
    "    UpCvd = 41\n",
    "    \n",
    "\n",
    "    X_test = []\n",
    "    y_test = []\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    X_val = []\n",
    "    y_val = []\n",
    "#     \n",
    "    testSubset_total = find_testValSubset(DownNoCvd,UpNoCvd,DownCvd,UpCvd,dataNoCvd,dataCvd)        \n",
    "        \n",
    "\n",
    "    validationSubset_total = find_testValSubset(0,noCvd_val,0,Cvd_val,dataNoCvd,dataCvd)\n",
    "\n",
    "    trainSubset_NoCvd ,trainSubset_Cvd ,trainSubset_total = find_trainSubset1(DownNoCvd,UpNoCvd,DownCvd,UpCvd,dataNoCvd,dataCvd)\n",
    "    \n",
    "    #creating X_train, y_train, X_test, y_test\n",
    "    X_test.append(np.delete(testSubset_total, f_size, axis=1))\n",
    "    y_test_temp = np.delete(testSubset_total, slice(0, f_size), axis=1)\n",
    "    y_test.append(np.reshape(y_test_temp, len(y_test_temp)))\n",
    "    X_test_temp = np.array(X_test)\n",
    "    X_test = X_test_temp[0]\n",
    "    y_test_temp = np.array(y_test)\n",
    "    y_test = y_test_temp[0]\n",
    "    \n",
    "    X_val.append(np.delete(validationSubset_total, f_size, axis=1))\n",
    "    y_val_temp = np.delete(validationSubset_total, slice(0, f_size), axis=1)\n",
    "    y_val.append(np.reshape(y_val_temp, len(y_val_temp)))\n",
    "    X_val_temp = np.array(X_val)\n",
    "    X_val = X_val_temp[0]\n",
    "    y_val_temp = np.array(y_val)\n",
    "    y_val = y_val_temp[0]\n",
    "        \n",
    "    X_train.append(np.delete(trainSubset_total, f_size, axis=1))\n",
    "    y_train_temp = np.delete(trainSubset_total, slice(0, f_size), axis=1)\n",
    "    y_train.append(np.reshape(y_train_temp, len(y_train_temp)))\n",
    "    X_train_temp = np.array(X_train)\n",
    "    X_train = X_train_temp[0]\n",
    "    y_train_temp = np.array(y_train)\n",
    "    y_train = y_train_temp[0]\n",
    "        \n",
    "\n",
    "    return X_test, y_test, X_train, y_train, X_val, y_val, trainSubset_NoCvd,trainSubset_Cvd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_test, y_test, X_train, y_train, X_val, y_val, trainSubset_NoCvd,trainSubset_Cvd = create_train_test_sets(f_size,10,dataNoCvd,dataCvd,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_weighted_average(models,X):\n",
    "    \n",
    "    # initializing lists that we will need later\n",
    "    y_pred = []\n",
    "    y_pred_prob = []\n",
    "    y_pred_models = []\n",
    "    y_pred_models1 = []\n",
    "    aver=[]\n",
    "    # count the number of models\n",
    "    models_size = len(models)\n",
    "    \n",
    "    # count the number of patients we want to predict their probability of cvd, in our case it will be always 1\n",
    "    X_size = len(X)\n",
    "    # initialiazing the weights\n",
    "    weights = [0.2, 0.15, 0.15, 0.1, 0.15, 0.25]\n",
    "    \n",
    "    # y_pred_models gives the output of each model as marginal output, it will bw need for shap\n",
    "    # y_pred_models1 gives the output of each model as probabilities\n",
    "    for i in range(models_size):\n",
    "        y_pred_models.append(models[i].predict(X,output_margin=True))  \n",
    "        y_pred_models1.append(models[i].predict_proba(X)[:,1])\n",
    "    \n",
    "    total_shap_values = []\n",
    "    total_expected_value = 0\n",
    "    \n",
    "    for i in range(models_size):\n",
    "        # for each model we create a shap explainer\n",
    "        explainer = shap.TreeExplainer(models[i],feature_perturbation = \"tree_path_dependent\")\n",
    "        \n",
    "        #  using the explainer we calculate the shapley values of each feature\n",
    "        shap_values = explainer.shap_values(X)\n",
    "        # we multiply the shapley values with the weight that correspond with the current model\n",
    "        shap_values_weights = [[l*weights[i] for l in k] for k in shap_values]\n",
    "        \n",
    "        if(i==0):\n",
    "             total_shap_values = shap_values_weights.copy()\n",
    "        else:\n",
    "            # we add the weighted shapley values of all models together\n",
    "            for a in range(len(total_shap_values)):\n",
    "                for b in range(len(total_shap_values[0])):\n",
    "                     total_shap_values[a][b] += shap_values_weights[a][b]\n",
    "        # we add together the expected value of each model\n",
    "        total_expected_value += explainer.expected_value*weights[i] \n",
    " \n",
    "    t_shap_values = np.array(total_shap_values)\n",
    "    \n",
    "       \n",
    "    for j in range(X_size):\n",
    "        \n",
    "        # here we perform the weighted voting\n",
    "        sum = 0\n",
    "        for i in range(models_size):\n",
    "            sum = sum + y_pred_models[i][j]*weights[i]\n",
    "        avr = sum  \n",
    "\n",
    "        aver.append(avr)\n",
    "\n",
    "        pred_prob = math.exp(avr)/(1 + math.exp(avr))\n",
    "        y_pred_prob.append(pred_prob)\n",
    "        \n",
    "        if pred_prob>= 0.5:\n",
    "            y_pred.append(1.)\n",
    "        else:\n",
    "            y_pred.append(0.)\n",
    "  \n",
    "            \n",
    "    return y_pred,t_shap_values,y_pred_prob, total_expected_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "import pickle\n",
    "\n",
    "def my_fit(f_size,ratio,dataNoCvd,dataCvd):   \n",
    "    Subarray=[]\n",
    "    models = []\n",
    "    rowsNoCvd = len(dataNoCvd)    \n",
    "    rowsCvd = len(dataCvd)\n",
    "    \n",
    "    gammas = [1.0, 1.0, 0.5, 1.0, 0.5, 1.5]\n",
    "    numOfSubsamples = rowsNoCvd//(rowsCvd*ratio)\n",
    "    numOfSubsamples= int(numOfSubsamples)\n",
    "    SubNoCvd = rowsNoCvd//numOfSubsamples\n",
    "    residue = rowsNoCvd- SubNoCvd*numOfSubsamples\n",
    "\n",
    "\n",
    "    Up = 0\n",
    "\n",
    "    # we create as many classifiers/models as occur from the partition of the data\n",
    "    # in our case we will have 6\n",
    "    for i in range(numOfSubsamples):\n",
    "        \n",
    "\n",
    "        \n",
    "        classifier = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=0.5,\n",
    "                  colsample_bynode=1, colsample_bytree=0.5, gamma=gammas[i],\n",
    "                  learning_rate=0.52, max_delta_step=0, max_depth=3,\n",
    "                  min_child_weight=1, missing=None, n_estimators=1000, n_jobs=1,\n",
    "                  nthread=None, objective='binary:logistic', random_state=0,\n",
    "                  reg_alpha=0, reg_lambda=1, scale_pos_weight=3, seed=None,\n",
    "                  silent=None, subsample=0.8, verbosity=1)\n",
    "        \n",
    "\n",
    "        Down = Up\n",
    "        Up= Up + SubNoCvd\n",
    "        if i < residue :\n",
    "            Up+= 1\n",
    "        \n",
    "        Sub1=dataNoCvd[Down:Up,:]\n",
    "        Sub2=np.concatenate((Sub1, dataCvd))\n",
    "        Subarray.append(Sub2)\n",
    "        \n",
    "    #Dividing to X and y of the previous traing set    \n",
    "        X=np.delete(Subarray[i], f_size, axis=1)\n",
    "        y=np.delete(Subarray[i], slice(0, f_size), axis=1)\n",
    "        y=np.reshape(y, len(y))\n",
    "        \n",
    "        \n",
    "       \n",
    "        \n",
    "        eval_set  = [(X,y), (X_val,y_val)]\n",
    "\n",
    "        classifier.fit( X, y, eval_set=eval_set, eval_metric=\"auc\", early_stopping_rounds=50,verbose = False)\n",
    "        \n",
    "        models.append(classifier)\n",
    "        \n",
    "        # save the model to disk\n",
    "        filename = 'finalized_model.sav'\n",
    "        pickle.dump(models, open(filename, 'wb'))\n",
    "    return models\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def my_brier_score(y_test, y_pred_prob):\n",
    "    sq_sum = 0\n",
    "    for i in range(len(y_test)):\n",
    "        dif = y_pred_prob[i] - y_test[i]\n",
    "        sq_sum += math.pow(dif, 2)\n",
    "    brier_sc = sq_sum/len(y_test)\n",
    "    \n",
    "    return brier_sc      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import  confusion_matrix,roc_curve, roc_auc_score, accuracy_score\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import brier_score_loss\n",
    "from statistics import mean,pstdev\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def create_ensemble_model():\n",
    "  \n",
    "    shap_values = []\n",
    "    y_pred_total = []\n",
    "    y_pred_prob_total = []\n",
    "    cv_expected_value = 0\n",
    "\n",
    "    # we training the model with the training and validation set that we created before\n",
    "    models = my_fit(f_size,2,trainSubset_NoCvd,trainSubset_Cvd)\n",
    "     \n",
    "    y_pred,shap_values,y_pred_prob, total_expected_value = predict_weighted_average(models,X_test)\n",
    "  \n",
    "\n",
    "    # we make some ckecks with the test set that we ceated before                   \n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    #     print(cm)\n",
    "\n",
    "    total1=sum(sum(cm))\n",
    "        #####from confusion matrix calculate accuracy\n",
    "    accuracy=(cm[0,0]+cm[1,1])/total1\n",
    "#     print ('Accuracy : ', accuracy)\n",
    "\n",
    "    specificity = cm[0,0]/(cm[0,0]+cm[0,1])\n",
    "#     print('Specificity : ', specificity )\n",
    "\n",
    "    sensitivity = cm[1,1]/(cm[1,0]+cm[1,1])\n",
    "#     print('Sensitivity : ', sensitivity)\n",
    "    brier_score = my_brier_score(y_test, y_pred_prob)\n",
    "#     print(brier_score)\n",
    "\n",
    "    auc = roc_auc_score(y_test, y_pred_prob)\n",
    "#     print(auc)\n",
    " \n",
    "        \n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_ensemble_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
