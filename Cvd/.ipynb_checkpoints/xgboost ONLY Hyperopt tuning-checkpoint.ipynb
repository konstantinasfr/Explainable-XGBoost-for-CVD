{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "560\n",
      "17\n"
     ]
    }
   ],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import numpy\n",
    "numpy.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "f_size = 16\n",
    "# Importing the dataset\n",
    "\n",
    "dataset = pd.read_csv('CVD dataset2.csv')\n",
    "# dataset = pd.read_csv('CVD-dataset-2-3-11-13-15.csv')\n",
    "data=dataset.iloc[:, :].values\n",
    "X = dataset.iloc[:, :-1].values\n",
    "y = dataset.iloc[:, f_size].values\n",
    "# X = dataset.iloc[:, [ 0, 1, 2, 5, 10, 11, 13, 15]].values\n",
    "\n",
    "rows = len(data)    # 3 rows in your example\n",
    "cols = len(data[0])\n",
    "print(rows)\n",
    "print(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "519\n",
      "41\n"
     ]
    }
   ],
   "source": [
    "dataNoCvd = data[~(data[:,f_size] > 0.0)]\n",
    "dataCvd = data[~(data[:,f_size] < 1.0)]\n",
    "rowsNoCvd = len(dataNoCvd)    \n",
    "rowsCvd = len(dataCvd)\n",
    "print(rowsNoCvd)\n",
    "print(rowsCvd)\n",
    "# print(dataNoCvd)\n",
    "# print(dataCvd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_NoCvd_limitsArray(i,DownNoCvd, UpNoCvd, fold_NoCvd_total, fold_NoCvd_residue):\n",
    "    if i>0 :\n",
    "        DownNoCvd = UpNoCvd\n",
    "        UpNoCvd= fold_NoCvd_total+UpNoCvd\n",
    "    if i < fold_NoCvd_residue :\n",
    "        UpNoCvd+= 1\n",
    "#     print(\"NoCvd limits, Down :\",DownNoCvd,\"Up \",UpNoCvd)\n",
    "    return   DownNoCvd, UpNoCvd   \n",
    " \n",
    "def find_Cvd_limitsArray(i,DownCvd, UpCvd, fold_Cvd_total, fold_Cvd_residue,cv):\n",
    "    if i>0 :\n",
    "        DownCvd = UpCvd\n",
    "        UpCvd= fold_Cvd_total+UpCvd\n",
    "    if i >= cv - fold_Cvd_residue :\n",
    "         UpCvd+= 1     \n",
    "#     print(\"Cvd limits, Down :\",DownCvd,\"Up \",UpCvd)\n",
    "    return   DownCvd, UpCvd  \n",
    "\n",
    "def find_testValSubset(DownNoCvd,UpNoCvd,DownCvd,UpCvd,dataNoCvd,dataCvd):\n",
    "    temp1=dataNoCvd[DownNoCvd:UpNoCvd,:]\n",
    "    temp2=dataCvd[DownCvd:UpCvd,:]\n",
    "    temp3=np.concatenate((temp1, temp2))\n",
    "#     print(len(temp1),\"  \",len(temp2),\"  \",len(temp3))\n",
    "    return temp3\n",
    "\n",
    "def find_trainSubset(DownNoCvd,UpNoCvd,DownCvd,UpCvd,dataNoCvd,dataCvd):\n",
    "    temp1 = np.delete(dataNoCvd, slice(DownNoCvd, UpNoCvd), axis=0)\n",
    "    temp2 = np.delete(dataCvd, slice(DownCvd, UpCvd), axis=0)\n",
    "    temp3 = np.concatenate((temp1, temp2))\n",
    "#     print(len(temp1),\"  \",len(temp2),\"  \",len(temp3))\n",
    "    return temp1, temp2, temp3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_test_sets(f_size,cv,dataNoCvd,dataCvd,val_ratio):\n",
    "    test_total = []\n",
    "    train_total = []\n",
    "    train_total_Cvd = []\n",
    "    train_total_NoCvd = []\n",
    "    validation_total = []\n",
    "    \n",
    "    rowsNoCvd = len(dataNoCvd)    \n",
    "    rowsCvd = len(dataCvd)\n",
    "\n",
    "    fold_Cvd_total = rowsCvd//cv\n",
    "    fold_Cvd_residue= rowsCvd%cv\n",
    "#     print(\"fold_Cvd_total  :\",fold_Cvd_total,\" fold_Cvd_residue  :\",fold_Cvd_residue)\n",
    "\n",
    "    fold_NoCvd_total = rowsNoCvd//cv\n",
    "    fold_NoCvd_residue= rowsNoCvd%cv\n",
    "#     print(\"fold_NoCvd_total:\",fold_NoCvd_total,\"fold_NoCvd_residue:\",fold_NoCvd_residue)\n",
    "    \n",
    "    Cvd_val = round(504*val_ratio*(rowsCvd/len(data)))\n",
    "    noCvd_val = round(504*val_ratio)-Cvd_val\n",
    "#     print(noCvd_val)\n",
    "    \n",
    "    DownNoCvd=0\n",
    "    DownCvd=0\n",
    "    UpNoCvd = fold_NoCvd_total\n",
    "    UpCvd = fold_Cvd_total\n",
    "\n",
    "    for i in range(cv):\n",
    "        X_test = []\n",
    "        y_test = []\n",
    "        X_train = []\n",
    "        y_train = []\n",
    "        X_val = []\n",
    "        y_val = []\n",
    "#         print(\"============\",i,\"==============\")\n",
    "        DownNoCvd, UpNoCvd = find_NoCvd_limitsArray(i, DownNoCvd, UpNoCvd, fold_NoCvd_total, fold_NoCvd_residue)\n",
    "        DownCvd, UpCvd = find_Cvd_limitsArray(i, DownCvd, UpCvd, fold_Cvd_total, fold_Cvd_residue,cv)\n",
    "#         print(DownNoCvd,UpNoCvd,DownCvd,UpCvd)\n",
    "        testSubset_total = find_testValSubset(DownNoCvd,UpNoCvd,DownCvd,UpCvd,dataNoCvd,dataCvd)\n",
    "        if i!=9:\n",
    "#             print(UpNoCvd,UpNoCvd+noCvd_val,UpCvd,UpCvd+Cvd_val)\n",
    "            validationSubset_total = find_testValSubset(UpNoCvd,UpNoCvd+noCvd_val,UpCvd,UpCvd+Cvd_val,dataNoCvd,dataCvd)\n",
    "#             print(DownNoCvd,UpNoCvd+noCvd_val,DownCvd,UpCvd+Cvd_val)\n",
    "            trainSubset_NoCvd ,trainSubset_Cvd ,trainSubset_total = find_trainSubset(DownNoCvd,UpNoCvd+noCvd_val,DownCvd,UpCvd+Cvd_val,dataNoCvd,dataCvd)\n",
    "        else:\n",
    "#             print(DownNoCvd-noCvd_val,DownNoCvd,DownCvd-Cvd_val,DownCvd)\n",
    "            validationSubset_total = find_testValSubset(DownNoCvd-noCvd_val,DownNoCvd,DownCvd-Cvd_val,DownCvd,dataNoCvd,dataCvd)\n",
    "#             print(DownNoCvd-noCvd_val,UpNoCvd,DownCvd-Cvd_val,UpCvd)\n",
    "            trainSubset_NoCvd ,trainSubset_Cvd ,trainSubset_total = find_trainSubset(DownNoCvd-noCvd_val,UpNoCvd,DownCvd-Cvd_val,UpCvd,dataNoCvd,dataCvd)\n",
    "    #creating X_train, y_train, X_test, y_test\n",
    "        X_test.append(np.delete(testSubset_total, f_size, axis=1))\n",
    "        y_test_temp = np.delete(testSubset_total, slice(0, f_size), axis=1)\n",
    "        y_test.append(np.reshape(y_test_temp, len(y_test_temp)))\n",
    "        X_test_temp = np.array(X_test)\n",
    "        X_test = X_test_temp[0]\n",
    "        y_test_temp = np.array(y_test)\n",
    "        y_test = y_test_temp[0]\n",
    "        \n",
    "        X_val.append(np.delete(validationSubset_total, f_size, axis=1))\n",
    "        y_val_temp = np.delete(validationSubset_total, slice(0, f_size), axis=1)\n",
    "        y_val.append(np.reshape(y_val_temp, len(y_val_temp)))\n",
    "        X_val_temp = np.array(X_val)\n",
    "        X_val = X_val_temp[0]\n",
    "        y_val_temp = np.array(y_val)\n",
    "        y_val = y_val_temp[0]\n",
    "        \n",
    "        X_train.append(np.delete(trainSubset_total, f_size, axis=1))\n",
    "        y_train_temp = np.delete(trainSubset_total, slice(0, f_size), axis=1)\n",
    "        y_train.append(np.reshape(y_train_temp, len(y_train_temp)))\n",
    "        X_train_temp = np.array(X_train)\n",
    "        X_train = X_train_temp[0]\n",
    "        y_train_temp = np.array(y_train)\n",
    "        y_train = y_train_temp[0]\n",
    "        \n",
    "    #add every subset in a list so we can handle thm later \n",
    "        test_total.append([X_test,y_test])\n",
    "        validation_total.append([X_val,y_val])\n",
    "        train_total.append([X_train,y_train]) \n",
    "        train_total_NoCvd.append(trainSubset_NoCvd)\n",
    "        train_total_Cvd.append(trainSubset_Cvd)\n",
    "    return train_total, test_total, validation_total, train_total_NoCvd, train_total_Cvd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_total, test_total, validation_total, train_total_NoCvd, train_total_Cvd = create_train_test_sets(f_size,10,dataNoCvd,dataCvd,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "454\n",
      "ratio in train set: 0.07268722466960352\n",
      "ratio in test set: 0.07142857142857142\n",
      "454\n",
      "ratio in train set: 0.07268722466960352\n",
      "ratio in test set: 0.07142857142857142\n",
      "454\n",
      "ratio in train set: 0.07268722466960352\n",
      "ratio in test set: 0.07142857142857142\n",
      "454\n",
      "ratio in train set: 0.07268722466960352\n",
      "ratio in test set: 0.07142857142857142\n",
      "454\n",
      "ratio in train set: 0.07268722466960352\n",
      "ratio in test set: 0.07142857142857142\n",
      "454\n",
      "ratio in train set: 0.07268722466960352\n",
      "ratio in test set: 0.07142857142857142\n",
      "454\n",
      "ratio in train set: 0.07268722466960352\n",
      "ratio in test set: 0.07142857142857142\n",
      "454\n",
      "ratio in train set: 0.07268722466960352\n",
      "ratio in test set: 0.07142857142857142\n",
      "454\n",
      "ratio in train set: 0.07268722466960352\n",
      "ratio in test set: 0.07142857142857142\n",
      "454\n",
      "ratio in train set: 0.07048458149779736\n",
      "ratio in test set: 0.08928571428571429\n"
     ]
    }
   ],
   "source": [
    "# Chech ratio in each train and test set\n",
    "def find_ratio(index_list):\n",
    "    one = 0\n",
    "    lenght=len(index_list[0])\n",
    "    for i in range(lenght):\n",
    "#         print(index_list[1])\n",
    "        if index_list[1][i] == 1.0 :\n",
    "            one+= 1\n",
    "    ratio = one/lenght\n",
    "    return ratio\n",
    "\n",
    "for i in range(10):\n",
    "    print(len(train_total[i][0]))\n",
    "    ratio = find_ratio(train_total[i])\n",
    "    print(\"ratio in train set:\", ratio)\n",
    "    ratio = find_ratio(test_total[i])\n",
    "    print(\"ratio in test set:\", ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import  confusion_matrix,roc_curve, roc_auc_score, accuracy_score\n",
    "from statistics import mean\n",
    "import pandas as pd, numpy as np\n",
    "import xgboost as xgb\n",
    "from hyperopt import hp, fmin, tpe, STATUS_OK, Trials \n",
    "\n",
    "total = []\n",
    "\n",
    "def objective(space):\n",
    "\n",
    "    clf = xgb.XGBClassifier(n_estimators = 10000,\n",
    "                            learning_rate = space['learning_rate'],\n",
    "                            max_depth = int(space['max_depth']),\n",
    "                            min_child_weight = int(space['min_child_weight']),\n",
    "                            gamma = space['gamma'],\n",
    "                            colsample_bylevel = space['colsample_bylevel'],\n",
    "                            reg_lambda = space['reg_lambda'],\n",
    "                            scale_pos_weight = space['scale_pos_weight'],\n",
    "                            colsample_bytree = space['colsample_bytree'],\n",
    "                            reg_alpha = space['reg_alpha'],\n",
    "                            subsample = space['subsample'])\n",
    "\n",
    "\n",
    "    accuracy, specificity, sensitivity, auc = my_cross_val( 10, train_total, test_total, validation_total, train_total_NoCvd, train_total_Cvd,clf)\n",
    "\n",
    "#     print(\"accuraccy:    \", mean(accuracy))\n",
    "#     print(\"specificity:  \", mean(specificity))\n",
    "#     print(\"sensitivity:  \", mean(sensitivity))\n",
    "#     print(\"auc        :  \", mean(auc))\n",
    "    total.append([mean(accuracy),mean(specificity),mean(sensitivity),mean(auc)])\n",
    "#     return np.mean([roc_auc_score(X, y),roc_auc_score(X, y)])\n",
    "#     return np.mean(cross_val_score(clf, X, y, cv=3, scoring='roc_auc'))\n",
    "    return{'loss':1-mean(auc), 'status': STATUS_OK , 'auc':mean(auc), 'accuracy':mean(accuracy)}\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/train-test-split-and-cross-validation-in-python-80b61beca4b6\n",
    "\n",
    "from sklearn.metrics import  confusion_matrix,roc_curve, roc_auc_score, accuracy_score\n",
    "from imxgboost.imbalance_xgb import imbalance_xgboost as imb_xgb\n",
    "from sklearn.model_selection import StratifiedKFold # import KFold\n",
    "from sklearn import metrics\n",
    "\n",
    "def my_cross_val( cv, train_total, test_total, validation_total, train_total_NoCvd, train_total_Cvd,clf):\n",
    "    accuracy = []\n",
    "    specificity = []\n",
    "    sensitivity = []\n",
    "    auc = []\n",
    "    \n",
    "    # print(X)\n",
    "    for i in range(cv):\n",
    "#         X_train = train_total[i][0]\n",
    "#         y_train = train_total[i][1]\n",
    "        X_test = test_total[i][0]\n",
    "        y_test = test_total[i][1]\n",
    "        \n",
    "\n",
    "        X_train = train_total[i][0]\n",
    "        y_train = train_total[i][1]\n",
    "        \n",
    "        X_val = validation_total[i][0]\n",
    "        y_val = validation_total[i][1]\n",
    "        \n",
    "        eval_set  = [(X_train,y_train), (X_val,y_val)]\n",
    "        \n",
    "        clf.fit( X_train, y_train, eval_set=eval_set, eval_metric=\"auc\", early_stopping_rounds=50,verbose = False )\n",
    "\n",
    "        \n",
    "        \n",
    "        y_pred = clf.predict(X_test)\n",
    "        \n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "    #     print(cm)\n",
    "\n",
    "        total1=sum(sum(cm))\n",
    "        #####from confusion matrix calculate accuracy\n",
    "        accuracy1=(cm[0,0]+cm[1,1])/total1\n",
    "    #   print ('Accuracy : ', accuracy1)\n",
    "\n",
    "        specificity1 = cm[0,0]/(cm[0,0]+cm[0,1])\n",
    "    #   print('Specificity : ', specificity1 )\n",
    "\n",
    "        sensitivity1 = cm[1,1]/(cm[1,0]+cm[1,1])\n",
    "    #   print('Sensitivity : ', sensitivity1)\n",
    "        \n",
    "#         y = np.array(y_test)\n",
    "#         pred = np.array(y_pred)\n",
    "#         fpr, tpr, thresholds = metrics.roc_curve(y, pred, pos_label=2)\n",
    "#         auc1 = metrics.auc(fpr, tpr)\n",
    "        \n",
    "        auc1 = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "    \n",
    "        accuracy.append(accuracy1)\n",
    "        specificity.append(specificity1)\n",
    "        sensitivity.append(sensitivity1) \n",
    "        auc.append(auc1)\n",
    "    return accuracy, specificity, sensitivity, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [01:02<00:00,  2.41trial/s, best loss: 0.41405731523378586]\n",
      "{'colsample_bylevel': 0.7508535005823251, 'colsample_bytree': 0.7083398153035388, 'gamma': 0.7928795745742508, 'learning_rate': 0.2849656341201214, 'max_depth': 5.0, 'min_child_weight': 5.0, 'reg_alpha': 0.13054827129081417, 'reg_lambda': 1.9213153628660709, 'scale_pos_weight': 7.0, 'subsample': 0.8707849783536298}\n"
     ]
    }
   ],
   "source": [
    "# https://gist.github.com/walterreade/6e20dba959277bd9af77\n",
    "\n",
    "from hyperopt import hp, fmin, tpe, STATUS_OK, Trials \n",
    "\n",
    "space ={\n",
    "        'learning_rate':hp.uniform ('learning_rate', 0.1, 0.5),\n",
    "        'max_depth': hp.quniform(\"max_depth\", 1, 5, 1),\n",
    "        'min_child_weight': hp.quniform('min_child_weight', 1, 7, 1),\n",
    "#         'subsample': hp.quniform('subsample', 0.5, 1, 0.05),\n",
    "        'gamma': hp.uniform('gamma', 0, 1),\n",
    "        'colsample_bylevel':hp.uniform('colsample_bylevel', 0.5, 1),\n",
    "        'reg_lambda':hp.uniform('reg_lambda', 0, 3),\n",
    "        'scale_pos_weight':hp.quniform(\"scale_pos_weight\", 1, 9, 1),\n",
    "        'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1),\n",
    "        'reg_alpha':hp.uniform('reg_alpha', 0, 0.3),\n",
    "        'subsample': hp.uniform ('subsample', 0.8, 1)\n",
    "    }      \n",
    "\n",
    "\n",
    "# parameters : [0.4, 3, 1000, 1, 1, 0.5, 1.5, 3, 0.5, 0.1, 1.0]\n",
    "# accuracy : 0.6910714285714286\n",
    "# spes     : 0.68789592760181\n",
    "# sens     : 0.74\n",
    "# auc      : 0.713947963800905\n",
    "# [13, 15, 11, 2, 3]\n",
    "# [13, 15, 11, 2, 3, 1, 14, 12, 10, 5, 6, 8, 4, 9, 0, 7, 16]\n",
    "    \n",
    "#         }\n",
    "# maxdepth': hp.choice(\"xmax_depth\",np.arange(5, 30, dtype=int) ),\n",
    "\n",
    "trials = Trials()\n",
    "best = fmin(fn=objective,\n",
    "            space=space,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=150,\n",
    "            trials=trials)\n",
    "\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.5910714285714286, 0.5955505279034691, 0.54, 0.5677752639517346], [0.5982142857142857, 0.6187405731523379, 0.335, 0.4768702865761689], [0.4142857142857143, 0.393552036199095, 0.69, 0.5417760180995475], [0.5660714285714286, 0.5703996983408748, 0.515, 0.5426998491704373], [0.5660714285714286, 0.564630467571644, 0.585, 0.574815233785822], [0.5910714285714286, 0.6012066365007541, 0.46, 0.5306033182503771], [0.5642857142857143, 0.5512443438914028, 0.74, 0.6456221719457014], [0.5160714285714286, 0.5069758672699849, 0.64, 0.5734879336349924], [0.3892857142857143, 0.37051282051282053, 0.64, 0.5052564102564102], [0.625, 0.6340874811463046, 0.515, 0.5745437405731523], [0.475, 0.455052790346908, 0.735, 0.595026395173454], [0.48928571428571427, 0.4800904977375566, 0.615, 0.5475452488687782], [0.44285714285714284, 0.4203996983408748, 0.74, 0.5801998491704374], [0.5964285714285714, 0.6128205128205129, 0.39, 0.5014102564102564], [0.5857142857142857, 0.593552036199095, 0.49, 0.5417760180995475], [0.5732142857142857, 0.5761689291101055, 0.54, 0.5580844645550528], [0.5714285714285714, 0.570550527903469, 0.59, 0.5802752639517346], [0.6125, 0.6243212669683258, 0.465, 0.5446606334841629], [0.6267857142857143, 0.6397812971342383, 0.465, 0.5523906485671192], [0.65, 0.6723227752639518, 0.36, 0.5161613876319758], [0.46964285714285714, 0.45294117647058824, 0.685, 0.5689705882352941], [0.5321428571428571, 0.5166666666666666, 0.735, 0.6258333333333334], [0.45535714285714285, 0.43778280542986425, 0.69, 0.5638914027149321], [0.4517857142857143, 0.43778280542986425, 0.64, 0.5388914027149321], [0.4625, 0.4415912518853695, 0.735, 0.5882956259426848], [0.5267857142857143, 0.5165912518853695, 0.665, 0.5907956259426848], [0.4589285714285714, 0.4416289592760181, 0.69, 0.565814479638009], [0.5125, 0.5011689291101056, 0.665, 0.5830844645550528], [0.5732142857142857, 0.5667043740573152, 0.665, 0.6158521870286576], [0.48392857142857143, 0.4743212669683258, 0.615, 0.5446606334841629], [0.5464285714285715, 0.5513197586726999, 0.485, 0.5181598793363499], [0.41964285714285715, 0.4012820512820513, 0.665, 0.5331410256410256], [0.5642857142857143, 0.56263197586727, 0.585, 0.573815987933635], [0.5428571428571428, 0.5377828054298642, 0.61, 0.5738914027149321], [0.5892857142857143, 0.5992835595776772, 0.465, 0.5321417797888386], [0.45535714285714285, 0.44347662141779787, 0.615, 0.5292383107088989], [0.4107142857142857, 0.39159125188536953, 0.665, 0.5282956259426848], [0.44107142857142856, 0.4223604826546003, 0.69, 0.5561802413273002], [0.6232142857142857, 0.635972850678733, 0.465, 0.5504864253393665], [0.40535714285714286, 0.3838612368024133, 0.69, 0.5369306184012066], [0.44107142857142856, 0.42639517345399697, 0.64, 0.5331975867269985], [0.5785714285714286, 0.5955882352941176, 0.365, 0.4802941176470588], [0.5089285714285714, 0.49536199095022626, 0.685, 0.5901809954751132], [0.5053571428571428, 0.4973981900452489, 0.61, 0.5536990950226244], [0.4767857142857143, 0.4609351432880845, 0.69, 0.5754675716440423], [0.6035714285714285, 0.6127450980392157, 0.49, 0.5513725490196079], [0.5910714285714286, 0.5993212669683258, 0.49, 0.5446606334841629], [0.4642857142857143, 0.44939668174962294, 0.66, 0.5546983408748114], [0.5910714285714286, 0.597473604826546, 0.51, 0.553736802413273], [0.6035714285714285, 0.6032428355957767, 0.615, 0.6091214177978883], [0.5678571428571428, 0.5607088989441931, 0.665, 0.6128544494720966], [0.5107142857142857, 0.49732277526395174, 0.69, 0.5936613876319758], [0.6214285714285714, 0.6417043740573152, 0.365, 0.5033521870286576], [0.55, 0.5358974358974359, 0.73, 0.632948717948718], [0.5607142857142857, 0.5531297134238311, 0.66, 0.6065648567119155], [0.5035714285714286, 0.49551282051282053, 0.615, 0.5552564102564103], [0.6428571428571429, 0.657051282051282, 0.465, 0.561025641025641], [0.5375, 0.5299773755656109, 0.63, 0.5799886877828054], [0.5464285714285715, 0.5435143288084464, 0.59, 0.5667571644042232], [0.4910714285714286, 0.4798265460030166, 0.63, 0.5549132730015083], [0.6392857142857142, 0.6588989441930618, 0.39, 0.5244494720965309], [0.5089285714285714, 0.49551282051282053, 0.685, 0.5902564102564103], [0.475, 0.46270739064856714, 0.635, 0.5488536953242835], [0.5482142857142858, 0.5432503770739064, 0.61, 0.5766251885369532], [0.4392857142857143, 0.4242835595776772, 0.635, 0.5296417797888386], [0.5375, 0.5300904977375566, 0.64, 0.5850452488687783], [0.5160714285714286, 0.5050150829562594, 0.665, 0.5850075414781297], [0.45, 0.4243966817496229, 0.79, 0.6071983408748115], [0.46785714285714286, 0.4454751131221719, 0.765, 0.6052375565610859], [0.4375, 0.4127828054298643, 0.765, 0.5888914027149321], [0.48214285714285715, 0.4666289592760181, 0.69, 0.5783144796380091], [0.41607142857142854, 0.39543740573152336, 0.69, 0.5427187028657617], [0.45535714285714285, 0.43959276018099547, 0.66, 0.5497963800904978], [0.4714285714285714, 0.4569758672699849, 0.665, 0.5609879336349924], [0.5089285714285714, 0.4914781297134238, 0.735, 0.6132390648567119], [0.4642857142857143, 0.4433634992458522, 0.735, 0.5891817496229261], [0.5196428571428572, 0.5108974358974359, 0.64, 0.575448717948718], [0.45714285714285713, 0.4473227752639517, 0.585, 0.5161613876319758], [0.4517857142857143, 0.4377073906485671, 0.635, 0.5363536953242836], [0.49464285714285716, 0.4802036199095023, 0.69, 0.5851018099547511], [0.5892857142857143, 0.5973981900452489, 0.49, 0.5436990950226244], [0.45535714285714285, 0.44155354449472095, 0.64, 0.5407767722473605], [0.5714285714285714, 0.5743966817496229, 0.54, 0.5571983408748115], [0.5446428571428572, 0.5377073906485671, 0.64, 0.5888536953242836], [0.6321428571428571, 0.6377450980392156, 0.565, 0.6013725490196078], [0.45714285714285713, 0.4416289592760181, 0.66, 0.550814479638009], [0.5339285714285714, 0.5300904977375566, 0.59, 0.5600452488687783], [0.4892857142857143, 0.4762443438914027, 0.66, 0.5681221719457014], [0.5446428571428572, 0.5336349924585219, 0.68, 0.606817496229261], [0.6089285714285715, 0.6282805429864253, 0.365, 0.49664027149321266], [0.48928571428571427, 0.4741704374057315, 0.685, 0.5795852187028657], [0.5571428571428572, 0.5531674208144797, 0.615, 0.5840837104072398], [0.42142857142857143, 0.39743589743589747, 0.735, 0.5662179487179487], [0.5357142857142857, 0.5319758672699849, 0.59, 0.5609879336349924], [0.6196428571428572, 0.6320135746606335, 0.465, 0.5485067873303168], [0.4, 0.3720588235294118, 0.755, 0.5635294117647058], [0.5517857142857143, 0.5435520361990951, 0.665, 0.6042760180995476], [0.41964285714285715, 0.4031297134238311, 0.64, 0.5215648567119155], [0.6089285714285715, 0.6185897435897436, 0.49, 0.5542948717948718], [0.4625, 0.4492081447963801, 0.64, 0.5446040723981901]]\n"
     ]
    }
   ],
   "source": [
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/        \n",
    "\n",
    "params = {\n",
    "                    \"learning_rate\"    :[0.3],  #0.01-0.2 Makes the model more robust by shrinking the weights on each step\n",
    "                    \"max_depth\"        :[2,3],  #3-10 control over-fitting as higher depth will allow model to learn relations very specific to a particular sample\n",
    "                    \"n_estimators\"     :[100,200,300,400],\n",
    "                    \"min_child_weight\" :[1,2],         #0.5-1 small values might lead to under-fitting\n",
    "                    \"gamma\"            :[0,0.5,1],            #Makes the algorithm conservative --> No overfitting\n",
    "                    \"colsample_bylevel\" :[0.5,0.75,1], #0.5-1\n",
    "                     \"reg_lambda\"      :[1, 1.5, 2], #  it should be explored to reduce overfitting.\n",
    "                    \"scale_pos_weight\" :[3,4,5],\n",
    "                    \"colsample_bytree\" :[0.5,0.75],\n",
    "                    \"reg_alpha\"        :[0.05,0.1,0.2],\n",
    "                    \"subsample\"        :[0.8]\n",
    "    \n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.740878001587038\n"
     ]
    }
   ],
   "source": [
    "params = [[],[],[],[],[],[]]\n",
    "for i in range(6):\n",
    "    best_par = optimizer.max.get('params')\n",
    "    params.append(best_par.get('learning_rate'))\n",
    "    params.append(round(best_par.get('max_depth')))\n",
    "    params.append(round(best_par.get('n_estimators')))\n",
    "    params.append(round(best_par.get('min_child_weight')))\n",
    "    params.append(best_par.get('gamma'))\n",
    "    params.append(best_par.get('colsample_bylevel'))\n",
    "    params.append(best_par.get('reg_lambda'))\n",
    "    params.append(round(best_par.get('scale_pos_weight')))\n",
    "    params.append(best_par.get('colsample_bytree'))\n",
    "    params.append(best_par.get('reg_alpha'))\n",
    "    params.append(best_par.get('subsample'))\n",
    "\n",
    "print(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
