{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "560\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import numpy\n",
    "numpy.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "f_size = 5\n",
    "# Importing the dataset\n",
    "dataset = pd.read_csv('CVD-dataset-2-3-11-13-15.csv')\n",
    "# dataset = pd.read_csv('CVD dataset2.csv')\n",
    "data=dataset.iloc[:, :].values\n",
    "X = dataset.iloc[:, :-1].values\n",
    "y = dataset.iloc[:, f_size].values\n",
    "# X = dataset.iloc[:, [ 0, 1, 2, 5, 10, 11, 13, 15]].values\n",
    "\n",
    "rows = len(data)    # 3 rows in your example\n",
    "cols = len(data[0])\n",
    "print(rows)\n",
    "print(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "519\n",
      "41\n"
     ]
    }
   ],
   "source": [
    "dataNoCvd = data[~(data[:,f_size] > 0.0)]\n",
    "dataCvd = data[~(data[:,f_size] < 1.0)]\n",
    "rowsNoCvd = len(dataNoCvd)    \n",
    "rowsCvd = len(dataCvd)\n",
    "print(rowsNoCvd)\n",
    "print(rowsCvd)\n",
    "# print(dataNoCvd)\n",
    "# print(dataCvd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_NoCvd_limitsArray(i,DownNoCvd, UpNoCvd, fold_NoCvd_total, fold_NoCvd_residue):\n",
    "    if i>0 :\n",
    "        DownNoCvd = UpNoCvd\n",
    "        UpNoCvd= fold_NoCvd_total+UpNoCvd\n",
    "    if i < fold_NoCvd_residue :\n",
    "        UpNoCvd+= 1\n",
    "#     print(\"NoCvd limits, Down :\",DownNoCvd,\"Up \",UpNoCvd)\n",
    "    return   DownNoCvd, UpNoCvd   \n",
    " \n",
    "def find_Cvd_limitsArray(i,DownCvd, UpCvd, fold_Cvd_total, fold_Cvd_residue,cv):\n",
    "    if i>0 :\n",
    "        DownCvd = UpCvd\n",
    "        UpCvd= fold_Cvd_total+UpCvd\n",
    "    if i >= cv - fold_Cvd_residue :\n",
    "         UpCvd+= 1     \n",
    "#     print(\"Cvd limits, Down :\",DownCvd,\"Up \",UpCvd)\n",
    "    return   DownCvd, UpCvd  \n",
    "\n",
    "def find_testSubset(DownNoCvd,UpNoCvd,DownCvd,UpCvd):\n",
    "    temp1=dataNoCvd[DownNoCvd:UpNoCvd,:]\n",
    "    temp2=dataCvd[DownCvd:UpCvd,:]\n",
    "    temp3=np.concatenate((temp1, temp2))\n",
    "#     print(len(temp1),\"  \",len(temp2),\"  \",len(temp3))\n",
    "    return temp1, temp2, temp3\n",
    "\n",
    "def find_trainSubset(DownNoCvd,UpNoCvd,DownCvd,UpCvd):\n",
    "    temp1 = np.delete(dataNoCvd, slice(DownNoCvd, UpNoCvd), axis=0)\n",
    "    temp2 = np.delete(dataCvd, slice(DownCvd, UpCvd), axis=0)\n",
    "    temp3 = np.concatenate((temp1, temp2))\n",
    "#     print(len(temp1),\"  \",len(temp2),\"  \",len(temp3))\n",
    "    return temp1, temp2, temp3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    " \n",
    "# calculate the Euclidean distance between two vectors\n",
    "def euclidean_distance(row1, row2):\n",
    "    distance = 0.0\n",
    "    for i in range(len(row1)-1):\n",
    "        distance += (row1[i] - row2[i])**2\n",
    "    return sqrt(distance)\n",
    " \n",
    "# Locate the most similar neighbors\n",
    "def get_neighbors(train, test_row, num_neighbors):\n",
    "    distances = list()\n",
    "    j = 0\n",
    "    for train_row in train:\n",
    "        dist = euclidean_distance(test_row, train_row)\n",
    "        distances.append((train_row, dist, j))\n",
    "        j+=1\n",
    "    distances.sort(key=lambda tup: tup[1],reverse = True)\n",
    "    neighbors = list()\n",
    "    index = list()\n",
    "    for i in range(num_neighbors):\n",
    "        neighbors.append(distances[i][0])\n",
    "        index.append(distances[i][2])\n",
    "    return neighbors,index\n",
    "\n",
    "def create_train_test_sets(cv,dataNoCvd,dataCvd):\n",
    "    test_NoCvd_total = [[],[],[],[],[],[],[],[],[],[]]\n",
    "    test_total = []\n",
    "    testSubset_total = [[],[],[],[],[],[],[],[],[],[]]\n",
    "    train_total = []\n",
    "    train_total_Cvd = []\n",
    "    train_total_NoCvd = []\n",
    "    \n",
    "    rowsNoCvd = len(dataNoCvd)    \n",
    "    rowsCvd = len(dataCvd)\n",
    "\n",
    "    fold_Cvd_total = rowsCvd//cv\n",
    "    fold_Cvd_residue= rowsCvd%cv\n",
    "    print(\"fold_Cvd_total  :\",fold_Cvd_total,\" fold_Cvd_residue  :\",fold_Cvd_residue)\n",
    "\n",
    "    fold_NoCvd_total = rowsNoCvd//cv\n",
    "    fold_NoCvd_residue= rowsNoCvd%cv\n",
    "    print(\"fold_NoCvd_total:\",fold_NoCvd_total,\"fold_NoCvd_residue:\",fold_NoCvd_residue)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "#     print(dataCvd)\n",
    "    cvd_temp = dataCvd\n",
    "    for i in range(rowsCvd//2):\n",
    "        neib = []\n",
    "        ind = []\n",
    "        neib,ind= get_neighbors(cvd_temp,cvd_temp[0],1)\n",
    "        index = ind[0]\n",
    "        testSubset_total[i%cv].append(cvd_temp[0].tolist())\n",
    "        testSubset_total[i%cv].append(cvd_temp[index].tolist())\n",
    "        cvd_temp = np.delete(cvd_temp, index,0)\n",
    "        cvd_temp = np.delete(cvd_temp, 0,0)\n",
    "#         print(len(cvd_temp))\n",
    "    \n",
    "#     for i in range(len(cvd_temp)):\n",
    "#         test_Cvd_total[cv-i-1].append(cvd_temp[0].tolist())\n",
    "    testSubset_total[9].append(cvd_temp[0].tolist())\n",
    "    \n",
    "    print(cvd_temp)    \n",
    "    \n",
    "    nocvd_temp = dataNoCvd\n",
    "    for i in range(rowsNoCvd//2):\n",
    "        neib = []\n",
    "        ind = []\n",
    "        neib,ind= get_neighbors(nocvd_temp,nocvd_temp[0],1)\n",
    "        index = ind[0]\n",
    "        testSubset_total[i%cv].append(nocvd_temp[0].tolist())\n",
    "        testSubset_total[i%cv].append(nocvd_temp[index].tolist())\n",
    "        nocvd_temp = np.delete(nocvd_temp, index,0)\n",
    "        nocvd_temp = np.delete(nocvd_temp, 0,0)\n",
    "#         print(len(nocvd_temp)) \n",
    "\n",
    "    testSubset_total[9].append(nocvd_temp[0].tolist())\n",
    "    \n",
    "    \n",
    "    for i in range(cv):\n",
    "        test = []\n",
    "        X_test = []\n",
    "#         X_test = np.array(X_test)\n",
    "#         print(type(X_test))\n",
    "        y_test = []\n",
    "        X_train = []\n",
    "        y_train = []\n",
    "        trainSubset_NoCvd = []\n",
    "        trainSubset_Cvd = []\n",
    "        s = 0\n",
    "        test = testSubset_total[i]\n",
    "        for j in range(len(data)):\n",
    "#             print(data[j])\n",
    "#             print(test)\n",
    "            if data[j].tolist() in test:\n",
    "                s+=1\n",
    "                testSubset = data[j]\n",
    "#                 print(type(X_test))\n",
    "                X_test_temp = np.delete(testSubset, 16, axis=0)\n",
    "                X_test.append(X_test_temp.tolist())\n",
    "#                 X_test.append(np.delete(testSubset, 16, axis=0))\n",
    "                y_test_temp1 = np.delete(testSubset, slice(0, 16), axis=0)\n",
    "#                 print(y_test_temp)\n",
    "                y_test_temp = np.reshape(y_test_temp1, len(y_test_temp1))\n",
    "#                 print(lala)\n",
    "                y_test.append(y_test_temp[0])\n",
    "            else:\n",
    "                testSubset = data[j]\n",
    "                if testSubset[16] == 1.0:\n",
    "                    trainSubset_Cvd.append(testSubset)\n",
    "                else:\n",
    "                    trainSubset_NoCvd.append(testSubset)\n",
    "#                 print(type(testSubset))\n",
    "                X_train_temp = np.delete(testSubset, 16, axis=0)\n",
    "#                 print(X_train_temp)\n",
    "                X_train.append(X_train_temp.tolist())\n",
    "                \n",
    "#                 X_test.append(np.delete(testSubset, 16, axis=0))\n",
    "                y_train_temp1 = np.delete(testSubset, slice(0, 16), axis=0)\n",
    "#                 print(y_test_temp)\n",
    "                y_train_temp = np.reshape(y_train_temp1, len(y_train_temp1))\n",
    "#                 print(lala)\n",
    "                y_train.append(y_train_temp[0])\n",
    "    \n",
    "        X_test = np.array(X_test)\n",
    "        y_test = np.array(y_test)\n",
    "        X_train = np.array(X_train)\n",
    "        y_train = np.array(y_train)\n",
    "        trainSubset_Cvd = np.array(trainSubset_Cvd)\n",
    "        trainSubset_NoCvd = np.array(trainSubset_NoCvd)\n",
    "        \n",
    "        test_total.append([X_test,y_test])\n",
    "        train_total.append([X_train,y_train]) \n",
    "        train_total_NoCvd.append(trainSubset_NoCvd)\n",
    "        train_total_Cvd.append(trainSubset_Cvd)\n",
    "        \n",
    "#     print(nocvd_temp)\n",
    "    \n",
    "    return test_total, train_total, train_total_NoCvd, train_total_Cvd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold_Cvd_total  : 4  fold_Cvd_residue  : 1\n",
      "fold_NoCvd_total: 51 fold_NoCvd_residue: 9\n",
      "[[  1.    66.    23.    25.21   2.     3.     1.     0.     2.     2.\n",
      "   50.     6.5  140.   240.    84.    50.     1.  ]]\n"
     ]
    }
   ],
   "source": [
    "test_total, train_total, train_total_NoCvd, train_total_Cvd = create_train_test_sets(10,dataNoCvd,dataCvd)\n",
    "# for i in range(10):\n",
    "#     print(len(test_total[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio in test set: 0.07142857142857142\n",
      "ratio in test set: 0.07142857142857142\n",
      "ratio in test set: 0.07142857142857142\n",
      "ratio in test set: 0.07142857142857142\n",
      "ratio in test set: 0.07142857142857142\n",
      "ratio in test set: 0.07142857142857142\n",
      "ratio in test set: 0.07142857142857142\n",
      "ratio in test set: 0.07142857142857142\n",
      "ratio in test set: 0.07142857142857142\n",
      "ratio in test set: 0.08928571428571429\n"
     ]
    }
   ],
   "source": [
    "# Chech ratio in each train and test set\n",
    "def find_ratio(index_list):\n",
    "    one = 0\n",
    "    lenght=len(index_list[0])\n",
    "    for i in range(lenght):\n",
    "#         print(index_list[1])\n",
    "        if index_list[1][i] == 1.0 :\n",
    "            one+= 1\n",
    "    ratio = one/lenght\n",
    "    return ratio\n",
    "\n",
    "for i in range(10):\n",
    "#     print(len(train_total[i][0]))\n",
    "#     ratio = find_ratio(train_total[i])\n",
    "#     print(\"ratio in train set:\", ratio)\n",
    "    ratio = find_ratio(test_total[i])\n",
    "    print(\"ratio in test set:\", ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio in test set: 0.07341269841269842\n",
      "ratio in test set: 0.07341269841269842\n",
      "ratio in test set: 0.07341269841269842\n",
      "ratio in test set: 0.07341269841269842\n",
      "ratio in test set: 0.07341269841269842\n",
      "ratio in test set: 0.07341269841269842\n",
      "ratio in test set: 0.07341269841269842\n",
      "ratio in test set: 0.07341269841269842\n",
      "ratio in test set: 0.07341269841269842\n",
      "ratio in test set: 0.07142857142857142\n"
     ]
    }
   ],
   "source": [
    "# Chech ratio in each train and test set\n",
    "def find_ratio(index_list):\n",
    "    one = 0\n",
    "    lenght=len(index_list[0])\n",
    "    for i in range(lenght):\n",
    "#         print(index_list[1])\n",
    "        if index_list[1][i] == 1.0 :\n",
    "            one+= 1\n",
    "    ratio = one/lenght\n",
    "    return ratio\n",
    "\n",
    "for i in range(10):\n",
    "#     print(len(train_total[i][0]))\n",
    "#     ratio = find_ratio(train_total[i])\n",
    "#     print(\"ratio in train set:\", ratio)\n",
    "    ratio = find_ratio(train_total[i])\n",
    "    print(\"ratio in test set:\", ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "def my_fit(ratio,dataNoCvd,dataCvd,params):   \n",
    "    Subarray=[]\n",
    "    models = []\n",
    "    # yarray\n",
    "    \n",
    "    rowsNoCvd = len(dataNoCvd)    \n",
    "    rowsCvd = len(dataCvd)\n",
    "#     print(\"Size of NoCvd cases in train set:\",rowsNoCvd)\n",
    "#     print(\"Size of Cvd cases in train set  :\",rowsCvd)\n",
    "\n",
    "\n",
    "    numOfSubsamples = rowsNoCvd//(rowsCvd*ratio)\n",
    "    numOfSubsamples= int(numOfSubsamples)\n",
    "    SubNoCvd = rowsNoCvd//numOfSubsamples\n",
    "    residue = rowsNoCvd- SubNoCvd*numOfSubsamples\n",
    "    \n",
    "#     print(numOfSubsamples)\n",
    "\n",
    "    Up = 0\n",
    "\n",
    "\n",
    "    for i in range(numOfSubsamples):\n",
    "        \n",
    "        classifier = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=params[5],\n",
    "                  colsample_bynode=1, colsample_bytree=1, gamma=params[4],\n",
    "                  learning_rate=params[0], max_delta_step=0, max_depth=params[1],\n",
    "                  min_child_weight=params[3], missing=None, n_estimators=params[2], n_jobs=1,\n",
    "                  nthread=None, objective='binary:logistic', random_state=0,\n",
    "                  reg_alpha=0, reg_lambda=params[6], scale_pos_weight=params[7], seed=None,\n",
    "                  silent=None, subsample=1.0, verbosity=1)\n",
    "    #     print(i)\n",
    "    #Creating the training set for each model\n",
    "        Down = Up\n",
    "        Up= Up + SubNoCvd\n",
    "        if i < residue :\n",
    "            Up+= 1\n",
    "        \n",
    "#         print(Down)\n",
    "#         print(Up)\n",
    "        \n",
    "        Sub1=dataNoCvd[Down:Up,:]\n",
    "        Sub2=np.concatenate((Sub1, dataCvd))\n",
    "        Subarray.append(Sub2)\n",
    "#         print(\"Size of subsample\",i+1,\":\",len(Subarray[i]))\n",
    "    #Dividing to X and y of the previous traing set    \n",
    "        X=np.delete(Subarray[i], 16, axis=1)\n",
    "        y=np.delete(Subarray[i], slice(0, 16), axis=1)\n",
    "        y=np.reshape(y, len(y))\n",
    "        \n",
    "        classifier.fit( X, y)\n",
    "        models.append(classifier)\n",
    "    return models     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "my_fit() missing 1 required positional argument: 'params'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-39d4d036918f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmy_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdataNoCvd\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdataCvd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: my_fit() missing 1 required positional argument: 'params'"
     ]
    }
   ],
   "source": [
    "models = my_fit(2,dataNoCvd,dataCvd)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def voting(models,X):\n",
    "    y_pred = []\n",
    "    y_pred_models = []\n",
    "\n",
    "    models_size = len(models)\n",
    "    X_size = len(X)\n",
    "            \n",
    "    for i in range(models_size):\n",
    "        y_pred_models.append(models[i].predict(X))\n",
    "        \n",
    "    for j in range(X_size):\n",
    "        case = 0\n",
    "        for i in range(models_size):\n",
    "            if y_pred_models[i][j] == 1.0:\n",
    "                case+=1\n",
    "        if case >  models_size/2 :\n",
    "            y_pred.append(1.) \n",
    "        else:\n",
    "            y_pred.append(0.) \n",
    "            \n",
    "    return y_pred        \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/train-test-split-and-cross-validation-in-python-80b61beca4b6\n",
    "\n",
    "from sklearn.metrics import  confusion_matrix,roc_curve, roc_auc_score, accuracy_score\n",
    "from imxgboost.imbalance_xgb import imbalance_xgboost as imb_xgb\n",
    "from sklearn.model_selection import StratifiedKFold # import KFold\n",
    "from sklearn import metrics\n",
    "\n",
    "def my_cross_val( cv, train_total, test_total, train_total_NoCvd, train_total_Cvd,params):\n",
    "    accuracy = []\n",
    "    specificity = []\n",
    "    sensitivity = []\n",
    "    auc = []\n",
    "    \n",
    "    # print(X)\n",
    "    for i in range(cv):\n",
    "#         X_train = train_total[i][0]\n",
    "#         y_train = train_total[i][1]\n",
    "        X_test = test_total[i][0]\n",
    "        y_test = test_total[i][1]\n",
    "        \n",
    "\n",
    "\n",
    "        models = my_fit( 3, train_total_NoCvd[i], train_total_Cvd[i],params)\n",
    "\n",
    "\n",
    "        y_pred = voting(models,X_test)\n",
    "\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "    #     print(cm)\n",
    "\n",
    "        total1=sum(sum(cm))\n",
    "        #####from confusion matrix calculate accuracy\n",
    "        accuracy1=(cm[0,0]+cm[1,1])/total1\n",
    "    #   print ('Accuracy : ', accuracy1)\n",
    "\n",
    "        specificity1 = cm[0,0]/(cm[0,0]+cm[0,1])\n",
    "    #   print('Specificity : ', specificity1 )\n",
    "\n",
    "        sensitivity1 = cm[1,1]/(cm[1,0]+cm[1,1])\n",
    "    #   print('Sensitivity : ', sensitivity1)\n",
    "        \n",
    "#         y = np.array(y_test)\n",
    "#         pred = np.array(y_pred)\n",
    "#         fpr, tpr, thresholds = metrics.roc_curve(y, pred, pos_label=2)\n",
    "#         auc1 = metrics.auc(fpr, tpr)\n",
    "        \n",
    "        auc1 = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "    \n",
    "        accuracy.append(accuracy1)\n",
    "        specificity.append(specificity1)\n",
    "        sensitivity.append(sensitivity1) \n",
    "        auc.append(auc1)\n",
    "    return accuracy, specificity, sensitivity, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "                    \"learning_rate\"    :[ 0.3],  #0.01-0.2 Makes the model more robust by shrinking the weights on each step\n",
    "                    \"max_depth\"        :[2],  #3-10 control over-fitting as higher depth will allow model to learn relations very specific to a particular sample\n",
    "                    \"n_estimators\"     :[200],\n",
    "                     \"subsample\"       :[1.0],         #0.5-1 small values might lead to under-fitting\n",
    "                    \"gamma\"            :[1],            #Makes the algorithm conservative --> No overfitting\n",
    "                    \"colsample_bytree\" :[0.75], #0.5-1\n",
    "                     \"reg_lambda\"      :[2], #  it should be explored to reduce overfitting.\n",
    "                    \"scale_pos_weight\" :[3]\n",
    "} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1, 1, 100, 3, 0, 0.5, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "comb = compute_comb(params)\n",
    "print(comb[0])\n",
    "accuracy, specificity, sensitivity, auc = my_cross_val( 10, train_total, test_total, train_total_NoCvd, train_total_Cvd,comb[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6732142857142858\n",
      "0.689894419306184\n",
      "0.47\n",
      "0.579947209653092\n"
     ]
    }
   ],
   "source": [
    "from statistics import mean\n",
    "print(mean(accuracy))\n",
    "print(mean(specificity))\n",
    "print(mean(sensitivity))\n",
    "print(mean(auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.geeksforgeeks.org/python-all-possible-permutations-of-n-lists/\n",
    "\n",
    "def compute_comb(params):\n",
    "    \n",
    "    # initializing lists \n",
    "    list1 = params.get(\"learning_rate\")\n",
    "    list2 = params.get(\"max_depth\")\n",
    "    list3 = params.get(\"n_estimators\") \n",
    "    list4 = params.get(\"min_child_weight\")\n",
    "    list5 = params.get(\"gamma\")    \n",
    "    list6 = params.get(\"colsample_bytree\")\n",
    "    list7 = params.get(\"reg_lambda\") \n",
    "    list8 = params.get(\"scale_pos_weight\")\n",
    "\n",
    "#     # printing lists  \n",
    "#     print (\"The original lists are : \" + str(list1) +\n",
    "#                                    \" \" + str(list2) + \n",
    "#                                    \" \" + str(list3) + \n",
    "#                                    \" \" + str(list4) + \n",
    "#                                    \" \" + str(list5) + \n",
    "#                                    \" \" + str(list6) +\n",
    "#                                    \" \" + str(list7) +\n",
    "#                                    \" \" + str(list8)) \n",
    "\n",
    "    # using list comprehension  \n",
    "    # to compute all possible permutations \n",
    "    res = [[i, j, k, l, m, n, o, p] for i in list1  \n",
    "                                    for j in list2 \n",
    "                                    for k in list3 \n",
    "                                    for l in list4 \n",
    "                                    for m in list5 \n",
    "                                    for n in list6 \n",
    "                                    for o in list7\n",
    "                                    for p in list8] \n",
    "\n",
    "    # printing result \n",
    "#     print (\"All possible permutations are : \" +  str(res))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class style:\n",
    "    BOLD = '\\033[1m'\n",
    "    END = '\\033[0m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/train-test-split-and-cross-validation-in-python-80b61beca4b6\n",
    "\n",
    "from sklearn.metrics import  confusion_matrix,roc_curve, roc_auc_score, accuracy_score\n",
    "from imxgboost.imbalance_xgb import imbalance_xgboost as imb_xgb\n",
    "from sklearn.model_selection import StratifiedKFold # import KFold\n",
    "from statistics import mean\n",
    "\n",
    "def gridsearch(params):\n",
    "    max_sens = 0\n",
    "    max_spes = 0\n",
    "    max_acc = 0\n",
    "    best_params = []\n",
    "    max_params = []\n",
    "    max_auc = 0\n",
    "    combs = compute_comb(params)\n",
    "    print(len(combs))\n",
    "    for j in range(len(combs)):\n",
    "        print(j)\n",
    "        accuracy, specificity, sensitivity, auc = my_cross_val( 10, train_total, test_total, train_total_NoCvd, train_total_Cvd,combs[j])\n",
    "        if mean(sensitivity)> 0.5 and mean(specificity)>0.6 and mean(accuracy)>0.6 and mean(auc)>0.5:\n",
    "            print(style.BOLD + \"----------------------------------------------------------------\"+ style.END)\n",
    "            print(combs[j])\n",
    "            print (style.BOLD + 'accuracy    ' + style.END, mean(accuracy))\n",
    "            print (style.BOLD + 'specificity ' + style.END, mean(specificity))\n",
    "            print (style.BOLD + 'sensitivity ' + style.END, mean(sensitivity))\n",
    "            print (style.BOLD + 'auc         ' + style.END, mean(auc))\n",
    "            print(style.BOLD + \"----------------------------------------------------------------\"+ style.END)\n",
    "        else:\n",
    "            print(combs[j])\n",
    "            print(\"accuracy    \",mean(accuracy))\n",
    "            print(\"specificity \",mean(specificity))\n",
    "            print(\"sensitivity \",mean(sensitivity))\n",
    "            print(\"auc         \",mean(auc))\n",
    "        best_params.append([combs[j],mean(accuracy),mean(specificity),mean(sensitivity),mean(auc)])\n",
    "        if mean(auc)> max_auc:\n",
    "            max_sens = mean(sensitivity)\n",
    "            max_spes = mean(specificity)\n",
    "            max_acc = mean(accuracy)\n",
    "            max_auc = mean(auc)\n",
    "            max_params = combs[j]\n",
    "    return max_sens, max_params, max_spes, max_acc, max_auc, best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ratio = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters : [0.4, 3, 1000, 1, 1, 0.5, 1.5, 3, 0.5, 0.1, 1.0]\n",
    "accuracy : 0.6910714285714286\n",
    "spes     : 0.68789592760181\n",
    "sens     : 0.74\n",
    "auc      : 0.713947963800905\n",
    "[13, 15, 11, 2, 3]\n",
    "[13, 15, 11, 2, 3, 1, 14, 12, 10, 5, 6, 8, 4, 9, 0, 7, 16]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
