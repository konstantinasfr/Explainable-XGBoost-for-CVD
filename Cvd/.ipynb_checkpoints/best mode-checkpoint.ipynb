{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [0.4, 2, 100, 2, 0.5, 0.5, 1, 4, 0.75, 0.1, 0.8]\n",
    "# 0.6964285714285714\n",
    "# 0.6976998491704374\n",
    "# 0.685\n",
    "# 0.6913499245852187"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.13652275379229872+0.1983663943990665+0.17036172695449242+0.1586931155192532+0.16569428238039674+0.17036172695449242"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1.2e+02*1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.18400932050314464"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-1.04491007e+00*0.1761006289308176\n",
    "-0.1840093210808136"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.03390685411320755"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-1.92542493e-01*0.1761006289308176\n",
    "0.03390685417367227"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "560\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import numpy\n",
    "numpy.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "# Importing the dataset\n",
    "# dataset = pd.read_csv('CVD dataset-1-3-7-8-10-11-13-15.csv')\n",
    "# dataset = pd.read_csv('CVD-dataset-1-3-7-8-10-11-12-15.csv')\n",
    "dataset = pd.read_csv('CVD-dataset-plin-4-7.csv')\n",
    "# dataset = pd.read_csv('CVD-dataset-1-2-3-10-11-13-14-15.csv') #weight+vot\n",
    "# dataset = pd.read_csv('CVD dataset-1- 2-3-11-12-13-14-15.csv') #weight+aver\n",
    "# dataset = pd.read_csv('CVD dataset-0-1-2-4-8-9-10-15.csv') #gain+vot\n",
    "# dataset = pd.read_csv('CVD-dataset-1-2-3-7-8-10-13-15.csv') #gain+aver\n",
    "data=dataset.iloc[:, :].values\n",
    "X = dataset.iloc[:, :-1].values\n",
    "y = dataset.iloc[:, 14].values\n",
    "# X = dataset.iloc[:, [ 0, 1, 2, 5, 10, 11, 13, 15]].values\n",
    "\n",
    "rows = len(data)    # 3 rows in your example\n",
    "cols = len(data[0])\n",
    "print(rows)\n",
    "print(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "519\n",
      "41\n"
     ]
    }
   ],
   "source": [
    "dataNoCvd = data[~(data[:,14] > 0.0)]\n",
    "dataCvd = data[~(data[:,14] < 1.0)]\n",
    "rowsNoCvd = len(dataNoCvd)    \n",
    "rowsCvd = len(dataCvd)\n",
    "print(rowsNoCvd)\n",
    "print(rowsCvd)\n",
    "# print(dataNoCvd)\n",
    "# print(dataCvd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_NoCvd_limitsArray(i,DownNoCvd, UpNoCvd, fold_NoCvd_total, fold_NoCvd_residue):\n",
    "    if i>0 :\n",
    "        DownNoCvd = UpNoCvd\n",
    "        UpNoCvd= fold_NoCvd_total+UpNoCvd\n",
    "    if i < fold_NoCvd_residue :\n",
    "        UpNoCvd+= 1\n",
    "#     print(\"NoCvd limits, Down :\",DownNoCvd,\"Up \",UpNoCvd)\n",
    "    return   DownNoCvd, UpNoCvd   \n",
    " \n",
    "def find_Cvd_limitsArray(i,DownCvd, UpCvd, fold_Cvd_total, fold_Cvd_residue,cv):\n",
    "    if i>0 :\n",
    "        DownCvd = UpCvd\n",
    "        UpCvd= fold_Cvd_total+UpCvd\n",
    "    if i >= cv - fold_Cvd_residue :\n",
    "         UpCvd+= 1     \n",
    "#     print(\"Cvd limits, Down :\",DownCvd,\"Up \",UpCvd)\n",
    "    return   DownCvd, UpCvd  \n",
    "\n",
    "def find_testSubset(DownNoCvd,UpNoCvd,DownCvd,UpCvd):\n",
    "    temp1=dataNoCvd[DownNoCvd:UpNoCvd,:]\n",
    "    temp2=dataCvd[DownCvd:UpCvd,:]\n",
    "    temp3=np.concatenate((temp1, temp2))\n",
    "#     print(len(temp1),\"  \",len(temp2),\"  \",len(temp3))\n",
    "    return temp1, temp2, temp3\n",
    "\n",
    "def find_trainSubset(DownNoCvd,UpNoCvd,DownCvd,UpCvd):\n",
    "    temp1 = np.delete(dataNoCvd, slice(DownNoCvd, UpNoCvd), axis=0)\n",
    "    temp2 = np.delete(dataCvd, slice(DownCvd, UpCvd), axis=0)\n",
    "    temp3 = np.concatenate((temp1, temp2))\n",
    "#     print(len(temp1),\"  \",len(temp2),\"  \",len(temp3))\n",
    "    return temp1, temp2, temp3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_test_sets(cv,dataNoCvd,dataCvd):\n",
    "    test_total = []\n",
    "    train_total = []\n",
    "    train_total_Cvd = []\n",
    "    train_total_NoCvd = []\n",
    "    \n",
    "    rowsNoCvd = len(dataNoCvd)    \n",
    "    rowsCvd = len(dataCvd)\n",
    "\n",
    "    fold_Cvd_total = rowsCvd//cv\n",
    "    fold_Cvd_residue= rowsCvd%cv\n",
    "    print(\"fold_Cvd_total  :\",fold_Cvd_total,\" fold_Cvd_residue  :\",fold_Cvd_residue)\n",
    "\n",
    "    fold_NoCvd_total = rowsNoCvd//cv\n",
    "    fold_NoCvd_residue= rowsNoCvd%cv\n",
    "    print(\"fold_NoCvd_total:\",fold_NoCvd_total,\"fold_NoCvd_residue:\",fold_NoCvd_residue)\n",
    "\n",
    "    DownNoCvd=0\n",
    "    DownCvd=0\n",
    "    UpNoCvd = fold_NoCvd_total\n",
    "    UpCvd = fold_Cvd_total\n",
    "\n",
    "    for i in range(cv):\n",
    "        X_test = []\n",
    "        y_test = []\n",
    "        X_train = []\n",
    "        y_train = []\n",
    "#         print(\"============\",i,\"==============\")\n",
    "        DownNoCvd, UpNoCvd = find_NoCvd_limitsArray(i, DownNoCvd, UpNoCvd, fold_NoCvd_total, fold_NoCvd_residue)\n",
    "        DownCvd, UpCvd = find_Cvd_limitsArray(i, DownCvd, UpCvd, fold_Cvd_total, fold_Cvd_residue,cv)\n",
    "\n",
    "        testSubset_NoCvd ,testSubset_Cvd ,testSubset_total = find_testSubset(DownNoCvd,UpNoCvd,DownCvd,UpCvd)\n",
    "        trainSubset_NoCvd ,trainSubset_Cvd ,trainSubset_total = find_trainSubset(DownNoCvd,UpNoCvd,DownCvd,UpCvd)\n",
    "     \n",
    "    #creating X_train, y_train, X_test, y_test\n",
    "        X_test.append(np.delete(testSubset_total, 8, axis=1))\n",
    "        y_test_temp = np.delete(testSubset_total, slice(0, 8), axis=1)\n",
    "        y_test.append(np.reshape(y_test_temp, len(y_test_temp)))\n",
    "        X_test_temp = np.array(X_test)\n",
    "        X_test = X_test_temp[0]\n",
    "        y_test_temp = np.array(y_test)\n",
    "        y_test = y_test_temp[0]\n",
    "    \n",
    "        X_train.append(np.delete(trainSubset_total, 8, axis=1))\n",
    "        y_train_temp = np.delete(trainSubset_total, slice(0, 8), axis=1)\n",
    "        y_train.append(np.reshape(y_train_temp, len(y_train_temp)))\n",
    "        X_train_temp = np.array(X_train)\n",
    "        X_train = X_train_temp[0]\n",
    "        y_train_temp = np.array(y_train)\n",
    "        y_train = y_train_temp[0]\n",
    "        \n",
    "    #add every subset in a list so we can handle thm later \n",
    "        test_total.append([X_test,y_test])\n",
    "        train_total.append([X_train,y_train]) \n",
    "        train_total_NoCvd.append(trainSubset_NoCvd)\n",
    "        train_total_Cvd.append(trainSubset_Cvd)\n",
    "    return train_total, test_total, train_total_NoCvd, train_total_Cvd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold_Cvd_total  : 4  fold_Cvd_residue  : 1\n",
      "fold_NoCvd_total: 51 fold_NoCvd_residue: 9\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 392 into shape (56,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-114-913c4f1e388b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_total\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_total\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_total_NoCvd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_total_Cvd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_train_test_sets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdataNoCvd\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdataCvd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-113-72cd6e67d06e>\u001b[0m in \u001b[0;36mcreate_train_test_sets\u001b[0;34m(cv, dataNoCvd, dataCvd)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestSubset_total\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0my_test_temp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestSubset_total\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0my_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test_temp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test_temp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0mX_test_temp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_test_temp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mreshape\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mreshape\u001b[0;34m(a, newshape, order)\u001b[0m\n\u001b[1;32m    299\u001b[0m            [5, 6]])\n\u001b[1;32m    300\u001b[0m     \"\"\"\n\u001b[0;32m--> 301\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'reshape'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbound\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;31m# A TypeError occurs if the object does have such a method in its\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 392 into shape (56,)"
     ]
    }
   ],
   "source": [
    "train_total, test_total, train_total_NoCvd, train_total_Cvd = create_train_test_sets(10,dataNoCvd,dataCvd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chech ratio in each train and test set\n",
    "def find_ratio(index_list):\n",
    "    one = 0\n",
    "    lenght=len(index_list[0])\n",
    "    for i in range(lenght):\n",
    "#         print(index_list[1])\n",
    "        if index_list[1][i] == 1.0 :\n",
    "            one+= 1\n",
    "    ratio = one/lenght\n",
    "    return ratio\n",
    "\n",
    "for i in range(10):\n",
    "    print(len(train_total[i][0]))\n",
    "    ratio = find_ratio(train_total[i])\n",
    "    print(\"ratio in train set:\", ratio)\n",
    "    ratio = find_ratio(test_total[i])\n",
    "    print(\"ratio in test set:\", ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "def my_fit(ratio,dataNoCvd,dataCvd,params):   \n",
    "    Subarray=[]\n",
    "    models = []\n",
    "    # yarray\n",
    "    \n",
    "    rowsNoCvd = len(dataNoCvd)    \n",
    "    rowsCvd = len(dataCvd)\n",
    "#     print(\"Size of NoCvd cases in train set:\",rowsNoCvd)\n",
    "#     print(\"Size of Cvd cases in train set  :\",rowsCvd)\n",
    "\n",
    "\n",
    "    numOfSubsamples = rowsNoCvd//(rowsCvd*ratio)\n",
    "    numOfSubsamples= int(numOfSubsamples)\n",
    "    SubNoCvd = rowsNoCvd//numOfSubsamples\n",
    "    residue = rowsNoCvd- SubNoCvd*numOfSubsamples\n",
    "\n",
    "\n",
    "    Up = 0\n",
    "\n",
    "\n",
    "    for i in range(numOfSubsamples):\n",
    "        \n",
    "        classifier = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=params[5],\n",
    "                  colsample_bynode=1, colsample_bytree=params[8], gamma=params[4],\n",
    "                  learning_rate=params[0], max_delta_step=0, max_depth=params[1],\n",
    "                  min_child_weight=params[3], missing=None, n_estimators=params[2], n_jobs=1,\n",
    "                  nthread=None, objective='binary:logistic', random_state=0,\n",
    "                  reg_alpha=params[9], reg_lambda=params[6], scale_pos_weight=params[7], seed=None,\n",
    "                  silent=None, subsample=params[10], verbosity=1)\n",
    "    #     print(i)\n",
    "    #Creating the training set for each model\n",
    "        Down = Up\n",
    "        Up= Up + SubNoCvd\n",
    "        if i < residue :\n",
    "            Up+= 1\n",
    "        \n",
    "#         print(Down)\n",
    "#         print(Up)\n",
    "        \n",
    "        Sub1=dataNoCvd[Down:Up,:]\n",
    "        Sub2=np.concatenate((Sub1, dataCvd))\n",
    "        Subarray.append(Sub2)\n",
    "        \n",
    "    #Dividing to X and y of the previous traing set    \n",
    "        X=np.delete(Subarray[i], 14, axis=1)\n",
    "        y=np.delete(Subarray[i], slice(0, 14), axis=1)\n",
    "        y=np.reshape(y, len(y))\n",
    "        \n",
    "        classifier.fit( X, y)\n",
    "#         eval_set = [(X,y)]\n",
    "#         classifier.fit( X, y,early_stopping_rounds = 5,eval_metric=\"logloss\",eval_set=eval_set, verbose=True)\n",
    "        models.append(classifier)\n",
    "    return models     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def predict_unweighted_average(models,X):\n",
    "    y_pred = []\n",
    "    y_pred_models = []\n",
    "\n",
    "    models_size = len(models)\n",
    "    X_size = len(X)\n",
    "       \n",
    "    #βρίσκω και αποθηκευω τα predictions κάθε μοντέλου για το Χ_test όχι όμως με την μορφή (0,1) όπως θα έκανε η\n",
    "# predict αλλά με μορφή πιθανότητας να έρθει 1 με την predict_proba, δηλαδή τώρα ο πίνακας y_pred_models έχει τις \n",
    "# πιθανότητες κάθε περίπτωση του X_test να εμφανίσει Cvd\n",
    "    for i in range(models_size): \n",
    "        y_pred_models.append(models[i].predict_proba(X)[:,1]) \n",
    "\n",
    "#Παίρνω τις πιθανότητες κάθε μοντέλου και για κάθε μία περίπτωση του X_test κάνω το average των πιθανοτήτων αυτών\n",
    "# οπότε βγαίνει μια μέση πιθανότητα για κάθε μία περίπτωση του X_test.\n",
    "    for j in range(X_size):\n",
    "        sum = 0\n",
    "        for i in range(models_size):\n",
    "            sum = sum + y_pred_models[i][j]\n",
    "        avr = sum/models_size\n",
    "        if avr>= 0.5 :         #Όμως επειδή θεωρώ ότι το μοντέλο κάνει classification αν η μέση πιθανότητα \n",
    "            y_pred.append(1.)  # είναι μεγαλυτερη από 0.5 τότε θα επιστρέφει 1, δηλαδή ότι έχει cvd, αλλιώς 0\n",
    "        else:\n",
    "            y_pred.append(0.)\n",
    "    return y_pred "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/train-test-split-and-cross-validation-in-python-80b61beca4b6\n",
    "\n",
    "from sklearn.metrics import  confusion_matrix,roc_curve, roc_auc_score, accuracy_score\n",
    "from imxgboost.imbalance_xgb import imbalance_xgboost as imb_xgb\n",
    "from sklearn.model_selection import StratifiedKFold # import KFold\n",
    "from sklearn import metrics\n",
    "\n",
    "def my_cross_val( cv, train_total, test_total, train_total_NoCvd, train_total_Cvd,params):\n",
    "    accuracy = []\n",
    "    specificity = []\n",
    "    sensitivity = []\n",
    "    auc = []\n",
    "    \n",
    "    # print(X)\n",
    "    for i in range(cv):\n",
    "#         X_train = train_total[i][0]\n",
    "#         y_train = train_total[i][1]\n",
    "        X_test = test_total[i][0]\n",
    "        y_test = test_total[i][1]\n",
    "        \n",
    "\n",
    "        \n",
    "        models = my_fit( 2, train_total_NoCvd[i], train_total_Cvd[i],params)\n",
    "\n",
    "\n",
    "        y_pred = predict_unweighted_average(models,X_test)\n",
    "\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "    #     print(cm)\n",
    "\n",
    "        total1=sum(sum(cm))\n",
    "        #####from confusion matrix calculate accuracy\n",
    "        accuracy1=(cm[0,0]+cm[1,1])/total1\n",
    "    #   print ('Accuracy : ', accuracy1)\n",
    "\n",
    "        specificity1 = cm[0,0]/(cm[0,0]+cm[0,1])\n",
    "    #   print('Specificity : ', specificity1 )\n",
    "\n",
    "        sensitivity1 = cm[1,1]/(cm[1,0]+cm[1,1])\n",
    "    #   print('Sensitivity : ', sensitivity1)\n",
    "        \n",
    "#         y = np.array(y_test)\n",
    "#         pred = np.array(y_pred)\n",
    "#         fpr, tpr, thresholds = metrics.roc_curve(y, pred, pos_label=2)\n",
    "#         auc1 = metrics.auc(fpr, tpr)\n",
    "        \n",
    "        auc1 = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "    \n",
    "        accuracy.append(accuracy1)\n",
    "        specificity.append(specificity1)\n",
    "        sensitivity.append(sensitivity1) \n",
    "        auc.append(auc1)\n",
    "    return accuracy, specificity, sensitivity, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/        \n",
    "\n",
    "params = {\n",
    "                    \"learning_rate\"    :[0.4],  #0.01-0.2 Makes the model more robust by shrinking the weights on each step\n",
    "                    \"max_depth\"        :[2],  #3-10 control over-fitting as higher depth will allow model to learn relations very specific to a particular sample\n",
    "                    \"n_estimators\"     :[100],\n",
    "                    \"min_child_weight\" :[2],         #0.5-1 small values might lead to under-fitting\n",
    "                    \"gamma\"            :[0.5],            #Makes the algorithm conservative --> No overfitting\n",
    "                    \"colsample_bylevel\" :[0.5], #0.5-1\n",
    "                     \"reg_lambda\"      :[1], #  it should be explored to reduce overfitting.\n",
    "                    \"scale_pos_weight\" :[4],\n",
    "                    \"colsample_bytree\" :[0.75],\n",
    "                    \"reg_alpha\"        :[0.1],\n",
    "                    \"subsample\"        :[0.8]\n",
    "    \n",
    "        }\n",
    "# [0.4, 2, 100, 2, 0.5, 0.5, 1, 4, 0.75, 0.1, 0.8]\n",
    "# 0.6964285714285714\n",
    "# 0.6976998491704374\n",
    "# 0.685\n",
    "# 0.6913499245852187\n",
    "\n",
    "# gain-aver\n",
    "#  [[2.8236261372771025, 15], [2.5580499288255822, 13], [2.556403643206909, 1], [2.3482621568517854, 11], \n",
    "#   [2.310497939649767, 10], [2.302223741666667, 7], [2.25283855005, 8], [2.2489727393473724, 3]]\n",
    "# 1 3 7 8 10 11 13 15\n",
    "\n",
    "#weight-aver\n",
    "# [[25.666666666666668, 14], [24.666666666666668, 13], [22.666666666666668, 3], \n",
    "#  [21.5, 11], [21.0, 1], [20.0, 15], [17.333333333333332, 12], [15.666666666666666, 2]]\n",
    "# 1 2 3 11 12 13 14 15\n",
    "\n",
    "#gain-voting\n",
    "# [[5, 7], [5, 3], [4, 15], [4, 10], [3, 1], [3, 12], [3, 8], [3, 11]]\n",
    "# 1 3 7 8 10 11 12 15\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.4, 2, 100, 2, 0.5, 0.5, 1, 4, 0.75, 0.1, 0.8]\n"
     ]
    }
   ],
   "source": [
    "comb = compute_comb(params)\n",
    "print(comb[0])\n",
    "accuracy, specificity, sensitivity, auc = my_cross_val( 10, train_total, test_total, train_total_NoCvd, train_total_Cvd,comb[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6803571428571429\n",
      "0.691892911010558\n",
      "0.54\n",
      "0.615946455505279\n"
     ]
    }
   ],
   "source": [
    "from statistics import mean\n",
    "print(mean(accuracy))\n",
    "print(mean(specificity))\n",
    "print(mean(sensitivity))\n",
    "print(mean(auc))\n",
    "#χωρίς 4 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6357142857142857\n",
      "0.6380090497737556\n",
      "0.615\n",
      "0.6265045248868778\n"
     ]
    }
   ],
   "source": [
    "from statistics import mean\n",
    "print(mean(accuracy))\n",
    "print(mean(specificity))\n",
    "print(mean(sensitivity))\n",
    "print(mean(auc))\n",
    "# Αποτελέσματα για gain-voting:\n",
    "# 0 1 2 4 8 9 10 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6607142857142857\n",
      "0.6648567119155354\n",
      "0.615\n",
      "0.6399283559577678\n"
     ]
    }
   ],
   "source": [
    "from statistics import mean\n",
    "print(mean(accuracy))\n",
    "print(mean(specificity))\n",
    "print(mean(sensitivity))\n",
    "print(mean(auc))\n",
    "# Αποτελέσματα για gain-aver:\n",
    "# 1 2 3 7 8 10 13 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6607142857142857\n",
      "0.6726998491704375\n",
      "0.515\n",
      "0.5938499245852187\n"
     ]
    }
   ],
   "source": [
    "from statistics import mean\n",
    "print(mean(accuracy))\n",
    "print(mean(specificity))\n",
    "print(mean(sensitivity))\n",
    "print(mean(auc))\n",
    "# Αποτελέσματα για weight+average:\n",
    "# 1 2 3 11 12 13 14 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6803571428571429\n",
      "0.691892911010558\n",
      "0.54\n",
      "0.615946455505279\n"
     ]
    }
   ],
   "source": [
    "from statistics import mean\n",
    "print(mean(accuracy))\n",
    "print(mean(specificity))\n",
    "print(mean(sensitivity))\n",
    "print(mean(auc))\n",
    "# Αποτελέσματα για weight+vot:\n",
    "# 1 2 3 10 11 13 14 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.geeksforgeeks.org/python-all-possible-permutations-of-n-lists/\n",
    "\n",
    "def compute_comb(params):\n",
    "    \n",
    "    # initializing lists \n",
    "    list1 = params.get(\"learning_rate\")\n",
    "    list2 = params.get(\"max_depth\")\n",
    "    list3 = params.get(\"n_estimators\") \n",
    "    list4 = params.get(\"min_child_weight\")\n",
    "    list5 = params.get(\"gamma\")    \n",
    "    list6 = params.get(\"colsample_bylevel\")\n",
    "    list7 = params.get(\"reg_lambda\") \n",
    "    list8 = params.get(\"scale_pos_weight\")\n",
    "    list9 = params.get(\"colsample_bytree\")\n",
    "    list10 = params.get(\"reg_alpha\")\n",
    "    list11 = params.get(\"subsample\")\n",
    "    \n",
    "\n",
    "#     # printing lists  \n",
    "#     print (\"The original lists are : \" + str(list1) +\n",
    "#                                    \" \" + str(list2) + \n",
    "#                                    \" \" + str(list3) + \n",
    "#                                    \" \" + str(list4) + \n",
    "#                                    \" \" + str(list5) + \n",
    "#                                    \" \" + str(list6) +\n",
    "#                                    \" \" + str(list7) +\n",
    "#                                    \" \" + str(list8)) \n",
    "\n",
    "    # using list comprehension  \n",
    "    # to compute all possible permutations \n",
    "    res = [[i, j, k, l, m, n, o, p,q,r,s] for i in list1  \n",
    "                                          for j in list2 \n",
    "                                          for k in list3 \n",
    "                                          for l in list4 \n",
    "                                          for m in list5 \n",
    "                                          for n in list6 \n",
    "                                          for o in list7\n",
    "                                          for p in list8\n",
    "                                          for q in list9\n",
    "                                          for r in list10\n",
    "                                          for s in list11] \n",
    "\n",
    "    # printing result \n",
    "#     print (\"All possible permutations are : \" +  str(res))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class style:\n",
    "    BOLD = '\\033[1m'\n",
    "    END = '\\033[0m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/train-test-split-and-cross-validation-in-python-80b61beca4b6\n",
    "\n",
    "from sklearn.metrics import  confusion_matrix,roc_curve, roc_auc_score, accuracy_score\n",
    "from imxgboost.imbalance_xgb import imbalance_xgboost as imb_xgb\n",
    "from sklearn.model_selection import StratifiedKFold # import KFold\n",
    "from statistics import mean\n",
    "\n",
    "def gridsearch(params):\n",
    "    max_sens = 0\n",
    "    max_spes = 0\n",
    "    max_acc = 0\n",
    "    best_params = []\n",
    "    max_params = []\n",
    "    max_auc = 0\n",
    "    combs = compute_comb(params)\n",
    "    print(len(combs))\n",
    "    for j in range(len(combs)):\n",
    "        print(j)\n",
    "        accuracy, specificity, sensitivity, auc = my_cross_val( 10, train_total, test_total, train_total_NoCvd, train_total_Cvd,combs[j])\n",
    "        if mean(sensitivity)> 0.5 and mean(specificity)>0.6 and mean(accuracy)>0.6 and mean(auc)>0.5:\n",
    "            print(style.BOLD + \"----------------------------------------------------------------\"+ style.END)\n",
    "            print(combs[j])\n",
    "            print (style.BOLD + 'accuracy    ' + style.END, mean(accuracy))\n",
    "            print (style.BOLD + 'specificity ' + style.END, mean(specificity))\n",
    "            print (style.BOLD + 'sensitivity ' + style.END, mean(sensitivity))\n",
    "            print (style.BOLD + 'auc         ' + style.END, mean(auc))\n",
    "            print(style.BOLD + \"----------------------------------------------------------------\"+ style.END)\n",
    "        else:\n",
    "            print(combs[j])\n",
    "            print(\"accuracy    \",mean(accuracy))\n",
    "            print(\"specificity \",mean(specificity))\n",
    "            print(\"sensitivity \",mean(sensitivity))\n",
    "            print(\"auc         \",mean(auc))\n",
    "        best_params.append([combs[j],mean(accuracy),mean(specificity),mean(sensitivity),mean(auc)])\n",
    "        if mean(auc)> max_auc:\n",
    "            max_sens = mean(sensitivity)\n",
    "            max_spes = mean(specificity)\n",
    "            max_acc = mean(accuracy)\n",
    "            max_auc = mean(auc)\n",
    "            max_params = combs[j]\n",
    "    return max_sens, max_params, max_spes, max_acc, max_auc, best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
