{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "560\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import numpy\n",
    "numpy.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "f_size = 5\n",
    "# Importing the dataset\n",
    "\n",
    "# dataset = pd.read_csv('CVD dataset2.csv')\n",
    "dataset = pd.read_csv('CVD-dataset-2-3-11-13-15.csv')\n",
    "data=dataset.iloc[:, :].values\n",
    "X = dataset.iloc[:, :-1].values\n",
    "y = dataset.iloc[:, f_size].values\n",
    "# X = dataset.iloc[:, [ 0, 1, 2, 5, 10, 11, 13, 15]].values\n",
    "\n",
    "rows = len(data)    # 3 rows in your example\n",
    "cols = len(data[0])\n",
    "print(rows)\n",
    "print(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "519\n",
      "41\n"
     ]
    }
   ],
   "source": [
    "dataNoCvd = data[~(data[:,f_size] > 0.0)]\n",
    "dataCvd = data[~(data[:,f_size] < 1.0)]\n",
    "rowsNoCvd = len(dataNoCvd)    \n",
    "rowsCvd = len(dataCvd)\n",
    "print(rowsNoCvd)\n",
    "print(rowsCvd)\n",
    "# print(dataNoCvd)\n",
    "# print(dataCvd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_NoCvd_limitsArray(i,DownNoCvd, UpNoCvd, fold_NoCvd_total, fold_NoCvd_residue):\n",
    "    if i>0 :\n",
    "        DownNoCvd = UpNoCvd\n",
    "        UpNoCvd= fold_NoCvd_total+UpNoCvd\n",
    "    if i < fold_NoCvd_residue :\n",
    "        UpNoCvd+= 1\n",
    "#     print(\"NoCvd limits, Down :\",DownNoCvd,\"Up \",UpNoCvd)\n",
    "    return   DownNoCvd, UpNoCvd   \n",
    " \n",
    "def find_Cvd_limitsArray(i,DownCvd, UpCvd, fold_Cvd_total, fold_Cvd_residue,cv):\n",
    "    if i>0 :\n",
    "        DownCvd = UpCvd\n",
    "        UpCvd= fold_Cvd_total+UpCvd\n",
    "    if i >= cv - fold_Cvd_residue :\n",
    "         UpCvd+= 1     \n",
    "#     print(\"Cvd limits, Down :\",DownCvd,\"Up \",UpCvd)\n",
    "    return   DownCvd, UpCvd  \n",
    "\n",
    "def find_testValSubset(DownNoCvd,UpNoCvd,DownCvd,UpCvd,dataNoCvd,dataCvd):\n",
    "    temp1=dataNoCvd[DownNoCvd:UpNoCvd,:]\n",
    "    temp2=dataCvd[DownCvd:UpCvd,:]\n",
    "    temp3=np.concatenate((temp1, temp2))\n",
    "#     print(len(temp1),\"  \",len(temp2),\"  \",len(temp3))\n",
    "    return temp3\n",
    "\n",
    "def find_trainSubset(DownNoCvd,UpNoCvd,DownCvd,UpCvd,dataNoCvd,dataCvd):\n",
    "    temp1 = np.delete(dataNoCvd, slice(DownNoCvd, UpNoCvd), axis=0)\n",
    "    temp2 = np.delete(dataCvd, slice(DownCvd, UpCvd), axis=0)\n",
    "    temp3 = np.concatenate((temp1, temp2))\n",
    "#     print(len(temp1),\"  \",len(temp2),\"  \",len(temp3))\n",
    "    return temp1, temp2, temp3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_test_sets(f_size,cv,dataNoCvd,dataCvd,val_ratio):\n",
    "    test_total = []\n",
    "    train_total = []\n",
    "    train_total_Cvd = []\n",
    "    train_total_NoCvd = []\n",
    "    validation_total = []\n",
    "    \n",
    "    rowsNoCvd = len(dataNoCvd)    \n",
    "    rowsCvd = len(dataCvd)\n",
    "\n",
    "    fold_Cvd_total = rowsCvd//cv\n",
    "    fold_Cvd_residue= rowsCvd%cv\n",
    "#     print(\"fold_Cvd_total  :\",fold_Cvd_total,\" fold_Cvd_residue  :\",fold_Cvd_residue)\n",
    "\n",
    "    fold_NoCvd_total = rowsNoCvd//cv\n",
    "    fold_NoCvd_residue= rowsNoCvd%cv\n",
    "#     print(\"fold_NoCvd_total:\",fold_NoCvd_total,\"fold_NoCvd_residue:\",fold_NoCvd_residue)\n",
    "    \n",
    "    Cvd_val = round(504*val_ratio*(rowsCvd/len(data)))\n",
    "    noCvd_val = round(504*val_ratio)-Cvd_val\n",
    "#     print(noCvd_val)\n",
    "    \n",
    "    DownNoCvd=0\n",
    "    DownCvd=0\n",
    "    UpNoCvd = fold_NoCvd_total\n",
    "    UpCvd = fold_Cvd_total\n",
    "\n",
    "    for i in range(cv):\n",
    "        X_test = []\n",
    "        y_test = []\n",
    "        X_train = []\n",
    "        y_train = []\n",
    "        X_val = []\n",
    "        y_val = []\n",
    "#         print(\"============\",i,\"==============\")\n",
    "        DownNoCvd, UpNoCvd = find_NoCvd_limitsArray(i, DownNoCvd, UpNoCvd, fold_NoCvd_total, fold_NoCvd_residue)\n",
    "        DownCvd, UpCvd = find_Cvd_limitsArray(i, DownCvd, UpCvd, fold_Cvd_total, fold_Cvd_residue,cv)\n",
    "#         print(DownNoCvd,UpNoCvd,DownCvd,UpCvd)\n",
    "        testSubset_total = find_testValSubset(DownNoCvd,UpNoCvd,DownCvd,UpCvd,dataNoCvd,dataCvd)\n",
    "        if i!=9:\n",
    "#             print(UpNoCvd,UpNoCvd+noCvd_val,UpCvd,UpCvd+Cvd_val)\n",
    "            validationSubset_total = find_testValSubset(UpNoCvd,UpNoCvd+noCvd_val,UpCvd,UpCvd+Cvd_val,dataNoCvd,dataCvd)\n",
    "#             print(DownNoCvd,UpNoCvd+noCvd_val,DownCvd,UpCvd+Cvd_val)\n",
    "            trainSubset_NoCvd ,trainSubset_Cvd ,trainSubset_total = find_trainSubset(DownNoCvd,UpNoCvd+noCvd_val,DownCvd,UpCvd+Cvd_val,dataNoCvd,dataCvd)\n",
    "        else:\n",
    "#             print(DownNoCvd-noCvd_val,DownNoCvd,DownCvd-Cvd_val,DownCvd)\n",
    "            validationSubset_total = find_testValSubset(DownNoCvd-noCvd_val,DownNoCvd,DownCvd-Cvd_val,DownCvd,dataNoCvd,dataCvd)\n",
    "#             print(DownNoCvd-noCvd_val,UpNoCvd,DownCvd-Cvd_val,UpCvd)\n",
    "            trainSubset_NoCvd ,trainSubset_Cvd ,trainSubset_total = find_trainSubset(DownNoCvd-noCvd_val,UpNoCvd,DownCvd-Cvd_val,UpCvd,dataNoCvd,dataCvd)\n",
    "    #creating X_train, y_train, X_test, y_test\n",
    "        X_test.append(np.delete(testSubset_total, f_size, axis=1))\n",
    "        y_test_temp = np.delete(testSubset_total, slice(0, f_size), axis=1)\n",
    "        y_test.append(np.reshape(y_test_temp, len(y_test_temp)))\n",
    "        X_test_temp = np.array(X_test)\n",
    "        X_test = X_test_temp[0]\n",
    "        y_test_temp = np.array(y_test)\n",
    "        y_test = y_test_temp[0]\n",
    "        \n",
    "        X_val.append(np.delete(validationSubset_total, f_size, axis=1))\n",
    "        y_val_temp = np.delete(validationSubset_total, slice(0, f_size), axis=1)\n",
    "        y_val.append(np.reshape(y_val_temp, len(y_val_temp)))\n",
    "        X_val_temp = np.array(X_val)\n",
    "        X_val = X_val_temp[0]\n",
    "        y_val_temp = np.array(y_val)\n",
    "        y_val = y_val_temp[0]\n",
    "        \n",
    "        X_train.append(np.delete(trainSubset_total, f_size, axis=1))\n",
    "        y_train_temp = np.delete(trainSubset_total, slice(0, f_size), axis=1)\n",
    "        y_train.append(np.reshape(y_train_temp, len(y_train_temp)))\n",
    "        X_train_temp = np.array(X_train)\n",
    "        X_train = X_train_temp[0]\n",
    "        y_train_temp = np.array(y_train)\n",
    "        y_train = y_train_temp[0]\n",
    "        \n",
    "    #add every subset in a list so we can handle thm later \n",
    "        test_total.append([X_test,y_test])\n",
    "        validation_total.append([X_val,y_val])\n",
    "        train_total.append([X_train,y_train]) \n",
    "        train_total_NoCvd.append(trainSubset_NoCvd)\n",
    "        train_total_Cvd.append(trainSubset_Cvd)\n",
    "    return train_total, test_total, validation_total, train_total_NoCvd, train_total_Cvd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_total, test_total, validation_total, train_total_NoCvd, train_total_Cvd = create_train_test_sets(f_size,10,dataNoCvd,dataCvd,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "454\n",
      "ratio in train set: 0.07268722466960352\n",
      "ratio in test set: 0.07142857142857142\n",
      "454\n",
      "ratio in train set: 0.07268722466960352\n",
      "ratio in test set: 0.07142857142857142\n",
      "454\n",
      "ratio in train set: 0.07268722466960352\n",
      "ratio in test set: 0.07142857142857142\n",
      "454\n",
      "ratio in train set: 0.07268722466960352\n",
      "ratio in test set: 0.07142857142857142\n",
      "454\n",
      "ratio in train set: 0.07268722466960352\n",
      "ratio in test set: 0.07142857142857142\n",
      "454\n",
      "ratio in train set: 0.07268722466960352\n",
      "ratio in test set: 0.07142857142857142\n",
      "454\n",
      "ratio in train set: 0.07268722466960352\n",
      "ratio in test set: 0.07142857142857142\n",
      "454\n",
      "ratio in train set: 0.07268722466960352\n",
      "ratio in test set: 0.07142857142857142\n",
      "454\n",
      "ratio in train set: 0.07268722466960352\n",
      "ratio in test set: 0.07142857142857142\n",
      "454\n",
      "ratio in train set: 0.07048458149779736\n",
      "ratio in test set: 0.08928571428571429\n"
     ]
    }
   ],
   "source": [
    "# Chech ratio in each train and test set\n",
    "def find_ratio(index_list):\n",
    "    one = 0\n",
    "    lenght=len(index_list[0])\n",
    "    for i in range(lenght):\n",
    "#         print(index_list[1])\n",
    "        if index_list[1][i] == 1.0 :\n",
    "            one+= 1\n",
    "    ratio = one/lenght\n",
    "    return ratio\n",
    "\n",
    "for i in range(10):\n",
    "    print(len(train_total[i][0]))\n",
    "    ratio = find_ratio(train_total[i])\n",
    "    print(\"ratio in train set:\", ratio)\n",
    "    ratio = find_ratio(test_total[i])\n",
    "    print(\"ratio in test set:\", ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import  confusion_matrix,roc_curve, roc_auc_score, accuracy_score\n",
    "from statistics import mean\n",
    "import pandas as pd, numpy as np\n",
    "import xgboost as xgb\n",
    "from hyperopt import hp, fmin, tpe, STATUS_OK, Trials \n",
    "\n",
    "total = []\n",
    "\n",
    "def objective(space):\n",
    "\n",
    "    clf = xgb.XGBClassifier(n_estimators = 10000,\n",
    "                            learning_rate = space['learning_rate'],\n",
    "                            max_depth = int(space['max_depth']),\n",
    "                            min_child_weight = int(space['min_child_weight']),\n",
    "                            subsample = space['subsample'],\n",
    "                            gamma = space['gamma'],\n",
    "                            colsample_bytree = space['colsample_bytree'])\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    accuracy, specificity, sensitivity, auc = my_cross_val( 10, train_total, test_total, validation_total, train_total_NoCvd, train_total_Cvd,clf)\n",
    "\n",
    "#     print(\"accuraccy:    \", mean(accuracy))\n",
    "#     print(\"specificity:  \", mean(specificity))\n",
    "#     print(\"sensitivity:  \", mean(sensitivity))\n",
    "#     print(\"auc        :  \", mean(auc))\n",
    "    total.append([mean(accuracy),mean(specificity),mean(sensitivity),mean(auc)])\n",
    "#     return np.mean([roc_auc_score(X, y),roc_auc_score(X, y)])\n",
    "#     return np.mean(cross_val_score(clf, X, y, cv=3, scoring='roc_auc'))\n",
    "    return{'loss':1-mean(auc), 'status': STATUS_OK , 'auc':mean(auc), 'accuracy':mean(accuracy)}\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], [], [], [], []]\n"
     ]
    }
   ],
   "source": [
    "print(limits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "limits= [[],[],[],[],[],[]]\n",
    "\n",
    "def my_fit(ratio,dataNoCvd,dataCvd,validation_total,clf):   \n",
    "    Subarray=[]\n",
    "    models = []\n",
    "    \n",
    "    # yarray\n",
    "    \n",
    "    rowsNoCvd = len(dataNoCvd)    \n",
    "    rowsCvd = len(dataCvd)\n",
    "#     print(\"Size of NoCvd cases in train set:\",rowsNoCvd)\n",
    "#     print(\"Size of Cvd cases in train set  :\",rowsCvd)\n",
    "\n",
    "\n",
    "    numOfSubsamples = rowsNoCvd//(rowsCvd*ratio)\n",
    "    numOfSubsamples= int(numOfSubsamples)\n",
    "    SubNoCvd = rowsNoCvd//numOfSubsamples\n",
    "    residue = rowsNoCvd- SubNoCvd*numOfSubsamples\n",
    "\n",
    "#     print(SubNoCvd,residue)\n",
    "    Up = 0\n",
    "\n",
    "\n",
    "    for i in range(numOfSubsamples):\n",
    "        \n",
    "        \n",
    "#         classifier = XGBClassifier()\n",
    "    #     print(i)\n",
    "    #Creating the training set for each model\n",
    "        Down = Up\n",
    "        Up= Up + SubNoCvd\n",
    "        if i < residue :\n",
    "            Up+= 1\n",
    "        \n",
    "        limits[i].append([Down,Up])\n",
    "        \n",
    "#         print(Down)\n",
    "#         print(Up)\n",
    "        \n",
    "        Sub1=dataNoCvd[Down:Up,:]\n",
    "        Sub2=np.concatenate((Sub1, dataCvd))\n",
    "        Subarray.append(Sub2)\n",
    "        \n",
    "    #Dividing to X and y of the previous traing set    \n",
    "        X=np.delete(Subarray[i], f_size, axis=1)\n",
    "        y=np.delete(Subarray[i], slice(0, f_size), axis=1)\n",
    "        y=np.reshape(y, len(y))\n",
    "        \n",
    "        X_val = validation_total[i][0]\n",
    "        y_val = validation_total[i][1]\n",
    "        \n",
    "        eval_set  = [(X,y), (X_val,y_val)]\n",
    "        \n",
    "        clf.fit( X, y, eval_set=eval_set, eval_metric=\"auc\", early_stopping_rounds=50,verbose = False)\n",
    "        \n",
    "        models.append(clf)\n",
    "    return models     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def voting(models,X):\n",
    "    y_pred = []\n",
    "    y_pred_models = []\n",
    "\n",
    "    models_size = len(models)\n",
    "    X_size = len(X)\n",
    "            \n",
    "    for i in range(models_size):\n",
    "        y_pred_models.append(models[i].predict(X))\n",
    "        \n",
    "    for j in range(X_size):\n",
    "        case = 0\n",
    "        for i in range(models_size):\n",
    "            if y_pred_models[i][j] == 1.0:\n",
    "                case+=1\n",
    "        if case >  models_size/2 :\n",
    "            y_pred.append(1.) \n",
    "        else:\n",
    "            y_pred.append(0.) \n",
    "            \n",
    "    return y_pred  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/train-test-split-and-cross-validation-in-python-80b61beca4b6\n",
    "\n",
    "from sklearn.metrics import  confusion_matrix,roc_curve, roc_auc_score, accuracy_score\n",
    "from imxgboost.imbalance_xgb import imbalance_xgboost as imb_xgb\n",
    "from sklearn.model_selection import StratifiedKFold # import KFold\n",
    "from sklearn import metrics\n",
    "\n",
    "def my_cross_val( cv, train_total, test_total, validation_total, train_total_NoCvd, train_total_Cvd,clf):\n",
    "    accuracy = []\n",
    "    specificity = []\n",
    "    sensitivity = []\n",
    "    auc = []\n",
    "    \n",
    "    # print(X)\n",
    "    for i in range(cv):\n",
    "#         X_train = train_total[i][0]\n",
    "#         y_train = train_total[i][1]\n",
    "        X_test = test_total[i][0]\n",
    "        y_test = test_total[i][1]\n",
    "        \n",
    "\n",
    "\n",
    "        models = my_fit( 2, train_total_NoCvd[i], train_total_Cvd[i], validation_total, clf)\n",
    "\n",
    "\n",
    "        y_pred = voting(models,X_test)\n",
    "\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "    #     print(cm)\n",
    "\n",
    "        total1=sum(sum(cm))\n",
    "        #####from confusion matrix calculate accuracy\n",
    "        accuracy1=(cm[0,0]+cm[1,1])/total1\n",
    "    #   print ('Accuracy : ', accuracy1)\n",
    "\n",
    "        specificity1 = cm[0,0]/(cm[0,0]+cm[0,1])\n",
    "    #   print('Specificity : ', specificity1 )\n",
    "\n",
    "        sensitivity1 = cm[1,1]/(cm[1,0]+cm[1,1])\n",
    "    #   print('Sensitivity : ', sensitivity1)\n",
    "        \n",
    "#         y = np.array(y_test)\n",
    "#         pred = np.array(y_pred)\n",
    "#         fpr, tpr, thresholds = metrics.roc_curve(y, pred, pos_label=2)\n",
    "#         auc1 = metrics.auc(fpr, tpr)\n",
    "        \n",
    "        auc1 = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "    \n",
    "        accuracy.append(accuracy1)\n",
    "        specificity.append(specificity1)\n",
    "        sensitivity.append(sensitivity1) \n",
    "        auc.append(auc1)\n",
    "    return accuracy, specificity, sensitivity, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:22<00:00,  1.21trial/s, best loss: 0.42736048265460025]\n",
      "{'colsample_bylevel': 0.5010830728307635, 'colsample_bytree': 0.5051687700773558, 'gamma': 0.9918505030433837, 'learning_rate': 0.40037932276157917, 'max_depth': 2.0, 'min_child_weight': 1.0, 'reg_alpha': 0.10456050352151675, 'reg_lambda': 1.5004830189952083, 'scale_pos_weight': 3.0, 'subsample': 0.9937952579476403}\n"
     ]
    }
   ],
   "source": [
    "# https://gist.github.com/walterreade/6e20dba959277bd9af77\n",
    "\n",
    "from hyperopt import hp, fmin, tpe, STATUS_OK, Trials \n",
    "\n",
    "space ={\n",
    "        'learning_rate':hp.uniform ('learning_rate', 0.4, 0.41),\n",
    "        'max_depth': hp.quniform(\"max_depth\", 2, 3, 1),\n",
    "        'min_child_weight': hp.quniform('min_child_weight', 1, 2, 1),\n",
    "#         'subsample': hp.quniform('subsample', 0.5, 1, 0.05),\n",
    "        'gamma': hp.uniform('gamma', 0.99, 1),\n",
    "        'colsample_bylevel':hp.uniform('colsample_bylevel', 0.5, 0.51),\n",
    "        'reg_lambda':hp.uniform('reg_lambda', 1.5, 1.51),\n",
    "        'scale_pos_weight':hp.quniform(\"scale_pos_weight\", 2, 3, 1),\n",
    "        'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 0.51),\n",
    "        'reg_alpha':hp.uniform('reg_alpha', 0.1, 0.11),\n",
    "        'subsample': hp.uniform ('subsample', 0.8, 1)\n",
    "    }      \n",
    "\n",
    "\n",
    "# parameters : [0.4, 3, 1000, 1, 1, 0.5, 1.5, 3, 0.5, 0.1, 1.0]\n",
    "# accuracy : 0.6910714285714286\n",
    "# spes     : 0.68789592760181\n",
    "# sens     : 0.74\n",
    "# auc      : 0.713947963800905\n",
    "# [13, 15, 11, 2, 3]\n",
    "# [13, 15, 11, 2, 3, 1, 14, 12, 10, 5, 6, 8, 4, 9, 0, 7, 16]\n",
    "    \n",
    "#         }\n",
    "# maxdepth': hp.choice(\"xmax_depth\",np.arange(5, 30, dtype=int) ),\n",
    "\n",
    "trials = Trials()\n",
    "best = fmin(fn=objective,\n",
    "            space=space,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=100,\n",
    "            trials=trials)\n",
    "\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.7303571428571428, 0.7688914027149321, 0.24, 0.504445701357466], [0.7053571428571429, 0.7284690799396681, 0.415, 0.5717345399698341], [0.7125, 0.7418552036199095, 0.34, 0.5409276018099547], [0.6910714285714286, 0.7187028657616893, 0.34, 0.5293514328808446], [0.6767857142857143, 0.7110482654600302, 0.24, 0.4755241327300151], [0.6928571428571428, 0.7187028657616893, 0.365, 0.5418514328808447], [0.7035714285714285, 0.7302790346907994, 0.36, 0.5451395173453997], [0.6946428571428571, 0.7206259426847662, 0.365, 0.542812971342383], [0.7303571428571429, 0.761236802413273, 0.34, 0.5506184012066365], [0.7607142857142857, 0.8015082956259427, 0.24, 0.5207541478129714], [0.6946428571428571, 0.7128205128205128, 0.465, 0.5889102564102564], [0.7285714285714285, 0.7707767722473605, 0.19, 0.48038838612368023], [0.6910714285714286, 0.7168174962292609, 0.365, 0.5409087481146304], [0.7142857142857143, 0.7458521870286576, 0.315, 0.5304260935143288], [0.7089285714285715, 0.7343137254901961, 0.39, 0.562156862745098], [0.7142857142857143, 0.7477752639517345, 0.29, 0.5188876319758673], [0.7625, 0.8014328808446456, 0.265, 0.5332164404223227], [0.6982142857142857, 0.7264328808446455, 0.34, 0.5332164404223227], [0.7035714285714285, 0.7320889894419306, 0.34, 0.5360444947209653], [0.7125, 0.7380467571644043, 0.39, 0.5640233785822021], [0.7017857142857142, 0.7265460030165912, 0.39, 0.5582730015082956], [0.6982142857142857, 0.7264328808446455, 0.34, 0.5332164404223229], [0.6821428571428572, 0.7070889894419305, 0.365, 0.5360444947209653], [0.6910714285714286, 0.7207767722473605, 0.315, 0.5178883861236803], [0.6821428571428572, 0.7033559577677224, 0.415, 0.5591779788838612], [0.6892857142857143, 0.7130090497737557, 0.39, 0.5515045248868778], [0.7017857142857142, 0.7265460030165912, 0.39, 0.5582730015082956], [0.7053571428571429, 0.7245852187028657, 0.465, 0.5947926093514329], [0.7, 0.7303544494720965, 0.315, 0.5226772247360483], [0.6821428571428572, 0.7188536953242836, 0.22, 0.46942684766214177], [0.6732142857142858, 0.7014328808446455, 0.315, 0.5082164404223227], [0.6910714285714286, 0.7187028657616893, 0.34, 0.5293514328808446], [0.7178571428571429, 0.7476621417797888, 0.34, 0.5438310708898945], [0.6875, 0.7091251885369533, 0.415, 0.5620625942684766], [0.6946428571428571, 0.7207390648567119, 0.365, 0.542869532428356], [0.7196428571428571, 0.7496229260935143, 0.34, 0.5448114630467572], [0.7, 0.7284313725490196, 0.34, 0.5342156862745098], [0.7410714285714286, 0.776659125188537, 0.295, 0.5358295625942685], [0.7142857142857143, 0.7399698340874812, 0.39, 0.5649849170437405], [0.6964285714285714, 0.7167797888386124, 0.44, 0.5783898944193062], [0.6785714285714286, 0.7033559577677224, 0.365, 0.5341779788838612], [0.7071428571428572, 0.7264705882352941, 0.465, 0.595735294117647], [0.7178571428571429, 0.7573529411764706, 0.22, 0.48867647058823527], [0.6875, 0.7129336349924585, 0.365, 0.5389668174962292], [0.6910714285714286, 0.720814479638009, 0.315, 0.5179072398190046], [0.7035714285714286, 0.7381221719457014, 0.27, 0.5040610859728507], [0.7357142857142858, 0.7745475113122172, 0.24, 0.5072737556561085], [0.7107142857142857, 0.7400075414781297, 0.34, 0.5400037707390648], [0.6982142857142857, 0.7226244343891403, 0.39, 0.5563122171945701], [0.7107142857142857, 0.7381975867269985, 0.37, 0.5540987933634992], [0.6982142857142857, 0.7226621417797888, 0.39, 0.5563310708898944], [0.6732142857142858, 0.6995098039215686, 0.345, 0.5222549019607843], [0.725, 0.7610482654600301, 0.265, 0.5130241327300151], [0.7035714285714286, 0.7322398190045248, 0.345, 0.5386199095022625], [0.7035714285714285, 0.7264705882352941, 0.415, 0.570735294117647], [0.725, 0.7476998491704374, 0.44, 0.5938499245852187], [0.7410714285714286, 0.7784690799396682, 0.265, 0.5217345399698341], [0.7, 0.7283559577677224, 0.34, 0.5341779788838612], [0.6714285714285714, 0.6975867269984917, 0.34, 0.5187933634992459], [0.7, 0.7246606334841629, 0.385, 0.5548303167420815], [0.7107142857142857, 0.7361236802413273, 0.39, 0.5630618401206636], [0.7017857142857142, 0.736236802413273, 0.27, 0.5031184012066364], [0.7125, 0.7400075414781297, 0.37, 0.5550037707390648], [0.7160714285714286, 0.7437782805429864, 0.365, 0.5543891402714932], [0.7107142857142857, 0.7400075414781297, 0.34, 0.5400037707390648], [0.6946428571428571, 0.720814479638009, 0.365, 0.5429072398190045], [0.7053571428571429, 0.7323529411764705, 0.365, 0.5486764705882353], [0.7107142857142857, 0.7476244343891403, 0.24, 0.49381221719457014], [0.7267857142857143, 0.7573529411764706, 0.34, 0.5486764705882353], [0.6928571428571428, 0.7149321266968326, 0.415, 0.5649660633484163], [0.6785714285714286, 0.7015082956259426, 0.39, 0.5457541478129713], [0.7410714285714286, 0.7804675716440422, 0.245, 0.5127337858220211], [0.7, 0.7246229260935143, 0.39, 0.5573114630467572], [0.6821428571428572, 0.703393665158371, 0.415, 0.5591968325791855], [0.7178571428571429, 0.7515837104072398, 0.295, 0.5232918552036199], [0.6910714285714286, 0.7168174962292609, 0.365, 0.5409087481146304], [0.6839285714285714, 0.7111236802413273, 0.34, 0.5255618401206636], [0.6982142857142857, 0.7343137254901961, 0.245, 0.489656862745098], [0.6964285714285714, 0.7226621417797888, 0.365, 0.5438310708898944], [0.7125, 0.7419683257918552, 0.34, 0.5409841628959275], [0.7821428571428571, 0.8245475113122172, 0.24, 0.5322737556561086], [0.675, 0.6975490196078431, 0.39, 0.5437745098039216], [0.7017857142857142, 0.7343137254901961, 0.295, 0.514656862745098], [0.6946428571428571, 0.7245852187028657, 0.315, 0.5197926093514329], [0.6892857142857143, 0.7131221719457014, 0.39, 0.5515610859728507], [0.6803571428571429, 0.7072021116138764, 0.34, 0.5236010558069382], [0.7196428571428571, 0.7572021116138763, 0.24, 0.49860105580693814], [0.7732142857142857, 0.8187405731523378, 0.19, 0.5043702865761689], [0.7285714285714285, 0.7555052790346908, 0.39, 0.5727526395173455], [0.7017857142857142, 0.7265082956259427, 0.39, 0.5582541478129713], [0.6928571428571428, 0.7187782805429864, 0.365, 0.5418891402714932], [0.7, 0.7285067873303167, 0.345, 0.5367533936651584], [0.7053571428571428, 0.7361236802413273, 0.315, 0.5255618401206636], [0.7375, 0.7708144796380091, 0.315, 0.5429072398190046], [0.7, 0.7206259426847662, 0.44, 0.5803129713423831], [0.6928571428571428, 0.7208521870286576, 0.34, 0.5304260935143288], [0.6964285714285714, 0.728393665158371, 0.29, 0.5091968325791856], [0.6892857142857143, 0.7170060331825038, 0.34, 0.5285030165912519], [0.7357142857142858, 0.7707013574660634, 0.29, 0.5303506787330317], [0.6857142857142857, 0.7111613876319759, 0.365, 0.538080693815988], [0.7071428571428572, 0.7342760180995476, 0.37, 0.5521380090497737], [0.6785714285714286, 0.7034690799396681, 0.365, 0.534234539969834], [0.7321428571428572, 0.7649698340874811, 0.315, 0.5399849170437405], [0.6803571428571429, 0.7072021116138764, 0.34, 0.5236010558069382], [0.7142857142857143, 0.7514328808446455, 0.24, 0.49571644042232277], [0.7553571428571428, 0.7996606334841629, 0.19, 0.49483031674208144], [0.6839285714285714, 0.7128582202111614, 0.315, 0.5139291101055807], [0.7160714285714286, 0.7478129713423831, 0.315, 0.5314064856711915], [0.7785714285714286, 0.8187782805429864, 0.265, 0.5418891402714932], [0.6875, 0.714894419306184, 0.34, 0.527447209653092], [0.7196428571428571, 0.7496606334841629, 0.34, 0.5448303167420814], [0.7571428571428571, 0.8014328808446456, 0.19, 0.49571644042232277], [0.6910714285714286, 0.7168929110105581, 0.365, 0.540946455505279], [0.7071428571428571, 0.7303544494720965, 0.415, 0.5726772247360482], [0.6857142857142857, 0.7110105580693816, 0.365, 0.5380052790346908], [0.6785714285714286, 0.7053167420814479, 0.34, 0.522658371040724], [0.7125, 0.7439291101055807, 0.315, 0.5294645550527903], [0.7178571428571429, 0.743815987933635, 0.39, 0.5669079939668175], [0.7125, 0.7439668174962293, 0.315, 0.5294834087481146], [0.7446428571428572, 0.7786953242835596, 0.32, 0.5493476621417798], [0.725, 0.74973604826546, 0.415, 0.58236802413273], [0.6964285714285714, 0.7226998491704374, 0.365, 0.5438499245852186], [0.6964285714285714, 0.7284313725490196, 0.295, 0.5117156862745098], [0.6767857142857143, 0.7071644042232278, 0.29, 0.49858220211161386], [0.7053571428571428, 0.7302790346907994, 0.39, 0.5601395173453997], [0.6857142857142857, 0.7207390648567119, 0.24, 0.48036953242835595], [0.6785714285714286, 0.7072398190045249, 0.315, 0.5111199095022624], [0.7214285714285714, 0.7458144796380091, 0.415, 0.5804072398190046], [0.6875, 0.7111236802413273, 0.39, 0.5505618401206636], [0.6839285714285714, 0.7110859728506788, 0.34, 0.5255429864253394], [0.7053571428571429, 0.7302413273001508, 0.39, 0.5601206636500754], [0.7089285714285715, 0.7379713423831071, 0.34, 0.5389856711915536], [0.6910714285714286, 0.7169683257918552, 0.365, 0.5409841628959275], [0.6785714285714286, 0.6996983408748114, 0.415, 0.5573491704374057], [0.7375, 0.768815987933635, 0.34, 0.5544079939668175], [0.775, 0.8187782805429864, 0.215, 0.5168891402714932], [0.7035714285714285, 0.7341628959276019, 0.315, 0.5245814479638009], [0.7178571428571429, 0.7459276018099548, 0.365, 0.5554638009049774], [0.775, 0.8207390648567119, 0.19, 0.5053695324283559], [0.6964285714285714, 0.7245852187028657, 0.345, 0.5347926093514329], [0.7785714285714286, 0.8187782805429864, 0.265, 0.5418891402714933], [0.6785714285714286, 0.7053167420814479, 0.34, 0.522658371040724], [0.7053571428571428, 0.7380090497737557, 0.29, 0.5140045248868779], [0.6928571428571428, 0.7129713423831071, 0.44, 0.5764856711915536], [0.7232142857142857, 0.7515837104072398, 0.365, 0.5582918552036199], [0.6892857142857143, 0.7167420814479638, 0.34, 0.5283710407239819], [0.6857142857142857, 0.7130467571644042, 0.34, 0.5265233785822021], [0.6910714285714286, 0.7188914027149321, 0.34, 0.529445701357466], [0.6946428571428571, 0.7225490196078431, 0.34, 0.5312745098039215], [0.7125, 0.7400075414781297, 0.365, 0.5525037707390649], [0.7160714285714286, 0.7458144796380091, 0.34, 0.5429072398190046], [0.7053571428571429, 0.7342006033182504, 0.34, 0.5371003016591251], [0.7017857142857142, 0.7303167420814479, 0.34, 0.535158371040724], [0.6964285714285714, 0.7284690799396681, 0.29, 0.509234539969834], [0.7785714285714286, 0.8228129713423831, 0.22, 0.5214064856711915], [0.7428571428571429, 0.7745475113122172, 0.34, 0.5572737556561086], [0.7, 0.7265082956259427, 0.365, 0.5457541478129714], [0.7035714285714285, 0.7380467571644043, 0.265, 0.5015233785822021], [0.6839285714285714, 0.7033559577677225, 0.435, 0.5691779788838612], [0.7089285714285714, 0.7418552036199095, 0.29, 0.5159276018099548], [0.7089285714285715, 0.7380467571644043, 0.34, 0.5390233785822021], [0.7178571428571429, 0.743815987933635, 0.39, 0.5669079939668175], [0.7160714285714286, 0.7457390648567119, 0.34, 0.5428695324283559], [0.6892857142857143, 0.714894419306184, 0.365, 0.5399472096530921], [0.6910714285714286, 0.7149321266968326, 0.39, 0.5524660633484163], [0.7178571428571429, 0.7495475113122172, 0.315, 0.5322737556561086], [0.7017857142857142, 0.7265837104072398, 0.39, 0.5582918552036199], [0.6875, 0.7110859728506788, 0.39, 0.5505429864253394], [0.7178571428571429, 0.7515082956259427, 0.29, 0.5207541478129714], [0.6910714285714286, 0.7188914027149321, 0.34, 0.529445701357466], [0.7017857142857142, 0.728393665158371, 0.365, 0.5466968325791856], [0.6946428571428571, 0.7245475113122172, 0.315, 0.5197737556561086], [0.6982142857142857, 0.7244343891402715, 0.365, 0.5447171945701358], [0.7714285714285715, 0.8168552036199095, 0.19, 0.5034276018099547], [0.7089285714285714, 0.7437028657616893, 0.265, 0.5043514328808446], [0.6964285714285714, 0.7208898944193062, 0.395, 0.557944947209653], [0.7017857142857142, 0.7266591251885369, 0.395, 0.5608295625942685], [0.6964285714285714, 0.7245098039215686, 0.34, 0.5322549019607843], [0.7089285714285715, 0.7342006033182503, 0.39, 0.5621003016591252], [0.6875, 0.7188536953242836, 0.29, 0.5044268476621417], [0.7, 0.7246606334841629, 0.39, 0.5573303167420814], [0.7017857142857142, 0.7302413273001508, 0.34, 0.5351206636500754], [0.6946428571428571, 0.7207013574660633, 0.365, 0.5428506787330316], [0.6928571428571428, 0.7207013574660633, 0.34, 0.5303506787330317], [0.8035714285714286, 0.8496983408748114, 0.215, 0.5323491704374057], [0.6785714285714286, 0.709238310708899, 0.295, 0.5021191553544495], [0.7160714285714286, 0.741817496229261, 0.385, 0.5634087481146305], [0.6946428571428571, 0.7285444947209653, 0.27, 0.49927224736048265], [0.6892857142857143, 0.7110482654600302, 0.415, 0.5630241327300151], [0.6839285714285714, 0.7073529411764706, 0.395, 0.5511764705882353], [0.7160714285714286, 0.7439668174962293, 0.365, 0.5544834087481146], [0.7017857142857142, 0.7265460030165912, 0.39, 0.5582730015082956], [0.6982142857142857, 0.7265460030165912, 0.34, 0.5332730015082956], [0.7125, 0.7496983408748115, 0.24, 0.4948491704374057], [0.6928571428571428, 0.7225490196078431, 0.315, 0.5187745098039216], [0.6910714285714286, 0.7091628959276018, 0.465, 0.5870814479638009], [0.7035714285714286, 0.7285444947209653, 0.39, 0.5592722473604826], [0.6839285714285714, 0.7093137254901961, 0.365, 0.537156862745098], [0.6892857142857143, 0.7130090497737557, 0.39, 0.5515045248868778], [0.6892857142857143, 0.7168552036199095, 0.34, 0.5284276018099547], [0.775, 0.8206636500754148, 0.19, 0.5053318250377075], [0.7803571428571429, 0.8207013574660633, 0.265, 0.5428506787330316], [0.7946428571428571, 0.840158371040724, 0.22, 0.5300791855203619], [0.7785714285714286, 0.82447209653092, 0.185, 0.50473604826546], [0.7589285714285714, 0.7958521870286577, 0.29, 0.5429260935143289], [0.7517857142857143, 0.7938914027149322, 0.215, 0.504445701357466], [0.7910714285714285, 0.8379713423831071, 0.19, 0.5139856711915536], [0.7571428571428571, 0.7938914027149321, 0.29, 0.5419457013574661], [0.7892857142857143, 0.8341251885369533, 0.21, 0.5220625942684766], [0.7446428571428572, 0.7842760180995475, 0.24, 0.5121380090497738], [0.7946428571428572, 0.8418929110105581, 0.19, 0.5159464555052791], [0.7607142857142857, 0.7997360482654601, 0.265, 0.53236802413273], [0.7785714285714286, 0.82447209653092, 0.19, 0.5072360482654601], [0.7678571428571429, 0.8110105580693816, 0.215, 0.5130052790346908], [0.7785714285714286, 0.8225490196078431, 0.215, 0.5187745098039216], [0.7928571428571428, 0.8381221719457014, 0.215, 0.5265610859728507], [0.7839285714285714, 0.8304298642533937, 0.19, 0.5102149321266968], [0.7535714285714286, 0.7938914027149321, 0.24, 0.5169457013574661], [0.7928571428571428, 0.8302790346907994, 0.315, 0.5726395173453998], [0.8214285714285714, 0.872737556561086, 0.17, 0.521368778280543], [0.7964285714285715, 0.8400075414781297, 0.24, 0.5400037707390648], [0.7928571428571428, 0.8341628959276017, 0.265, 0.5495814479638009], [0.7928571428571428, 0.8341628959276017, 0.265, 0.5495814479638009], [0.7910714285714286, 0.8361236802413273, 0.215, 0.5255618401206636], [0.7946428571428571, 0.8360859728506788, 0.265, 0.5505429864253394], [0.7982142857142858, 0.8457767722473605, 0.19, 0.5178883861236803], [0.7964285714285715, 0.8400075414781297, 0.24, 0.5400037707390648], [0.7875, 0.8322398190045249, 0.215, 0.5236199095022624], [0.7821428571428571, 0.8225867269984917, 0.265, 0.5437933634992459], [0.7946428571428571, 0.8322021116138764, 0.31, 0.5711010558069382], [0.7875, 0.8284313725490196, 0.265, 0.5467156862745098], [0.7785714285714286, 0.8225490196078431, 0.215, 0.5187745098039216], [0.7928571428571428, 0.8381221719457014, 0.22, 0.5290610859728506], [0.7946428571428571, 0.8322021116138764, 0.31, 0.5711010558069382], [0.7928571428571429, 0.8380844645550528, 0.215, 0.5265422322775264], [0.7875, 0.8303544494720965, 0.24, 0.5351772247360482], [0.7678571428571428, 0.8092006033182504, 0.245, 0.5271003016591251], [0.7785714285714286, 0.8225490196078431, 0.215, 0.5187745098039216], [0.7964285714285715, 0.8400075414781297, 0.24, 0.5400037707390648], [0.8196428571428571, 0.8708144796380091, 0.17, 0.5204072398190045], [0.7928571428571428, 0.8302790346907994, 0.315, 0.5726395173453998], [0.7857142857142857, 0.8303921568627451, 0.215, 0.5226960784313726], [0.7589285714285714, 0.7918174962292609, 0.335, 0.5634087481146305], [0.7285714285714285, 0.766817496229261, 0.24, 0.5034087481146304], [0.7928571428571428, 0.8322021116138764, 0.29, 0.5611010558069381], [0.7678571428571429, 0.8053921568627451, 0.29, 0.5476960784313726], [0.8214285714285714, 0.8707767722473605, 0.19, 0.5303883861236802], [0.7535714285714286, 0.7938914027149321, 0.24, 0.5169457013574661], [0.7839285714285714, 0.8303921568627451, 0.19, 0.5101960784313725], [0.7946428571428572, 0.8419683257918552, 0.195, 0.5184841628959276], [0.7517857142857143, 0.7881221719457013, 0.29, 0.5390610859728506], [0.7821428571428571, 0.8225867269984917, 0.26, 0.5412933634992458], [0.8089285714285714, 0.8630467571644043, 0.115, 0.48902337858220213], [0.7857142857142857, 0.8265460030165912, 0.265, 0.5457730015082957], [0.7928571428571428, 0.8322021116138764, 0.29, 0.5611010558069381], [0.7267857142857143, 0.7648567119155354, 0.24, 0.5024283559577677], [0.7928571428571428, 0.8380467571644042, 0.215, 0.5265233785822021], [0.7732142857142857, 0.8207390648567119, 0.165, 0.4928695324283559], [0.7482142857142857, 0.7861613876319758, 0.265, 0.5255806938159879], [0.7803571428571429, 0.8265082956259426, 0.19, 0.5082541478129713], [0.7964285714285714, 0.8360859728506788, 0.29, 0.5630429864253393], [0.8196428571428571, 0.8707767722473605, 0.17, 0.5203883861236802], [0.7875, 0.8322775263951735, 0.215, 0.5236387631975867], [0.7660714285714285, 0.8073529411764706, 0.24, 0.5236764705882353], [0.7892857142857143, 0.834238310708899, 0.215, 0.5246191553544495], [0.7946428571428571, 0.8322021116138764, 0.31, 0.5711010558069382], [0.7910714285714285, 0.8341251885369533, 0.235, 0.5345625942684766], [0.7964285714285714, 0.8360482654600302, 0.285, 0.5605241327300151], [0.7803571428571429, 0.8225867269984917, 0.235, 0.5287933634992459], [0.7821428571428571, 0.8263951734539969, 0.21, 0.5181975867269984], [0.7928571428571428, 0.8380844645550528, 0.215, 0.5265422322775264], [0.7857142857142857, 0.8341628959276018, 0.165, 0.4995814479638009], [0.7892857142857143, 0.8342006033182504, 0.215, 0.5246003016591252], [0.7785714285714286, 0.8225490196078431, 0.215, 0.5187745098039216], [0.7875, 0.8342006033182504, 0.19, 0.5121003016591251], [0.7785714285714286, 0.8206636500754148, 0.235, 0.5278318250377074], [0.7946428571428571, 0.8322021116138764, 0.31, 0.5711010558069382], [0.7946428571428571, 0.8322021116138764, 0.31, 0.5711010558069382], [0.7946428571428571, 0.8322021116138764, 0.31, 0.5711010558069382], [0.7928571428571428, 0.8341628959276017, 0.265, 0.5495814479638009], [0.8214285714285714, 0.8708144796380091, 0.19, 0.5304072398190045], [0.7928571428571428, 0.8322021116138764, 0.29, 0.5611010558069381], [0.7803571428571429, 0.8264328808446455, 0.19, 0.5082164404223227], [0.7946428571428572, 0.8380467571644042, 0.24, 0.5390233785822021], [0.7732142857142857, 0.8150452488687783, 0.24, 0.5275226244343891], [0.8267857142857142, 0.8786199095022624, 0.17, 0.5243099547511312], [0.7785714285714286, 0.8263951734539969, 0.165, 0.4956975867269985], [0.7964285714285715, 0.8400075414781297, 0.24, 0.5400037707390648], [0.7946428571428572, 0.8400075414781297, 0.215, 0.5275037707390648], [0.7660714285714286, 0.8091628959276018, 0.215, 0.512081447963801], [0.7910714285714285, 0.8341628959276018, 0.24, 0.5370814479638009], [0.7696428571428572, 0.8111990950226244, 0.245, 0.5280995475113122], [0.7910714285714285, 0.8379713423831071, 0.185, 0.5114856711915535], [0.7285714285714285, 0.7650829562594268, 0.265, 0.5150414781297135], [0.7946428571428571, 0.8322021116138764, 0.31, 0.5711010558069382], [0.7839285714285714, 0.8245475113122172, 0.265, 0.5447737556561086], [0.7910714285714285, 0.8361236802413273, 0.215, 0.5255618401206636], [0.7589285714285714, 0.7996606334841629, 0.24, 0.5198303167420815], [0.8267857142857142, 0.8786199095022624, 0.17, 0.5243099547511312], [0.7803571428571429, 0.8206636500754148, 0.265, 0.5428318250377074]]\n"
     ]
    }
   ],
   "source": [
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/        \n",
    "\n",
    "params = {\n",
    "                    \"learning_rate\"    :[0.3],  #0.01-0.2 Makes the model more robust by shrinking the weights on each step\n",
    "                    \"max_depth\"        :[2,3],  #3-10 control over-fitting as higher depth will allow model to learn relations very specific to a particular sample\n",
    "                    \"n_estimators\"     :[100,200,300,400],\n",
    "                    \"min_child_weight\" :[1,2],         #0.5-1 small values might lead to under-fitting\n",
    "                    \"gamma\"            :[0,0.5,1],            #Makes the algorithm conservative --> No overfitting\n",
    "                    \"colsample_bylevel\" :[0.5,0.75,1], #0.5-1\n",
    "                     \"reg_lambda\"      :[1, 1.5, 2], #  it should be explored to reduce overfitting.\n",
    "                    \"scale_pos_weight\" :[3,4,5],\n",
    "                    \"colsample_bytree\" :[0.5,0.75],\n",
    "                    \"reg_alpha\"        :[0.05,0.1,0.2],\n",
    "                    \"subsample\"        :[0.8]\n",
    "    \n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.740878001587038\n"
     ]
    }
   ],
   "source": [
    "params = [[],[],[],[],[],[]]\n",
    "for i in range(6):\n",
    "    best_par = optimizer.max.get('params')\n",
    "    params.append(best_par.get('learning_rate'))\n",
    "    params.append(round(best_par.get('max_depth')))\n",
    "    params.append(round(best_par.get('n_estimators')))\n",
    "    params.append(round(best_par.get('min_child_weight')))\n",
    "    params.append(best_par.get('gamma'))\n",
    "    params.append(best_par.get('colsample_bylevel'))\n",
    "    params.append(best_par.get('reg_lambda'))\n",
    "    params.append(round(best_par.get('scale_pos_weight')))\n",
    "    params.append(best_par.get('colsample_bytree'))\n",
    "    params.append(best_par.get('reg_alpha'))\n",
    "    params.append(best_par.get('subsample'))\n",
    "\n",
    "print(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
