{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Age    DD    BMI  Smoking  ypolip  PulsBP  aca0   gla0   chl0    tg0  \\\n",
      "0    71.0  18.0  19.78      1.0     1.0      55   6.6   90.0  153.0   71.0   \n",
      "1    38.0   2.0  23.31      3.0     1.0      20  10.0  260.0  189.0   79.0   \n",
      "2    81.0   7.0  29.06      2.0     1.0      65   5.4  140.0  161.0   90.0   \n",
      "3    50.0  11.0  27.93      3.0     1.0      40   7.1  160.0  204.0   78.0   \n",
      "4    62.0  25.0  20.70      1.0     1.0      45   4.2  100.0  153.0   51.0   \n",
      "..    ...   ...    ...      ...     ...     ...   ...    ...    ...    ...   \n",
      "555  58.0   3.0  24.96      2.0     1.0      60   5.8  140.0  210.0  138.0   \n",
      "556  77.0  17.0  25.10      1.0     1.0      70   6.9  145.0  202.0   72.0   \n",
      "557  66.0   0.0  29.21      1.0     2.0      60   6.4  130.0  330.0  282.0   \n",
      "558  71.0  22.0  31.57      1.0     1.0      80   9.5  145.0  216.0  235.0   \n",
      "559  56.0   0.0  32.28      1.0     2.0      60   7.1  125.0  156.0   85.0   \n",
      "\n",
      "     hdl0  CVD prediction  \n",
      "0    50.0             0.0  \n",
      "1    47.0             0.0  \n",
      "2    40.0             0.0  \n",
      "3    45.0             0.0  \n",
      "4    46.0             0.0  \n",
      "..    ...             ...  \n",
      "555  51.0             0.0  \n",
      "556  55.0             0.0  \n",
      "557  94.0             0.0  \n",
      "558  44.0             0.0  \n",
      "559  43.0             0.0  \n",
      "\n",
      "[560 rows x 12 columns]\n",
      "560\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import numpy\n",
    "numpy.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "f_size = 11\n",
    "# Importing the dataset\n",
    "dataset = pd.read_csv('CVDdataset.csv') #11\n",
    "# dataset = pd.read_csv('CVD dataset-plin-7-9.csv') #14\n",
    "# dataset = pd.read_csv('CVD-dataset-plin-0-4-6-7-9.csv') #11\n",
    "print(dataset)\n",
    "# dataset = pd.read_csv('CVD dataset2.csv')\n",
    "# dataset = pd.read_csv('CVD-dataset-2-3-11-13-15.csv')\n",
    "# dataset = pd.read_csv('CVD dataset-plin-4-7-9.csv') #διαβάζω το dataset που δεν περιλαμβανει τα features 7 και 9\n",
    "# dataset = pd.read_csv('CVD-dataset-1-2-3-5-10-11-12-13-14-15.csv')\n",
    "data=dataset.iloc[:, :].values\n",
    "X = dataset.iloc[:, :-1].values\n",
    "y = dataset.iloc[:, f_size].values\n",
    "# X = dataset.iloc[:, [ 0, 1, 2, 5, 10, 11, 13, 15]].values\n",
    "\n",
    "rows = len(data)    # 3 rows in your example\n",
    "cols = len(data[0])\n",
    "print(rows)\n",
    "print(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0.98, 'Age')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset.boxplot(column = 'Age', by = 'CVD prediction');\n",
    "plt.title('')\n",
    "plt.suptitle('Age')\n",
    "# dataset.boxplot(grid=False, rot=60, fontsize=15,figsize=(20,10));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "519\n",
      "41\n"
     ]
    }
   ],
   "source": [
    "dataNoCvd = data[~(data[:,f_size] > 0.0)]\n",
    "dataCvd = data[~(data[:,f_size] < 1.0)]\n",
    "rowsNoCvd = len(dataNoCvd)    \n",
    "rowsCvd = len(dataCvd)\n",
    "print(rowsNoCvd)\n",
    "print(rowsCvd)\n",
    "# print(dataNoCvd)\n",
    "# print(dataCvd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_NoCvd_limitsArray(i,DownNoCvd, UpNoCvd, fold_NoCvd_total, fold_NoCvd_residue):\n",
    "    if i>0 :\n",
    "        DownNoCvd = UpNoCvd\n",
    "        UpNoCvd= fold_NoCvd_total+UpNoCvd\n",
    "    if i < fold_NoCvd_residue :\n",
    "        UpNoCvd+= 1\n",
    "#     print(\"NoCvd limits, Down :\",DownNoCvd,\"Up \",UpNoCvd)\n",
    "    return   DownNoCvd, UpNoCvd   \n",
    " \n",
    "def find_Cvd_limitsArray(i,DownCvd, UpCvd, fold_Cvd_total, fold_Cvd_residue,cv):\n",
    "    if i>0 :\n",
    "        DownCvd = UpCvd\n",
    "        UpCvd= fold_Cvd_total+UpCvd\n",
    "    if i >= cv - fold_Cvd_residue :\n",
    "         UpCvd+= 1     \n",
    "#     print(\"Cvd limits, Down :\",DownCvd,\"Up \",UpCvd)\n",
    "    return   DownCvd, UpCvd  \n",
    "\n",
    "def find_testValSubset(DownNoCvd,UpNoCvd,DownCvd,UpCvd,dataNoCvd,dataCvd):\n",
    "    temp1=dataNoCvd[DownNoCvd:UpNoCvd,:]\n",
    "    temp2=dataCvd[DownCvd:UpCvd,:]\n",
    "    temp3=np.concatenate((temp1, temp2))\n",
    "#     print(len(temp1),\"  \",len(temp2),\"  \",len(temp3))\n",
    "    return temp3\n",
    "\n",
    "def find_trainSubset(DownNoCvd,UpNoCvd,DownCvd,UpCvd,dataNoCvd,dataCvd):\n",
    "    temp1 = np.delete(dataNoCvd, slice(DownNoCvd, UpNoCvd), axis=0)\n",
    "    temp2 = np.delete(dataCvd, slice(DownCvd, UpCvd), axis=0)\n",
    "    temp3 = np.concatenate((temp1, temp2))\n",
    "#     print(len(temp1),\"  \",len(temp2),\"  \",len(temp3))\n",
    "    return temp1, temp2, temp3\n",
    "\n",
    "def find_trainSubset1(DownNoCvd,UpNoCvd,DownCvd,UpCvd,dataNoCvd,dataCvd,sur):\n",
    "    temp1 = np.delete(dataNoCvd, slice(DownNoCvd, UpNoCvd), axis=0)\n",
    "    temp2 = np.delete(dataCvd, slice(DownCvd, UpCvd), axis=0)\n",
    "#     print(\"len 1-2:\",len(temp1),len(temp2))\n",
    "    if sur == False :\n",
    "        temp3 = np.delete(temp1, slice(0, 46), axis=0)\n",
    "        temp4 = np.delete(temp2, slice(0, 4), axis=0)\n",
    "    else:\n",
    "        temp3 = np.delete(temp1, slice(0, 32), axis=0)\n",
    "        temp4 = np.delete(temp2, slice(0, 18), axis=0)\n",
    "        \n",
    "#     print(\"len 3-4:\",len(temp3),len(temp4))\n",
    "    temp5 = np.concatenate((temp3, temp4))\n",
    "#     print(len(temp5))\n",
    "    return temp3, temp4, temp5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_test_sets(f_size,cv,dataNoCvd,dataCvd,val_ratio,sur):\n",
    "    test_total = []\n",
    "    train_total = []\n",
    "    train_total_Cvd = []\n",
    "    train_total_NoCvd = []\n",
    "    validation_total = []\n",
    "    rowsNoCvd = len(dataNoCvd)    \n",
    "    rowsCvd = len(dataCvd)\n",
    "\n",
    "    fold_Cvd_total = rowsCvd//cv\n",
    "    fold_Cvd_residue= rowsCvd%cv\n",
    "    print(\"fold_Cvd_total  :\",fold_Cvd_total,\" fold_Cvd_residue  :\",fold_Cvd_residue)\n",
    "\n",
    "    fold_NoCvd_total = rowsNoCvd//cv\n",
    "    fold_NoCvd_residue= rowsNoCvd%cv\n",
    "    print(\"fold_NoCvd_total:\",fold_NoCvd_total,\"fold_NoCvd_residue:\",fold_NoCvd_residue)\n",
    "    \n",
    "    Cvd_val = round(504*val_ratio*(rowsCvd/len(data)))\n",
    "    noCvd_val = round(504*val_ratio)-Cvd_val\n",
    "#     print(noCvd_val)\n",
    "    \n",
    "    DownNoCvd=0\n",
    "    DownCvd=0\n",
    "    UpNoCvd = fold_NoCvd_total\n",
    "    UpCvd = fold_Cvd_total\n",
    "\n",
    "    for i in range(cv):\n",
    "        X_test = []\n",
    "        y_test = []\n",
    "        X_train = []\n",
    "        y_train = []\n",
    "        X_val = []\n",
    "        y_val = []\n",
    "#         print(\"============\",i,\"==============\")\n",
    "        DownNoCvd, UpNoCvd = find_NoCvd_limitsArray(i, DownNoCvd, UpNoCvd, fold_NoCvd_total, fold_NoCvd_residue)\n",
    "        DownCvd, UpCvd = find_Cvd_limitsArray(i, DownCvd, UpCvd, fold_Cvd_total, fold_Cvd_residue,cv)\n",
    "#         print(DownNoCvd,UpNoCvd,DownCvd,UpCvd)\n",
    "        testSubset_total = find_testValSubset(DownNoCvd,UpNoCvd,DownCvd,UpCvd,dataNoCvd,dataCvd)        \n",
    "        \n",
    "        if i!=9:\n",
    "        #όταν δεν είμαστε στο 10ο fold(ξεκινάμε από 0) παίρνουμε τα επόμενα 56 και 4 instaces για το validation set\n",
    "            validationSubset_total = find_testValSubset(UpNoCvd,UpNoCvd+noCvd_val,UpCvd,UpCvd+Cvd_val,dataNoCvd,dataCvd)\n",
    "            trainSubset_NoCvd ,trainSubset_Cvd ,trainSubset_total = find_trainSubset(DownNoCvd,UpNoCvd+noCvd_val,DownCvd,UpCvd+Cvd_val,dataNoCvd,dataCvd)\n",
    "        else:\n",
    "        #όταν είμαστε στο 10ο fold παίρνουμε τα προηγούμενα 56 και 4 instaces για το validation set\n",
    "#         DownNoCvd-noCvd_val,DownNoCvd,DownCvd-Cvd_val,DownCvd\n",
    "            validationSubset_total = find_testValSubset(0,noCvd_val,0,Cvd_val,dataNoCvd,dataCvd)\n",
    "            print(DownNoCvd,UpNoCvd,DownCvd,UpCvd)\n",
    "            trainSubset_NoCvd ,trainSubset_Cvd ,trainSubset_total = find_trainSubset1(DownNoCvd,UpNoCvd,DownCvd,UpCvd,dataNoCvd,dataCvd,sur)\n",
    "    \n",
    "    #creating X_train, y_train, X_test, y_test\n",
    "        X_test.append(np.delete(testSubset_total, f_size, axis=1))\n",
    "        y_test_temp = np.delete(testSubset_total, slice(0, f_size), axis=1)\n",
    "        y_test.append(np.reshape(y_test_temp, len(y_test_temp)))\n",
    "        X_test_temp = np.array(X_test)\n",
    "        X_test = X_test_temp[0]\n",
    "        y_test_temp = np.array(y_test)\n",
    "        y_test = y_test_temp[0]\n",
    "        \n",
    "        X_val.append(np.delete(validationSubset_total, f_size, axis=1))\n",
    "        y_val_temp = np.delete(validationSubset_total, slice(0, f_size), axis=1)\n",
    "        y_val.append(np.reshape(y_val_temp, len(y_val_temp)))\n",
    "        X_val_temp = np.array(X_val)\n",
    "        X_val = X_val_temp[0]\n",
    "        y_val_temp = np.array(y_val)\n",
    "        y_val = y_val_temp[0]\n",
    "        \n",
    "        X_train.append(np.delete(trainSubset_total, f_size, axis=1))\n",
    "        y_train_temp = np.delete(trainSubset_total, slice(0, f_size), axis=1)\n",
    "        y_train.append(np.reshape(y_train_temp, len(y_train_temp)))\n",
    "        X_train_temp = np.array(X_train)\n",
    "        X_train = X_train_temp[0]\n",
    "        y_train_temp = np.array(y_train)\n",
    "        y_train = y_train_temp[0]\n",
    "        \n",
    "    #add every subset in a list so we can handle thm later \n",
    "        test_total.append([X_test,y_test])\n",
    "        validation_total.append([X_val,y_val])\n",
    "        train_total.append([X_train,y_train]) \n",
    "        train_total_NoCvd.append(trainSubset_NoCvd)\n",
    "        train_total_Cvd.append(trainSubset_Cvd)\n",
    "    return train_total, test_total, validation_total, train_total_NoCvd, train_total_Cvd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold_Cvd_total  : 4  fold_Cvd_residue  : 1\n",
      "fold_NoCvd_total: 51 fold_NoCvd_residue: 9\n",
      "468 519 36 41\n"
     ]
    }
   ],
   "source": [
    "train_total, test_total, validation_total, train_total_NoCvd, train_total_Cvd = create_train_test_sets(f_size,10,dataNoCvd,dataCvd,0.1,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "454\n",
      "ratio in train set: 0.07268722466960352\n",
      "ratio in test set: 0.07142857142857142\n",
      "ratio in validation set: 0.08\n",
      "454\n",
      "ratio in train set: 0.07268722466960352\n",
      "ratio in test set: 0.07142857142857142\n",
      "ratio in validation set: 0.08\n",
      "454\n",
      "ratio in train set: 0.07268722466960352\n",
      "ratio in test set: 0.07142857142857142\n",
      "ratio in validation set: 0.08\n",
      "454\n",
      "ratio in train set: 0.07268722466960352\n",
      "ratio in test set: 0.07142857142857142\n",
      "ratio in validation set: 0.08\n",
      "454\n",
      "ratio in train set: 0.07268722466960352\n",
      "ratio in test set: 0.07142857142857142\n",
      "ratio in validation set: 0.08\n",
      "454\n",
      "ratio in train set: 0.07268722466960352\n",
      "ratio in test set: 0.07142857142857142\n",
      "ratio in validation set: 0.08\n",
      "454\n",
      "ratio in train set: 0.07268722466960352\n",
      "ratio in test set: 0.07142857142857142\n",
      "ratio in validation set: 0.08\n",
      "454\n",
      "ratio in train set: 0.07268722466960352\n",
      "ratio in test set: 0.07142857142857142\n",
      "ratio in validation set: 0.08\n",
      "454\n",
      "ratio in train set: 0.07268722466960352\n",
      "ratio in test set: 0.07142857142857142\n",
      "ratio in validation set: 0.08\n",
      "454\n",
      "ratio in train set: 0.07048458149779736\n",
      "ratio in test set: 0.08928571428571429\n",
      "ratio in validation set: 0.08\n"
     ]
    }
   ],
   "source": [
    "# Chech ratio in each train and test set\n",
    "def find_ratio(index_list):\n",
    "    one = 0\n",
    "    lenght=len(index_list[0])\n",
    "    for i in range(lenght):\n",
    "#         print(index_list[1])\n",
    "        if index_list[1][i] == 1.0 :\n",
    "            one+= 1\n",
    "    ratio = one/lenght\n",
    "    return ratio\n",
    "\n",
    "for i in range(10):\n",
    "    print(len(train_total[i][0]))\n",
    "    ratio = find_ratio(train_total[i])\n",
    "    print(\"ratio in train set:\", ratio)\n",
    "    ratio = find_ratio(test_total[i])\n",
    "    print(\"ratio in test set:\", ratio)\n",
    "    ratio = find_ratio(validation_total[i])\n",
    "    print(\"ratio in validation set:\", ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "from matplotlib import pyplot\n",
    "from xgboost import plot_importance\n",
    "import math\n",
    "\n",
    "def predict_weighted_average(models,X,weights):\n",
    "#     print(\"efwef\",len(weights), len(models))\n",
    "    y_pred = []\n",
    "    y_pred_prob = []\n",
    "    y_pred_models = []\n",
    "    y_pred_models1 = []\n",
    "#     weights = [0.16, 0.175, 0.2, 0.147, 0.148, 0.16]\n",
    "#     weights = [0.15, 0.4, 0.2, 0.05, 0.05, 0.15]\n",
    "#   weights = [0.16, 0.173, 0.2, 0.147, 0.16, 0.16]\n",
    "    models_size = len(models)\n",
    "#     print(models_size)\n",
    "    X_size = len(X)\n",
    "            \n",
    "    for i in range(models_size):\n",
    "        y_pred_models.append(models[i].predict(X,output_margin=True))  \n",
    "        y_pred_models1.append(models[i].predict_proba(X)[:,1])\n",
    "        \n",
    "        \n",
    "    total_shap_values = []\n",
    "    total_expected_value = 0\n",
    "    for i in range(models_size):\n",
    "        explainer = shap.TreeExplainer(models[i])\n",
    "#         explainer = shap.TreeExplainer(models[i],feature_perturbation=\"interventional\",model_output=\"probability\")\n",
    "        # calculate shap values. This is what we will plot.\n",
    "        # Calculate shap_values for all of val_X rather than a single row, to have more data for plot.\n",
    "         \n",
    "        pp =models[i].get_booster().get_score(importance_type=\"cover\")\n",
    "#         print(pp)\n",
    "        p = fill_importances(pp)\n",
    "#         print(p)\n",
    "        for key in p:\n",
    "            p[key] = p[key]*weights[i]*0.1\n",
    "            \n",
    "        if(i==0):\n",
    "             importances = p.copy()\n",
    "        else:\n",
    "            for key in importances:\n",
    "                importances[key] += p[key]\n",
    "   \n",
    "            \n",
    "        shap_values = explainer.shap_values(X)\n",
    "        shap_values_auc = [[l*weights[i] for l in k] for k in shap_values]\n",
    "        \n",
    "        if(i==0):\n",
    "             total_shap_values = shap_values_auc.copy()\n",
    "        else:\n",
    "            for a in range(len(total_shap_values)):\n",
    "                for b in range(len(total_shap_values[0])):\n",
    "                     total_shap_values[a][b] += shap_values_auc[a][b]\n",
    "        total_expected_value += explainer.expected_value*weights[i] \n",
    "#         print(\"explainer.expected_value\",explainer.expected_value)\n",
    "  \n",
    "    feature_names = ['Age','DD','BMI','Smoking','ypolip','PulsBP','aca0','gla0','chl0','tg0','hdl0']\n",
    "    \n",
    "    \n",
    " \n",
    "    t_shap_values = numpy.array(total_shap_values)\n",
    "    \n",
    "#     shap.summary_plot(t_shap_values, X,feature_names)  \n",
    "    \n",
    "####################\n",
    "#     shap.force_plot(total_expected_value, t_shap_values[1,:], X[1],feature_names,matplotlib=True,link='logit')\n",
    "\n",
    "#             ,link='logit'\n",
    "            \n",
    "            \n",
    "        \n",
    "    for j in range(X_size):\n",
    "        sum = 0\n",
    "        sum1 = 0\n",
    "        for i in range(models_size):\n",
    "            sum = sum + y_pred_models[i][j]*weights[i]\n",
    "            sum1 = sum1 + y_pred_models1[i][j]*weights[i]\n",
    "        avr = sum\n",
    "#         /models_size\n",
    "        pred_prob = math.exp(avr)/(1 + math.exp(avr))\n",
    "        y_pred_prob.append(pred_prob)\n",
    "        \n",
    "        if avr>= 0 :\n",
    "            y_pred.append(1.)\n",
    "            if(j==1):\n",
    "                print(\"prediction YES\")\n",
    "                print(avr)\n",
    "                print(pred_prob)\n",
    "#                 print(sum1)\n",
    "        else:\n",
    "            y_pred.append(0.)\n",
    "            if(j==1):\n",
    "                print(\"prediction NO\")\n",
    "                print(avr)\n",
    "                print(pred_prob)\n",
    "#                 print(sum1)\n",
    "    return y_pred,t_shap_values,importances,y_pred_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_importances(p):\n",
    "        if p.get(\"f0\")== None:\n",
    "            p['f0'] = 0\n",
    "        if p.get(\"f1\")== None:\n",
    "            p['f1'] = 0\n",
    "        if p.get(\"f2\")== None:\n",
    "            p['f2'] = 0\n",
    "        if p.get(\"f3\")== None:\n",
    "            p['f3'] = 0\n",
    "        if p.get(\"f4\")== None:\n",
    "            p['f4'] = 0\n",
    "        if p.get(\"f5\")== None:\n",
    "            p['f5'] = 0\n",
    "        if p.get(\"f6\")== None:\n",
    "            p['f6'] = 0\n",
    "        if p.get(\"f7\")== None:\n",
    "            p['f7'] = 0\n",
    "        if p.get(\"f8\")== None:\n",
    "            p['f8'] = 0\n",
    "        if p.get(\"f9\")== None:\n",
    "            p['f9'] = 0\n",
    "        if p.get(\"f10\")== None:\n",
    "            p['f10'] = 0\n",
    "        return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "def my_fit(ratio,dataNoCvd,dataCvd, validation_total, params):   \n",
    "    Subarray=[]\n",
    "    models = []\n",
    "    # yarray\n",
    "    \n",
    "    rowsNoCvd = len(dataNoCvd)    \n",
    "    rowsCvd = len(dataCvd)\n",
    "#     print(\"Size of NoCvd cases in train set:\",rowsNoCvd)\n",
    "#     print(\"Size of Cvd cases in train set  :\",rowsCvd)\n",
    "#     print(\"validation_total\",len(validation_total[0][0]))\n",
    "\n",
    "\n",
    "    numOfSubsamples = rowsNoCvd//(rowsCvd*ratio)\n",
    "    numOfSubsamples= int(numOfSubsamples)\n",
    "    SubNoCvd = rowsNoCvd//numOfSubsamples\n",
    "    residue = rowsNoCvd- SubNoCvd*numOfSubsamples\n",
    "\n",
    "\n",
    "    Up = 0\n",
    "    \n",
    "    for i in range(numOfSubsamples):\n",
    "        \n",
    "        classifier = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=params[5],\n",
    "                  colsample_bynode=1, colsample_bytree=params[8], gamma=params[4],\n",
    "                  learning_rate=params[0], max_delta_step=0, max_depth=params[1],\n",
    "                  min_child_weight=params[3], missing=None, n_estimators=params[2], n_jobs=1,\n",
    "                  nthread=None, objective='binary:logistic', random_state=0,\n",
    "                  reg_alpha=params[9], reg_lambda=params[6], scale_pos_weight=params[7], seed=None,\n",
    "                  silent=None, subsample=params[10], verbosity=1)\n",
    "    #     print(i)\n",
    "    #Creating the training set for each model\n",
    "        Down = Up\n",
    "        Up= Up + SubNoCvd\n",
    "        if i < residue :\n",
    "            Up+= 1\n",
    "        \n",
    "#         print(Down)\n",
    "#         print(Up)\n",
    "        \n",
    "        Sub1=dataNoCvd[Down:Up,:]\n",
    "        Sub2=np.concatenate((Sub1, dataCvd))\n",
    "        Subarray.append(Sub2)\n",
    "        \n",
    "    #Dividing to X and y of the previous traing set    \n",
    "        X=np.delete(Subarray[i], f_size, axis=1)\n",
    "        y=np.delete(Subarray[i], slice(0, f_size), axis=1)\n",
    "        y=np.reshape(y, len(y))\n",
    "        \n",
    "        X_val = validation_total[i][0]\n",
    "        y_val = validation_total[i][1]\n",
    "#         print(len(X),len(X_val))\n",
    "        eval_set  = [(X,y), (X_val,y_val)]\n",
    "        \n",
    "        classifier.fit( X, y, eval_set=eval_set, eval_metric=\"auc\", early_stopping_rounds=50,verbose = False)\n",
    "#         classifier.fit( X, y)\n",
    "\n",
    "        \n",
    "        models.append(classifier)\n",
    "\n",
    "        \n",
    "    return models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/train-test-split-and-cross-validation-in-python-80b61beca4b6\n",
    "\n",
    "from sklearn.metrics import  confusion_matrix,roc_curve, roc_auc_score, accuracy_score\n",
    "from imxgboost.imbalance_xgb import imbalance_xgboost as imb_xgb\n",
    "from sklearn.model_selection import StratifiedKFold # import KFold\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import brier_score_loss\n",
    "\n",
    "def my_cross_val( cv, train_total, test_total, train_total_NoCvd, train_total_Cvd,params, weights, ratioo):\n",
    "    accuracy = []\n",
    "    specificity = []\n",
    "    sensitivity = []\n",
    "    auc = []\n",
    "    brier_score = []\n",
    "    cv_total_shap_values = []\n",
    "    X_total = []\n",
    "    y_pred_total = []\n",
    "    \n",
    "    # print(X)\n",
    "    for i in range(cv):\n",
    "#         X_train = train_total[i][0]\n",
    "#         y_train = train_total[i][1]\n",
    "        X_test = test_total[i][0]\n",
    "        y_test = test_total[i][1]\n",
    "        \n",
    "\n",
    "\n",
    "        models = my_fit( ratioo, train_total_NoCvd[i], train_total_Cvd[i], validation_total,params)\n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "        y_pred,shap_values,importances,y_pred_prob = predict_weighted_average(models,X_test,weights)\n",
    "        \n",
    "#         print(len(shap_values[0]))\n",
    "#         print(len(shap_values))\n",
    "        \n",
    "        \n",
    "        if(i==0):\n",
    "             cv_total_importances = importances.copy()\n",
    "        else:\n",
    "            for key in cv_total_importances:\n",
    "                cv_total_importances[key] += importances[key]\n",
    "\n",
    "        \n",
    "        \n",
    "        if(i==0):\n",
    "            #combine all shapley values in one array\n",
    "            X_total = X_test.copy()\n",
    "            cv_total_shap_values = shap_values.copy() \n",
    "            \n",
    "            #combine all y_pred in one array\n",
    "            y_pred_total = y_pred.copy()\n",
    "            \n",
    "        else:\n",
    "            #combine all shapley values in one array\n",
    "            X_total = np.concatenate((X_total, X_test), axis=0)\n",
    "#             print(\"lala\",len(cv_total_shap_values))\n",
    "#             print(cv_total_shap_values)\n",
    "#             print(shap_values)\n",
    "#             print(cv_total_shap_values)\n",
    "            cv_total_shap_values = np.concatenate((cv_total_shap_values, shap_values), axis=0)\n",
    "            \n",
    "            #combine all y_pred in one array\n",
    "            y_pred_total = np.concatenate((y_pred_total, y_pred), axis=0)\n",
    "            \n",
    "                        \n",
    "       \n",
    "                        \n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "    #     print(cm)\n",
    "\n",
    "        total1=sum(sum(cm))\n",
    "        #####from confusion matrix calculate accuracy\n",
    "        accuracy1=(cm[0,0]+cm[1,1])/total1\n",
    "    #   print ('Accuracy : ', accuracy1)\n",
    "\n",
    "        specificity1 = cm[0,0]/(cm[0,0]+cm[0,1])\n",
    "    #   print('Specificity : ', specificity1 )\n",
    "\n",
    "        sensitivity1 = cm[1,1]/(cm[1,0]+cm[1,1])\n",
    "    #   print('Sensitivity : ', sensitivity1)\n",
    "        brier_score_loss1 = brier_score_loss(y_test, y_pred_prob)\n",
    "#         y = np.array(y_test)\n",
    "#         pred = np.array(y_pred)\n",
    "#         fpr, tpr, thresholds = metrics.roc_curve(y, pred, pos_label=2)\n",
    "#         auc1 = metrics.auc(fpr, tpr)\n",
    "        \n",
    "        auc1 = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "    \n",
    "        accuracy.append(accuracy1)\n",
    "        specificity.append(specificity1)\n",
    "        sensitivity.append(sensitivity1) \n",
    "        auc.append(auc1)\n",
    "        brier_score.append(brier_score_loss1)\n",
    "     \n",
    "    feature_names = ['Age','DD','BMI','Smoking','ypolip','PulsBP','aca0','gla0','chl0','tg0','hdl0']\n",
    "    \n",
    "#     print(\"total shap\",len(cv_total_shap_values[0]))\n",
    "#     print(\"total shap\",len(cv_total_shap_values))\n",
    "#     print(\"total shap\",len(X_total[0]))\n",
    "#     print(\"total shap\",len(X_total))\n",
    "    \n",
    "    cv_total_importances['Age'] = cv_total_importances.pop('f0')\n",
    "    cv_total_importances['DD'] = cv_total_importances.pop('f1')\n",
    "    cv_total_importances['BMI'] = cv_total_importances.pop('f2')\n",
    "    cv_total_importances['Smoking'] = cv_total_importances.pop('f3')\n",
    "    cv_total_importances['ypolip'] = cv_total_importances.pop('f4')\n",
    "    cv_total_importances['PulsBP'] = cv_total_importances.pop('f5')\n",
    "    cv_total_importances['aca0'] = cv_total_importances.pop('f6')\n",
    "    cv_total_importances['gla0'] = cv_total_importances.pop('f7')\n",
    "    cv_total_importances['chl0'] = cv_total_importances.pop('f8')\n",
    "    cv_total_importances['tg0'] = cv_total_importances.pop('f9')\n",
    "    cv_total_importances['hdl0'] = cv_total_importances.pop('f10')\n",
    "    \n",
    "    plot_importance(cv_total_importances,importance_type='cover',max_num_features=None)\n",
    "    \n",
    "#     shap.summary_plot(cv_total_shap_values, X_total,feature_names)\n",
    "    \n",
    "    return accuracy, specificity, sensitivity, auc, brier_score, y_pred_total, X_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3 3]\n",
      " [4 4]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [2 2]\n",
      " [3 3]\n",
      " [4 4]\n",
      " [5 5]\n",
      " [6 6]\n",
      " [7 7]\n",
      " [8 8]\n",
      " [9 9]]\n"
     ]
    }
   ],
   "source": [
    "a=np.array([[1,1],[2,2]])\n",
    "b=np.array([[3,3],[4,4]])\n",
    "\n",
    "for i in range(10):\n",
    "    a = [[i,i]]\n",
    "    b = np.concatenate((b, a), axis=0)\n",
    "#     b += a\n",
    "print((b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.geeksforgeeks.org/python-all-possible-permutations-of-n-lists/\n",
    "\n",
    "def compute_comb(params):\n",
    "    \n",
    "    # initializing lists \n",
    "    list1 = params.get(\"learning_rate\")\n",
    "    list2 = params.get(\"max_depth\")\n",
    "    list3 = params.get(\"n_estimators\") \n",
    "    list4 = params.get(\"min_child_weight\")\n",
    "    list5 = params.get(\"gamma\")    \n",
    "    list6 = params.get(\"colsample_bylevel\")\n",
    "    list7 = params.get(\"reg_lambda\") \n",
    "    list8 = params.get(\"scale_pos_weight\")\n",
    "    list9 = params.get(\"colsample_bytree\")\n",
    "    list10 = params.get(\"reg_alpha\")\n",
    "    list11 = params.get(\"subsample\")\n",
    "    \n",
    "\n",
    "#     # printing lists  \n",
    "#     print (\"The original lists are : \" + str(list1) +\n",
    "#                                    \" \" + str(list2) + \n",
    "#                                    \" \" + str(list3) + \n",
    "#                                    \" \" + str(list4) + \n",
    "#                                    \" \" + str(list5) + \n",
    "#                                    \" \" + str(list6) +\n",
    "#                                    \" \" + str(list7) +\n",
    "#                                    \" \" + str(list8)) \n",
    "\n",
    "    # using list comprehension  \n",
    "    # to compute all possible permutations \n",
    "    res = [[i, j, k, l, m, n, o, p,q,r,s] for i in list1  \n",
    "                                          for j in list2 \n",
    "                                          for k in list3 \n",
    "                                          for l in list4 \n",
    "                                          for m in list5 \n",
    "                                          for n in list6 \n",
    "                                          for o in list7\n",
    "                                          for p in list8\n",
    "                                          for q in list9\n",
    "                                          for r in list10\n",
    "                                          for s in list11] \n",
    "\n",
    "    # printing result \n",
    "#     print (\"All possible permutations are : \" +  str(res))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class style:\n",
    "    BOLD = '\\033[1m'\n",
    "    END = '\\033[0m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/train-test-split-and-cross-validation-in-python-80b61beca4b6\n",
    "\n",
    "from sklearn.metrics import  confusion_matrix,roc_curve, roc_auc_score, accuracy_score\n",
    "from imxgboost.imbalance_xgb import imbalance_xgboost as imb_xgb\n",
    "from sklearn.model_selection import StratifiedKFold # import KFold\n",
    "from statistics import mean\n",
    "train_total_sur =[] \n",
    "test_total_sur=[] \n",
    "validation_total_sur=[]\n",
    "train_total_NoCvd_sur=[] \n",
    "train_total_Cvd_sur=[]\n",
    "def gridsearch(params,weights):\n",
    "    max_sens = 0\n",
    "    max_spes = 0\n",
    "    max_acc = 0\n",
    "    best_params = []\n",
    "    max_params = []\n",
    "    max_params1 = []\n",
    "    max_auc = 0\n",
    "    max_acc1 = 0\n",
    "    combs = compute_comb(params)\n",
    "#     print(len(combs))\n",
    "    for j in range(len(combs)):\n",
    "#         print(j)\n",
    "        accuracy, specificity, sensitivity, auc, brier_score, y_pred_total, X_total = my_cross_val( 10, train_total, test_total, train_total_NoCvd, train_total_Cvd,combs[j],weights,2)\n",
    "        if mean(auc)>0.69:\n",
    "            print(style.BOLD + \"----------------------------------------------------------------\"+ style.END)\n",
    "            print(combs[j])\n",
    "            print (style.BOLD + 'accuracy    ' + style.END, mean(accuracy))\n",
    "            print (style.BOLD + 'specificity ' + style.END, mean(specificity))\n",
    "            print (style.BOLD + 'sensitivity ' + style.END, mean(sensitivity))\n",
    "            print (style.BOLD + 'auc         ' + style.END, mean(auc))\n",
    "            print (style.BOLD + 'brier_score ' + style.END, mean(brier_score))\n",
    "            print (style.BOLD + 'weights     ' + style.END, weights)\n",
    "            print(style.BOLD + \"----------------------------------------------------------------\"+ style.END)\n",
    "#         else:\n",
    "#             print(combs[j])\n",
    "#             print(\"accuracy    \",mean(accuracy))\n",
    "#             print(\"specificity \",mean(specificity))\n",
    "#             print(\"sensitivity \",mean(sensitivity))\n",
    "#             print(\"auc         \",mean(auc))\n",
    "#             print(\"weights     \",weights)\n",
    "            \n",
    "        best_params.append([combs[j],mean(accuracy),mean(specificity),mean(sensitivity),mean(auc), mean(brier_score), weights])\n",
    "        if mean(auc)> max_auc:\n",
    "            max_sens = mean(sensitivity)\n",
    "            max_spes = mean(specificity)\n",
    "            max_acc = mean(accuracy)\n",
    "            max_auc = mean(auc)\n",
    "            max_params = combs[j]\n",
    "            \n",
    "        #=========================== Surrogate section ==============================    \n",
    "        zero = 0\n",
    "        one = 0\n",
    "        dataNoCvd_surrog = []\n",
    "        dataCvd_surrog = []\n",
    "        dataNoCvd_surrog_temp = []\n",
    "        dataCvd_surrog_temp = []\n",
    "        dataa = X_total.copy()\n",
    "        print(type(X_total))\n",
    "        for i in range(len(y_pred_total)):\n",
    "#             np.concatenate(X_total[i], y_pred_total[i])\n",
    "#             X_total[i].append(y_pred_total[i])\n",
    "            if y_pred_total[i] == 0:\n",
    "                zero += 1\n",
    "                temp1 = np.append(X_total[i],y_pred_total[i])\n",
    "                dataNoCvd_surrog_temp.append(temp1)\n",
    "            else:\n",
    "                one += 1\n",
    "                temp2 = np.append(X_total[i],y_pred_total[i])\n",
    "                dataCvd_surrog_temp.append(temp2)\n",
    "        print(\"Ones  :\",one,\" percentage\",one/len(y_pred_total))\n",
    "        print(\"Zeros :\",zero,\" percentage\",zero/len(y_pred_total))\n",
    "#         print(dataNoCvd_surrog)\n",
    "        dataNoCvd_surrog = np.array(dataNoCvd_surrog_temp)\n",
    "        dataCvd_surrog = np.array(dataCvd_surrog_temp)\n",
    "        print(len(dataNoCvd_surrog))\n",
    "        print(len(dataCvd_surrog))\n",
    "        \n",
    "        train_total_sur, test_total_sur, validation_total_sur, train_total_NoCvd_sur, train_total_Cvd_sur = create_train_test_sets(f_size,10,dataNoCvd_surrog,dataCvd_surrog,0.1,True)\n",
    "       \n",
    "        for i in range(10):\n",
    "#             print(len(train_total_NoCvd_sur[i][0]))\n",
    "            zeroos = 0\n",
    "            onees = 0\n",
    "            zeroos1 = 0\n",
    "            onees1 = 0\n",
    "            zeroos2 = 0\n",
    "            onees2 = 0\n",
    "#             print(train_total_sur[i])\n",
    "#             print(test_total_sur[i])\n",
    "#             print(validation_total_sur[i])\n",
    "            for k in range(len(train_total_sur[i][1])):\n",
    "                if train_total_sur[i][1][k] == 0:\n",
    "                    zeroos += 1\n",
    "                else:\n",
    "                    onees += 1\n",
    "            for kk in range(len(test_total_sur[i][1])):        \n",
    "                if test_total_sur[i][1][kk] == 0:\n",
    "                    zeroos1 += 1\n",
    "                else:\n",
    "                    onees1 += 1\n",
    "                    \n",
    "            for kkk in range(len(validation_total_sur[i][1])):      \n",
    "                if validation_total_sur[i][1][kkk] == 0:\n",
    "                    zeroos2 += 1\n",
    "                else:\n",
    "                    onees2 += 1\n",
    "                    \n",
    "            print(\"Ones  train:\",onees,\" percentage\",onees/len(train_total_sur[i][1]))\n",
    "            print(\"Zeros train:\",zeroos,\" percentage\",zeroos/len(train_total_sur[i][1]))\n",
    "            print(\"Ones  test:\",onees1,\" percentage\",onees1/len(test_total_sur[i][1]))\n",
    "            print(\"Zeros test:\",zeroos1,\" percentage\",zeroos1/len(test_total_sur[i][1]))\n",
    "            print(\"Ones  valid:\",onees2,\" percentage\",onees2/len(validation_total_sur[i][1]))\n",
    "            print(\"Zeros valid:\",zeroos2,\" percentage\",zeroos2/len(validation_total_sur[i][1]))\n",
    "            \n",
    "#             print((train_total_sur[i][1]))\n",
    "            ratio_sur = find_ratio(train_total_sur[i])\n",
    "            print(\"ratio_sur in train set:\", ratio_sur)\n",
    "            ratio_sur = find_ratio(test_total_sur[i])\n",
    "            print(\"ratio_sur in test set:\", ratio_sur)\n",
    "            ratio_sur = find_ratio(validation_total_sur[i])\n",
    "            print(\"ratio_sur in validation set:\", ratio_sur)\n",
    "            print(\" \")\n",
    "         \n",
    "        \n",
    "        params1 = {\n",
    "                    \"learning_rate\"    :[0.1,0.2,0.3],  #0.01-0.2 Makes the model more robust by shrinking the weights on each step\n",
    "                    \"max_depth\"        :[2,3,4],  #3-10 control over-fitting as higher depth will allow model to learn relations very specific to a particular sample\n",
    "                    \"n_estimators\"     :[1000],\n",
    "                    \"min_child_weight\" :[1,2,3,4],         #0.5-1 small values might lead to under-fitting\n",
    "                    \"gamma\"            :[0,0.25,0.5,0.75,1],            #Makes the algorithm conservative --> No overfitting\n",
    "                    \"colsample_bylevel\" :[0.5,0.75,0.85,1], #0.5-1\n",
    "                     \"reg_lambda\"      :[1, 1.5, 2], #  it should be explored to reduce overfitting.\n",
    "                    \"scale_pos_weight\" :[1,2,3],\n",
    "                    \"colsample_bytree\" :[0.5,0.75,0.85,1.0],\n",
    "                    \"reg_alpha\"        :[0,0.1,0.2],\n",
    "                    \"subsample\"        :[0.8,0.9,1.0]\n",
    "    \n",
    "        }\n",
    "        combs1 = compute_comb(params1)\n",
    "        print(len(combs1))\n",
    "        for m in range(len(combs1)):\n",
    "            print (m,\"/\",len(combs1), end=\"\\r\")  \n",
    "            accuracy, specificity, sensitivity, auc,q,qq,qqq = my_cross_val( 10, train_total_sur, test_total_sur, train_total_NoCvd_sur, train_total_Cvd_sur,combs1[m],weights,1)\n",
    "            if mean(accuracy)>0.81:\n",
    "                print(style.BOLD + \"----------------------------------------------------------------\"+ style.END)\n",
    "                print(combs1[m])\n",
    "                print (style.BOLD + 'accuracy    ' + style.END, mean(accuracy))\n",
    "                print (style.BOLD + 'specificity ' + style.END, mean(specificity))\n",
    "                print (style.BOLD + 'sensitivity ' + style.END, mean(sensitivity))\n",
    "                print (style.BOLD + 'auc         ' + style.END, mean(auc))\n",
    "    #             print (style.BOLD + 'brier_score ' + style.END, mean(brier_score))\n",
    "                print(style.BOLD + \"----------------------------------------------------------------\"+ style.END)\n",
    "            if mean(accuracy)> max_acc1:\n",
    "                max_sens1 = mean(sensitivity)\n",
    "                max_spes1 = mean(specificity)\n",
    "                max_acc1 = mean(accuracy)\n",
    "                max_auc1 = mean(auc)\n",
    "                max_params1 = combs1[m]\n",
    "    return max_sens, max_params, max_spes, max_acc, max_auc, best_params, max_sens1, max_params1, max_spes1, max_acc1, max_auc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import  confusion_matrix,roc_curve, roc_auc_score, accuracy_score\n",
    "from imxgboost.imbalance_xgb import imbalance_xgboost as imb_xgb\n",
    "from sklearn.model_selection import StratifiedKFold # import KFold\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import brier_score_loss\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def my_cross_val_sur( cv, train_total, test_total, train_total_NoCvd, train_total_Cvd, validation_total,params):\n",
    "    accuracy = []\n",
    "    specificity = []\n",
    "    sensitivity = []\n",
    "    auc = []\n",
    "    brier_score = []\n",
    "    cv_total_shap_values = []\n",
    "    X_total = []\n",
    "    y_pred_total = []\n",
    "    \n",
    "    # print(X)\n",
    "    for i in range(cv):\n",
    "        X = train_total[i][0]\n",
    "        y = train_total[i][1]\n",
    "        X_test = test_total[i][0]\n",
    "        y_test = test_total[i][1]\n",
    "        \n",
    "        X_val = validation_total[i][0]\n",
    "        y_val = validation_total[i][1]\n",
    "\n",
    "#         stop_early(RandomForestClassifier(n_estimators=400, max_depth=2),n_min_iterations=100,X_train, y_train,X_test, y_test)\n",
    "#         plt.grid()\n",
    "#         clf = RandomForestClassifier(max_depth=150, random_state=0, min_samples_split=3, min_samples_leaf=2)\n",
    "#         clf.fit(X_train, y_train)\n",
    "\n",
    "#         y_pred = clf.predict(X_test)   \n",
    "        classifier = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=params[5],\n",
    "                  colsample_bynode=1, colsample_bytree=params[8], gamma=params[4],\n",
    "                  learning_rate=params[0], max_delta_step=0, max_depth=params[1],\n",
    "                  min_child_weight=params[3], missing=None, n_estimators=params[2], n_jobs=1,\n",
    "                  nthread=None, objective='binary:logistic', random_state=0,\n",
    "                  reg_alpha=params[9], reg_lambda=params[6], scale_pos_weight=params[7], seed=None,\n",
    "                  silent=None, subsample=params[10], verbosity=1)\n",
    "    \n",
    "        eval_set  = [(X,y), (X_val,y_val)]\n",
    "        \n",
    "        classifier.fit( X, y, eval_set=eval_set, eval_metric=\"rmse\", early_stopping_rounds=50,verbose = False)\n",
    "#         classifier.fit( X, y)\n",
    "                        \n",
    "        y_pred = classifier.predict(X_test)  \n",
    "                        \n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "    #     print(cm)\n",
    "\n",
    "        total1=sum(sum(cm))\n",
    "        #####from confusion matrix calculate accuracy\n",
    "        accuracy1=(cm[0,0]+cm[1,1])/total1\n",
    "    #   print ('Accuracy : ', accuracy1)\n",
    "\n",
    "        specificity1 = cm[0,0]/(cm[0,0]+cm[0,1])\n",
    "    #   print('Specificity : ', specificity1 )\n",
    "\n",
    "        sensitivity1 = cm[1,1]/(cm[1,0]+cm[1,1])\n",
    "    #   print('Sensitivity : ', sensitivity1)\n",
    "    \n",
    "#         brier_score_loss1 = brier_score_loss(y_test, y_pred_prob)\n",
    "\n",
    "#         y = np.array(y_test)\n",
    "#         pred = np.array(y_pred)\n",
    "#         fpr, tpr, thresholds = metrics.roc_curve(y, pred, pos_label=2)\n",
    "#         auc1 = metrics.auc(fpr, tpr)\n",
    "        \n",
    "        auc1 = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "    \n",
    "        accuracy.append(accuracy1)\n",
    "        specificity.append(specificity1)\n",
    "        sensitivity.append(sensitivity1) \n",
    "        auc.append(auc1)\n",
    "#         brier_score.append(brier_score_loss1)\n",
    "     \n",
    "    \n",
    "    return accuracy, specificity, sensitivity, auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ratio = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Αποτέλεσμα της gridsearch με τις ακόλουθες παραμέτρους, γίνονται print όλα και αυτά που ικανοποιούν την συνθήκη : if mean(sensitivity)> 0.5 and mean(specificity)>0.6 and mean(accuracy)>0.6 and mean(auc)>0.5 είναι με bold και μεταξύ γραμμών."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#σωστό\n",
    "params = {\n",
    "                    \"learning_rate\"    :[0.5],  #0.01-0.2 Makes the model more robust by shrinking the weights on each step\n",
    "                    \"max_depth\"        :[2],  #3-10 control over-fitting as higher depth will allow model to learn relations very specific to a particular sample\n",
    "                    \"n_estimators\"     :[1000],\n",
    "                    \"min_child_weight\" :[3],         #0.5-1 small values might lead to under-fitting\n",
    "                    \"gamma\"            :[0.5],            #Makes the algorithm conservative --> No overfitting\n",
    "                    \"colsample_bylevel\" :[0.5], #0.5-1\n",
    "                     \"reg_lambda\"      :[1], #  it should be explored to reduce overfitting.\n",
    "                    \"scale_pos_weight\" :[3],\n",
    "                    \"colsample_bytree\" :[0.75],\n",
    "                    \"reg_alpha\"        :[0],\n",
    "                    \"subsample\"        :[1.0]\n",
    "    \n",
    "        }\n",
    "# [0.5, 2, 1000, 3, 0.5, 0.6, 1, 3, 0.8, 0, 1.0]\n",
    "# accuracy     0.6875\n",
    "# specificity  0.6802790346907994\n",
    "# sensitivity  0.785\n",
    "# auc          0.7326395173453997\n",
    "# weights      [0.2, 0.05, 0.33, 0.13, 0.15, 0.14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "weights = [0.2, 0.05, 0.33, 0.13, 0.15, 0.14]\n",
    "\n",
    "max_sens ,max_params, max_spes, max_acc, max_auc, best_params, max_sens1, max_params1, max_spes1, max_acc1, max_auc1 = gridsearch(params,weights)\n",
    "\n",
    "# parameters : [0.5, 2, 1000, 3, 0.5, 0.5, 1, 3, 0.75, 0, 1.0]\n",
    "# accuracy : 0.6928571428571428\n",
    "# spes     : 0.6860859728506787\n",
    "# sens     : 0.785\n",
    "# auc      : 0.7355429864253394\n",
    "# weights  : [0.2, 0.1, 0.35, 0.1, 0.15, 0.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[0.1, 2, 1000, 4, 0, 0.5, 1.5, 1, 1.0, 0.1, 0.8]\n",
    "accuracy     0.8107142857142857\n",
    "specificity  0.903078078078078\n",
    "sensitivity  0.6410526315789473\n",
    "auc          0.7720653548285127\n",
    "\n",
    "[0.1, 4, 1000, 4, 1, 0.5, 1, 1, 0.5, 0.2, 1.0]\n",
    "accuracy     0.8107142857142857\n",
    "specificity  0.8948198198198198\n",
    "sensitivity  0.6560526315789473\n",
    "auc          0.7754362256993836"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
