{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weighted Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "560\n",
      "17\n"
     ]
    }
   ],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import numpy\n",
    "numpy.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "# Importing the dataset\n",
    "dataset = pd.read_csv('CVD dataset2.csv')\n",
    "data=dataset.iloc[:, :].values\n",
    "X = dataset.iloc[:, :-1].values\n",
    "y = dataset.iloc[:, 16].values\n",
    "# X = dataset.iloc[:, [ 0, 1, 2, 5, 10, 11, 13, 15]].values\n",
    "\n",
    "rows = len(data)    # 3 rows in your example\n",
    "cols = len(data[0])\n",
    "print(rows)\n",
    "print(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "519\n",
      "41\n"
     ]
    }
   ],
   "source": [
    "dataNoCvd = data[~(data[:,16] > 0.0)]\n",
    "dataCvd = data[~(data[:,16] < 1.0)]\n",
    "rowsNoCvd = len(dataNoCvd)    \n",
    "rowsCvd = len(dataCvd)\n",
    "print(rowsNoCvd)\n",
    "print(rowsCvd)\n",
    "# print(dataNoCvd)\n",
    "# print(dataCvd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_NoCvd_limitsArray(i,DownNoCvd, UpNoCvd, fold_NoCvd_total, fold_NoCvd_residue):\n",
    "    if i>0 :\n",
    "        DownNoCvd = UpNoCvd\n",
    "        UpNoCvd= fold_NoCvd_total+UpNoCvd\n",
    "    if i < fold_NoCvd_residue :\n",
    "        UpNoCvd+= 1\n",
    "#     print(\"NoCvd limits, Down :\",DownNoCvd,\"Up \",UpNoCvd)\n",
    "    return   DownNoCvd, UpNoCvd   \n",
    " \n",
    "def find_Cvd_limitsArray(i,DownCvd, UpCvd, fold_Cvd_total, fold_Cvd_residue,cv):\n",
    "    if i>0 :\n",
    "        DownCvd = UpCvd\n",
    "        UpCvd= fold_Cvd_total+UpCvd\n",
    "    if i >= cv - fold_Cvd_residue :\n",
    "         UpCvd+= 1     \n",
    "#     print(\"Cvd limits, Down :\",DownCvd,\"Up \",UpCvd)\n",
    "    return   DownCvd, UpCvd  \n",
    "\n",
    "def find_testSubset(DownNoCvd,UpNoCvd,DownCvd,UpCvd):\n",
    "    temp1=dataNoCvd[DownNoCvd:UpNoCvd,:]\n",
    "    temp2=dataCvd[DownCvd:UpCvd,:]\n",
    "    temp3=np.concatenate((temp1, temp2))\n",
    "#     print(len(temp1),\"  \",len(temp2),\"  \",len(temp3))\n",
    "    return temp1, temp2, temp3\n",
    "\n",
    "def find_trainSubset(DownNoCvd,UpNoCvd,DownCvd,UpCvd):\n",
    "    temp1 = np.delete(dataNoCvd, slice(DownNoCvd, UpNoCvd), axis=0)\n",
    "    temp2 = np.delete(dataCvd, slice(DownCvd, UpCvd), axis=0)\n",
    "    temp3 = np.concatenate((temp1, temp2))\n",
    "#     print(len(temp1),\"  \",len(temp2),\"  \",len(temp3))\n",
    "    return temp1, temp2, temp3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_test_sets(cv,dataNoCvd,dataCvd):\n",
    "    test_total = []\n",
    "    train_total = []\n",
    "    train_total_Cvd = []\n",
    "    train_total_NoCvd = []\n",
    "    \n",
    "    rowsNoCvd = len(dataNoCvd)    \n",
    "    rowsCvd = len(dataCvd)\n",
    "\n",
    "    fold_Cvd_total = rowsCvd//cv\n",
    "    fold_Cvd_residue= rowsCvd%cv\n",
    "    print(\"fold_Cvd_total  :\",fold_Cvd_total,\" fold_Cvd_residue  :\",fold_Cvd_residue)\n",
    "\n",
    "    fold_NoCvd_total = rowsNoCvd//cv\n",
    "    fold_NoCvd_residue= rowsNoCvd%cv\n",
    "    print(\"fold_NoCvd_total:\",fold_NoCvd_total,\"fold_NoCvd_residue:\",fold_NoCvd_residue)\n",
    "\n",
    "    DownNoCvd=0\n",
    "    DownCvd=0\n",
    "    UpNoCvd = fold_NoCvd_total\n",
    "    UpCvd = fold_Cvd_total\n",
    "\n",
    "    for i in range(cv):\n",
    "        X_test = []\n",
    "        y_test = []\n",
    "        X_train = []\n",
    "        y_train = []\n",
    "#         print(\"============\",i,\"==============\")\n",
    "        DownNoCvd, UpNoCvd = find_NoCvd_limitsArray(i, DownNoCvd, UpNoCvd, fold_NoCvd_total, fold_NoCvd_residue)\n",
    "        DownCvd, UpCvd = find_Cvd_limitsArray(i, DownCvd, UpCvd, fold_Cvd_total, fold_Cvd_residue,cv)\n",
    "\n",
    "        testSubset_NoCvd ,testSubset_Cvd ,testSubset_total = find_testSubset(DownNoCvd,UpNoCvd,DownCvd,UpCvd)\n",
    "        trainSubset_NoCvd ,trainSubset_Cvd ,trainSubset_total = find_trainSubset(DownNoCvd,UpNoCvd,DownCvd,UpCvd)\n",
    "     \n",
    "    #creating X_train, y_train, X_test, y_test\n",
    "        X_test.append(np.delete(testSubset_total, 16, axis=1))\n",
    "        y_test_temp = np.delete(testSubset_total, slice(0, 16), axis=1)\n",
    "        y_test.append(np.reshape(y_test_temp, len(y_test_temp)))\n",
    "        X_test_temp = np.array(X_test)\n",
    "        X_test = X_test_temp[0]\n",
    "        y_test_temp = np.array(y_test)\n",
    "        y_test = y_test_temp[0]\n",
    "    \n",
    "        X_train.append(np.delete(trainSubset_total, 16, axis=1))\n",
    "        y_train_temp = np.delete(trainSubset_total, slice(0, 16), axis=1)\n",
    "        y_train.append(np.reshape(y_train_temp, len(y_train_temp)))\n",
    "        X_train_temp = np.array(X_train)\n",
    "        X_train = X_train_temp[0]\n",
    "        y_train_temp = np.array(y_train)\n",
    "        y_train = y_train_temp[0]\n",
    "        \n",
    "    #add every subset in a list so we can handle thm later \n",
    "        test_total.append([X_test,y_test])\n",
    "        train_total.append([X_train,y_train]) \n",
    "        train_total_NoCvd.append(trainSubset_NoCvd)\n",
    "        train_total_Cvd.append(trainSubset_Cvd)\n",
    "    return train_total, test_total, train_total_NoCvd, train_total_Cvd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold_Cvd_total  : 4  fold_Cvd_residue  : 1\n",
      "fold_NoCvd_total: 51 fold_NoCvd_residue: 9\n"
     ]
    }
   ],
   "source": [
    "train_total, test_total, train_total_NoCvd, train_total_Cvd = create_train_test_sets(10,dataNoCvd,dataCvd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "504\n",
      "ratio in train set: 0.07341269841269842\n",
      "ratio in test set: 0.07142857142857142\n",
      "504\n",
      "ratio in train set: 0.07341269841269842\n",
      "ratio in test set: 0.07142857142857142\n",
      "504\n",
      "ratio in train set: 0.07341269841269842\n",
      "ratio in test set: 0.07142857142857142\n",
      "504\n",
      "ratio in train set: 0.07341269841269842\n",
      "ratio in test set: 0.07142857142857142\n",
      "504\n",
      "ratio in train set: 0.07341269841269842\n",
      "ratio in test set: 0.07142857142857142\n",
      "504\n",
      "ratio in train set: 0.07341269841269842\n",
      "ratio in test set: 0.07142857142857142\n",
      "504\n",
      "ratio in train set: 0.07341269841269842\n",
      "ratio in test set: 0.07142857142857142\n",
      "504\n",
      "ratio in train set: 0.07341269841269842\n",
      "ratio in test set: 0.07142857142857142\n",
      "504\n",
      "ratio in train set: 0.07341269841269842\n",
      "ratio in test set: 0.07142857142857142\n",
      "504\n",
      "ratio in train set: 0.07142857142857142\n",
      "ratio in test set: 0.08928571428571429\n"
     ]
    }
   ],
   "source": [
    "# Chech ratio in each train and test set\n",
    "def find_ratio(index_list):\n",
    "    one = 0\n",
    "    lenght=len(index_list[0])\n",
    "    for i in range(lenght):\n",
    "#         print(index_list[1])\n",
    "        if index_list[1][i] == 1.0 :\n",
    "            one+= 1\n",
    "    ratio = one/lenght\n",
    "    return ratio\n",
    "\n",
    "for i in range(10):\n",
    "    print(len(train_total[i][0]))\n",
    "    ratio = find_ratio(train_total[i])\n",
    "    print(\"ratio in train set:\", ratio)\n",
    "    ratio = find_ratio(test_total[i])\n",
    "    print(\"ratio in test set:\", ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "def my_fit(ratio,dataNoCvd,dataCvd,params):   \n",
    "    Subarray=[]\n",
    "    models = []\n",
    "    # yarray\n",
    "    \n",
    "    rowsNoCvd = len(dataNoCvd)    \n",
    "    rowsCvd = len(dataCvd)\n",
    "#     print(\"Size of NoCvd cases in train set:\",rowsNoCvd)\n",
    "#     print(\"Size of Cvd cases in train set  :\",rowsCvd)\n",
    "\n",
    "\n",
    "    numOfSubsamples = rowsNoCvd//(rowsCvd*ratio)\n",
    "    SubNoCvd = rowsNoCvd//numOfSubsamples\n",
    "    residue = rowsNoCvd- SubNoCvd*numOfSubsamples\n",
    "\n",
    "\n",
    "    Up = 0\n",
    "\n",
    "\n",
    "    for i in range(numOfSubsamples):\n",
    "        \n",
    "        classifier = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=params[5],\n",
    "                  colsample_bynode=1, colsample_bytree=0.75, gamma=params[4],\n",
    "                  learning_rate=params[0], max_delta_step=0, max_depth=params[1],\n",
    "                  min_child_weight=params[3], missing=None, n_estimators=params[2], n_jobs=1,\n",
    "                  nthread=None, objective='binary:logistic', random_state=0,\n",
    "                  reg_alpha=0.1, reg_lambda=params[6], scale_pos_weight=params[7], seed=None,\n",
    "                  silent=None, subsample=0.8, verbosity=1)\n",
    "    #     print(i)\n",
    "    #Creating the training set for each model\n",
    "        Down = Up\n",
    "        Up= Up + SubNoCvd\n",
    "        if i < residue :\n",
    "            Up+= 1\n",
    "        \n",
    "#         print(Down)\n",
    "#         print(Up)\n",
    "        \n",
    "        Sub1=dataNoCvd[Down:Up,:]\n",
    "        Sub2=np.concatenate((Sub1, dataCvd))\n",
    "        Subarray.append(Sub2)\n",
    "        \n",
    "    #Dividing to X and y of the previous traing set    \n",
    "        X=np.delete(Subarray[i], 16, axis=1)\n",
    "        y=np.delete(Subarray[i], slice(0, 16), axis=1)\n",
    "        y=np.reshape(y, len(y))\n",
    "        \n",
    "        classifier.fit( X, y)\n",
    "        models.append(classifier)\n",
    "    return models   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def predict_weighted_average(models,X,weights):\n",
    "    y_pred = []\n",
    "    y_pred_models = []\n",
    "#     weights = [0.16, 0.175, 0.2, 0.147, 0.148, 0.16]\n",
    "#     weights = [0.15, 0.4, 0.2, 0.05, 0.05, 0.15]\n",
    "#   weights = [0.16, 0.173, 0.2, 0.147, 0.16, 0.16]\n",
    "    models_size = len(models)\n",
    "    X_size = len(X)\n",
    "            \n",
    "    for i in range(models_size):\n",
    "        y_pred_models.append(models[i].predict_proba(X)[:,1]) \n",
    "        \n",
    "    for j in range(X_size):\n",
    "        sum = 0\n",
    "        for i in range(models_size):\n",
    "            \n",
    "            sum = sum + y_pred_models[i][j]*weights[i]\n",
    "        avr = sum\n",
    "#         /models_size\n",
    "        if avr>= 0.5 :\n",
    "            y_pred.append(1.)\n",
    "        else:\n",
    "            y_pred.append(0.)\n",
    "    return y_pred        \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "{'ss': 2, 'pp': 0}\n"
     ]
    }
   ],
   "source": [
    "k= {\"ss\":2,\"pp\":2}\n",
    "if k.get(\"o\")!= None:\n",
    "    print(0)\n",
    "print(k.get(\"sp\"))\n",
    "l= {\"ss\":0,\"pp\":0}\n",
    "l.update({\"ss\": k.get(\"ss\")+l.get(\"ss\")})\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/train-test-split-and-cross-validation-in-python-80b61beca4b6\n",
    "\n",
    "from sklearn.metrics import  confusion_matrix,roc_curve, roc_auc_score, accuracy_score\n",
    "from imxgboost.imbalance_xgb import imbalance_xgboost as imb_xgb\n",
    "from sklearn.model_selection import StratifiedKFold # import KFold\n",
    "from sklearn import metrics\n",
    "from xgboost import plot_importance\n",
    "from matplotlib import pyplot\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "def my_cross_val( cv, train_total, test_total, train_total_NoCvd, train_total_Cvd,params, weights):\n",
    "    accuracy = []\n",
    "    specificity = []\n",
    "    sensitivity = []\n",
    "    auc = []\n",
    "    # print(X)\n",
    "    for i in range(cv):\n",
    "#         X_train = train_total[i][0]\n",
    "#         y_train = train_total[i][1]\n",
    "        X_test = test_total[i][0]\n",
    "        y_test = test_total[i][1]\n",
    "        \n",
    "\n",
    "\n",
    "        models = my_fit( 2, train_total_NoCvd[i], train_total_Cvd[i],params)\n",
    "        \n",
    "\n",
    "        y_pred = predict_weighted_average(models,X_test,weights)\n",
    "\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "    #     print(cm)\n",
    "\n",
    "        total1=sum(sum(cm))\n",
    "        #####from confusion matrix calculate accuracy\n",
    "        accuracy1=(cm[0,0]+cm[1,1])/total1\n",
    "    #   print ('Accuracy : ', accuracy1)\n",
    "\n",
    "        specificity1 = cm[0,0]/(cm[0,0]+cm[0,1])\n",
    "    #   print('Specificity : ', specificity1 )\n",
    "\n",
    "        sensitivity1 = cm[1,1]/(cm[1,0]+cm[1,1])\n",
    "    #   print('Sensitivity : ', sensitivity1)\n",
    "        \n",
    "#         y = np.array(y_test)\n",
    "#         pred = np.array(y_pred)\n",
    "#         fpr, tpr, thresholds = metrics.roc_curve(y, pred, pos_label=2)\n",
    "#         auc1 = metrics.auc(fpr, tpr)\n",
    "        \n",
    "        auc1 = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "    \n",
    "        accuracy.append(accuracy1)\n",
    "        specificity.append(specificity1)\n",
    "        sensitivity.append(sensitivity1) \n",
    "        auc.append(auc1)\n",
    "    importancies = [[0,0],[0,1],[0,2],[0,3],[0,4],[0,5],[0,6],[0,7],[0,8],[0,9],[0,10],[0,11],[0,12],[0,13],[0,14],[0,15]]\n",
    "    times_used    = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "    for j in range(len(models)):\n",
    "#         models[j].get_score(importance_type='gain')\n",
    "#         plot_importance(models[j],importance_type='gain')\n",
    "        p =models[j].get_booster().get_score(importance_type=\"gain\")\n",
    "#         print(p)\n",
    "        \n",
    "        importancies,times_used = importances_average(p,importancies,times_used)\n",
    "#         plot_importance(models[j], max_num_features=16)\n",
    "#         plot_importance(models[j],importance_type='weight')\n",
    "#         pyplot.show()\n",
    "    for i in range(len(importancies)):\n",
    "        if times_used[i] != 0:\n",
    "            importancies[i][0] = importancies[i][0]/times_used[i]\n",
    "#         print(importancies[i])\n",
    "    \n",
    "   \n",
    "    importancies.sort(key=lambda tup: tup[0],reverse=True)\n",
    "    most_important = list()\n",
    "    for i in range(8):\n",
    "        most_important.append(importancies[i])\n",
    "    print(most_important)\n",
    "\n",
    "    return accuracy, specificity, sensitivity, auc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def importances_average(p,importancies,times_used):\n",
    "        if p.get(\"f0\")!= None:\n",
    "            importancies[0][0] += p.get(\"f0\")\n",
    "            times_used[0] += 1\n",
    "        if p.get(\"f1\")!= None:\n",
    "            importancies[1][0] += p.get(\"f1\")\n",
    "            times_used[1] += 1\n",
    "        if p.get(\"f2\")!= None:\n",
    "            importancies[2][0] += p.get(\"f2\")\n",
    "            times_used[2] += 1\n",
    "        if p.get(\"f3\")!= None:\n",
    "            importancies[3][0] += p.get(\"f3\")\n",
    "            times_used[3] += 1\n",
    "        if p.get(\"f4\")!= None:\n",
    "            importancies[4][0] += p.get(\"f4\")\n",
    "            times_used[4] += 1\n",
    "        if p.get(\"f5\")!= None:\n",
    "            importancies[5][0] += p.get(\"f5\")\n",
    "            times_used[5] += 1\n",
    "        if p.get(\"f6\")!= None:\n",
    "            importancies[6][0] += p.get(\"f6\")\n",
    "            times_used[6] += 1\n",
    "        if p.get(\"f7\")!= None:\n",
    "            importancies[7][0] += p.get(\"f7\")\n",
    "            times_used[7] += 1\n",
    "        if p.get(\"f8\")!= None:\n",
    "            importancies[8][0] += p.get(\"f8\")\n",
    "            times_used[8] += 1\n",
    "        if p.get(\"f9\")!= None:\n",
    "            importancies[9][0] += p.get(\"f9\")\n",
    "            times_used[9] += 1\n",
    "        if p.get(\"f10\")!= None:\n",
    "            importancies[10][0] += p.get(\"f10\")\n",
    "            times_used[10] += 1\n",
    "        if p.get(\"f11\")!= None:\n",
    "            importancies[11][0] += p.get(\"f11\")\n",
    "            times_used[11] += 1\n",
    "        if p.get(\"f12\")!= None:\n",
    "            importancies[12][0] += p.get(\"f12\")\n",
    "            times_used[12] += 1\n",
    "        if p.get(\"f13\")!= None:\n",
    "            importancies[13][0] += p.get(\"f13\")\n",
    "            times_used[13] += 1\n",
    "        if p.get(\"f14\")!= None:\n",
    "            importancies[14][0] += p.get(\"f14\")\n",
    "            times_used[14] += 1\n",
    "        if p.get(\"f15\")!= None:\n",
    "            importancies[15][0] += p.get(\"f15\")\n",
    "            times_used[15] += 1\n",
    "        return importancies,times_used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/        \n",
    "params = {\n",
    "                    \"learning_rate\"    :[ 0.5],  #0.01-0.2 Makes the model more robust by shrinking the weights on each step\n",
    "                    \"max_depth\"        :[2],  #3-10 control over-fitting as higher depth will allow model to learn relations very specific to a particular sample\n",
    "                    \"n_estimators\"     :[200],\n",
    "                    \"min_child_weight\" :[3],         #0.5-1 small values might lead to under-fitting\n",
    "                    \"gamma\"            :[0],            #Makes the algorithm conservative --> No overfitting\n",
    "                    \"colsample_bytree\" :[1.0], #0.5-1\n",
    "                     \"reg_lambda\"      :[1], #  it should be explored to reduce overfitting.\n",
    "                    \"scale_pos_weight\" :[5]\n",
    "        }\n",
    "# parameters : [0.3, 2, 300, 3, 0, 1.0, 1, 5]\n",
    "# 0.6803571428571429\n",
    "# 0.680316742081448\n",
    "# 0.69\n",
    "# 0.685158371040724\n",
    "# weights = [0.28, 0.15, 0.1, 0.1, 0.17, 0.2]\n",
    "# weights = [0.30, 0.15, 0.1, 0.1, 0.15, 0.2]\n",
    "# [0.5, 2, 200, 3, 0, 1.0, 1, 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/        \n",
    "params = {\n",
    "                    \"learning_rate\"    :[0.5],  #0.01-0.2 Makes the model more robust by shrinking the weights on each step\n",
    "                    \"max_depth\"        :[2],  #3-10 control over-fitting as higher depth will allow model to learn relations very specific to a particular sample\n",
    "                    \"n_estimators\"     :[100],\n",
    "                    \"min_child_weight\" :[2],         #0.5-1 small values might lead to under-fitting\n",
    "                    \"gamma\"            :[0.5],            #Makes the algorithm conservative --> No overfitting\n",
    "                    \"colsample_bytree\" :[0.75], #0.5-1\n",
    "                     \"reg_lambda\"      :[1], #  it should be explored to reduce overfitting.\n",
    "                    \"scale_pos_weight\" :[5]\n",
    "        }\n",
    "# parameters : [0.5, 2, 100, 2, 0.5, 0.75, 1, 5]\n",
    "# accuracy : 0.6785714285714286\n",
    "# spes     : 0.6802790346907994\n",
    "# sens     : 0.665\n",
    "# auc      : 0.6726395173453997\n",
    "# weights  : [0.15, 0.2, 0.15, 0.25, 0.15, 0.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "                    \"learning_rate\"    :[0.4],  #0.01-0.2 Makes the model more robust by shrinking the weights on each step\n",
    "                    \"max_depth\"        :[2],  #3-10 control over-fitting as higher depth will allow model to learn relations very specific to a particular sample\n",
    "                    \"n_estimators\"     :[100],\n",
    "                    \"min_child_weight\" :[3],         #0.5-1 small values might lead to under-fitting\n",
    "                    \"gamma\"            :[0],            #Makes the algorithm conservative --> No overfitting\n",
    "                    \"colsample_bytree\" :[1.0], #0.5-1\n",
    "                     \"reg_lambda\"      :[1], #  it should be explored to reduce overfitting.\n",
    "                    \"scale_pos_weight\" :[5]\n",
    "        }\n",
    "\n",
    "# parameters : [0.4, 2, 100, 3, 0, 1.0, 1, 5, 0.5, 0.05, 0.8]\n",
    "\n",
    "# parameters : [0.4, 2, 100, 3, 0, 1.0, 1, 5]\n",
    "# accuracy : 0.6678571428571428\n",
    "# spes     : 0.6649321266968325\n",
    "# sens     : 0.715\n",
    "# auc      : 0.6899660633484163\n",
    "# weights  : [0.2, 0.15, 0.15, 0.2, 0.1, 0.2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "                    \"learning_rate\"    :[0.4],  #0.01-0.2 Makes the model more robust by shrinking the weights on each step\n",
    "                    \"max_depth\"        :[2],  #3-10 control over-fitting as higher depth will allow model to learn relations very specific to a particular sample\n",
    "                    \"n_estimators\"     :[100],\n",
    "                    \"min_child_weight\" :[2],         #0.5-1 small values might lead to under-fitting\n",
    "                    \"gamma\"            :[0.5],            #Makes the algorithm conservative --> No overfitting\n",
    "                    \"colsample_bytree\" :[0.5], #0.5-1\n",
    "                     \"reg_lambda\"      :[1], #  it should be explored to reduce overfitting.\n",
    "                    \"scale_pos_weight\" :[4]\n",
    "        }\n",
    "\n",
    "# parameters : [0.4, 2, 100, 2, 0.5, 0.5, 1, 4]\n",
    "# accuracy : 0.6875\n",
    "# spes     : 0.6842006033182504\n",
    "# sens     : 0.735\n",
    "# auc      : 0.7096003016591252\n",
    "# weights  : [0.15, 0.4, 0.1, 0.05, 0.05, 0.25]\n",
    "\n",
    "# Αποτελέσματα για gain+average:\n",
    "#  [[2.8236261372771025, 15], [2.5580499288255822, 13], [2.556403643206909, 1], [2.3482621568517854, 11], \n",
    "#   [2.310497939649767, 10], [2.302223741666667, 7], [2.25283855005, 8], [2.2489727393473724, 3]]\n",
    "# 1 3 7 8 10 11 13 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.4, 2, 100, 2, 0.5, 0.5, 1, 4]\n",
      "[[2.8236261372771025, 15], [2.5580499288255822, 13], [2.556403643206909, 1], [2.3482621568517854, 11], [2.310497939649767, 10], [2.302223741666667, 7], [2.25283855005, 8], [2.2489727393473724, 3]]\n"
     ]
    }
   ],
   "source": [
    "comb = compute_comb(params)\n",
    "print(comb[0])\n",
    "# weights  = [0.15, 0.2, 0.15, 0.25, 0.15, 0.1]\n",
    "# weights = [0.28, 0.15, 0.1, 0.1, 0.17, 0.2]\n",
    "weights = [0.15, 0.4, 0.1, 0.05, 0.05, 0.25]\n",
    "accuracy, specificity, sensitivity, auc = my_cross_val( 10, train_total, test_total, train_total_NoCvd, train_total_Cvd,comb[0],weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6875\n",
      "0.6842006033182504\n",
      "0.735\n",
      "0.7096003016591252\n"
     ]
    }
   ],
   "source": [
    "from statistics import mean\n",
    "print(mean(accuracy))\n",
    "print(mean(specificity))\n",
    "print(mean(sensitivity))\n",
    "print(mean(auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6732142857142858\n",
      "0.6707013574660633\n",
      "0.715\n",
      "0.6928506787330316\n"
     ]
    }
   ],
   "source": [
    "from statistics import mean\n",
    "print(mean(accuracy))\n",
    "print(mean(specificity))\n",
    "print(mean(sensitivity))\n",
    "print(mean(auc))\n",
    "# weights = [0.19, 0.15, 0.14, 0.22, 0.1, 0.2]\n",
    "# weights = [0.19, 0.14, 0.14, 0.22, 0.11, 0.2]\n",
    "# parameters : [0.4, 2, 100, 3, 0, 1.0, 1, 5, 0.5, 0.05, 0.8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6714285714285715\n",
      "0.6687782805429864\n",
      "0.715\n",
      "0.6918891402714932\n"
     ]
    }
   ],
   "source": [
    "from statistics import mean\n",
    "print(mean(accuracy))\n",
    "print(mean(specificity))\n",
    "print(mean(sensitivity))\n",
    "print(mean(auc))\n",
    "# weights = [0.2, 0.15, 0.14, 0.21, 0.1, 0.2]\n",
    "# parameters : [0.4, 2, 100, 3, 0, 1.0, 1, 5, 0.5, 0.05, 0.8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6803571428571429\n",
      "0.680316742081448\n",
      "0.69\n",
      "0.685158371040724\n"
     ]
    }
   ],
   "source": [
    "from statistics import mean\n",
    "print(mean(accuracy))\n",
    "print(mean(specificity))\n",
    "print(mean(sensitivity))\n",
    "print(mean(auc))\n",
    "# weights = [0.28, 0.15, 0.1, 0.1, 0.17, 0.2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.geeksforgeeks.org/python-all-possible-permutations-of-n-lists/\n",
    "\n",
    "def compute_comb(params):\n",
    "    \n",
    "    # initializing lists \n",
    "    list1 = params.get(\"learning_rate\")\n",
    "    list2 = params.get(\"max_depth\")\n",
    "    list3 = params.get(\"n_estimators\") \n",
    "    list4 = params.get(\"min_child_weight\")\n",
    "    list5 = params.get(\"gamma\")    \n",
    "    list6 = params.get(\"colsample_bytree\")\n",
    "    list7 = params.get(\"reg_lambda\") \n",
    "    list8 = params.get(\"scale_pos_weight\")\n",
    "\n",
    "#     # printing lists  \n",
    "#     print (\"The original lists are : \" + str(list1) +\n",
    "#                                    \" \" + str(list2) + \n",
    "#                                    \" \" + str(list3) + \n",
    "#                                    \" \" + str(list4) + \n",
    "#                                    \" \" + str(list5) + \n",
    "#                                    \" \" + str(list6) +\n",
    "#                                    \" \" + str(list7) +\n",
    "#                                    \" \" + str(list8)) \n",
    "\n",
    "    # using list comprehension  \n",
    "    # to compute all possible permutations \n",
    "    res = [[i, j, k, l, m, n, o, p] for i in list1  \n",
    "                                    for j in list2 \n",
    "                                    for k in list3 \n",
    "                                    for l in list4 \n",
    "                                    for m in list5 \n",
    "                                    for n in list6 \n",
    "                                    for o in list7\n",
    "                                    for p in list8] \n",
    "\n",
    "    # printing result \n",
    "#     print (\"All possible permutations are : \" +  str(res))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class style:\n",
    "    BOLD = '\\033[1m'\n",
    "    END = '\\033[0m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/train-test-split-and-cross-validation-in-python-80b61beca4b6\n",
    "\n",
    "from sklearn.metrics import  confusion_matrix,roc_curve, roc_auc_score, accuracy_score\n",
    "from imxgboost.imbalance_xgb import imbalance_xgboost as imb_xgb\n",
    "from sklearn.model_selection import StratifiedKFold # import KFold\n",
    "from statistics import mean\n",
    "\n",
    "def gridsearch(params,weights):\n",
    "    max_sens = 0\n",
    "    max_spes = 0\n",
    "    max_acc = 0\n",
    "    best_params = []\n",
    "    max_params = []\n",
    "    max_auc = 0\n",
    "    combs = compute_comb(params)\n",
    "    print(len(combs))\n",
    "    for j in range(len(combs)):\n",
    "        print(j)\n",
    "        accuracy, specificity, sensitivity, auc = my_cross_val( 10, train_total, test_total, train_total_NoCvd, train_total_Cvd,combs[j],weights)\n",
    "        if mean(sensitivity)> 0.5 and mean(specificity)>0.6 and mean(accuracy)>0.6 and mean(auc)>0.5:\n",
    "            print(style.BOLD + \"----------------------------------------------------------------\"+ style.END)\n",
    "            print(combs[j])\n",
    "            print (style.BOLD + 'accuracy    ' + style.END, mean(accuracy))\n",
    "            print (style.BOLD + 'specificity ' + style.END, mean(specificity))\n",
    "            print (style.BOLD + 'sensitivity ' + style.END, mean(sensitivity))\n",
    "            print (style.BOLD + 'auc         ' + style.END, mean(auc))\n",
    "            print (style.BOLD + 'weights     ' + style.END, weights)\n",
    "            print(style.BOLD + \"----------------------------------------------------------------\"+ style.END)\n",
    "        else:\n",
    "            print(combs[j])\n",
    "            print(\"accuracy    \",mean(accuracy))\n",
    "            print(\"specificity \",mean(specificity))\n",
    "            print(\"sensitivity \",mean(sensitivity))\n",
    "            print(\"auc         \",mean(auc))\n",
    "            print(\"weights     \",weights)\n",
    "            \n",
    "        best_params.append([combs[j],mean(accuracy),mean(specificity),mean(sensitivity),mean(auc), weights])\n",
    "        if mean(auc)> max_auc:\n",
    "            max_sens = mean(sensitivity)\n",
    "            max_spes = mean(specificity)\n",
    "            max_acc = mean(accuracy)\n",
    "            max_auc = mean(auc)\n",
    "            max_params = combs[j]\n",
    "    return max_sens, max_params, max_spes, max_acc, max_auc, best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
